--- a/src/blib2to3/pgen2/pgen.py
+++ b/src/blib2to3/pgen2/pgen.py
@@ -22,6 +22,12 @@
     first: dict[str, Optional[dict[str, int]]]
 
     def __init__(self, filename: Path, stream: Optional[IO[str]] = None) -> None:
+        """TODO: docstring
+
+        Args:
+            filename:
+            stream:
+        """
         close_stream = None
         if stream is None:
             stream = open(filename, encoding="utf-8")
@@ -122,6 +128,7 @@
                     return ilabel
 
     def addfirstsets(self) -> None:
+        """TODO: docstring"""
         names = list(self.dfas.keys())
         names.sort()
         for name in names:
@@ -162,6 +169,7 @@
         self.first[name] = totalset
 
     def parse(self) -> tuple[dict[str, list["DFAState"]], str]:
+        """TODO: docstring"""
         dfas = {}
         startsymbol: Optional[str] = None
         # MSTART: (NEWLINE | RULE)* ENDMARKER
@@ -344,6 +352,7 @@
         return value
 
     def gettoken(self) -> None:
+        """TODO: docstring"""
         tup = next(self.generator)
         while tup[0] in (tokenize.COMMENT, tokenize.NL):
             tup = next(self.generator)
@@ -415,5 +424,10 @@
 
 
 def generate_grammar(filename: Path = "Grammar.txt") -> PgenGrammar:
+    """TODO: docstring
+
+    Args:
+        filename:
+    """
     p = ParserGenerator(filename)
     return p.make_grammar()

--- a/src/blib2to3/pgen2/tokenize.py
+++ b/src/blib2to3/pgen2/tokenize.py
@@ -128,6 +128,12 @@
 
 
 def tokenize(source: str, grammar: Optional[Grammar] = None) -> Iterator[TokenInfo]:
+    """TODO: docstring
+
+    Args:
+        source:
+        grammar:
+    """
     async_keywords = False if grammar is None else grammar.async_keywords
 
     lines = source.split("\n")
@@ -243,6 +249,15 @@
 def printtoken(
     type: int, token: str, srow_col: Coord, erow_col: Coord, line: str
 ) -> None:  # for testing
+    """TODO: docstring
+
+    Args:
+        type:
+        token:
+        srow_col:
+        erow_col:
+        line:
+    """
     (srow, scol) = srow_col
     (erow, ecol) = erow_col
     print(

--- a/src/blib2to3/pgen2/driver.py
+++ b/src/blib2to3/pgen2/driver.py
@@ -194,6 +194,12 @@
         return self.parse_tokens(tokens, debug)
 
     def _partially_consume_prefix(self, prefix: str, column: int) -> tuple[str, str]:
+        """TODO: docstring
+
+        Args:
+            prefix:
+            column:
+        """
         lines: list[str] = []
         current_line = ""
         current_column = 0

--- a/src/blib2to3/pgen2/tokenize.py
+++ b/src/blib2to3/pgen2/tokenize.py
@@ -138,6 +138,12 @@
 
 
 def tokenize(source: str, grammar: Optional[Grammar] = None) -> Iterator[TokenInfo]:
+    """TODO: docstring
+
+    Args:
+        source:
+        grammar:
+    """
     lines = source.split("\n")
     lines += [""]  # For newline tokens in files that don't end in a newline
     line, column = 1, 0

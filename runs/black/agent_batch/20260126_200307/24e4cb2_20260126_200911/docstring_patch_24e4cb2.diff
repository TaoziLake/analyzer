--- a/src/blib2to3/pgen2/tokenize.py
+++ b/src/blib2to3/pgen2/tokenize.py
@@ -138,6 +138,12 @@
 
 
 def tokenize(source: str, grammar: Optional[Grammar] = None) -> Iterator[TokenInfo]:
+    """TODO: docstring
+
+    Args:
+        source:
+        grammar:
+    """
     async_keywords = False if grammar is None else grammar.async_keywords
 
     lines = source.split("\n")

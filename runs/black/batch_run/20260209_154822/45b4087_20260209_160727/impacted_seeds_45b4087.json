{
  "commit": "45b4087",
  "parent": "f209c53a0882c6a0e6a7cdcd452d44ba8b9ac364",
  "repo": "D:\\locbench\\black",
  "num_files_in_diff": 44,
  "num_py_files_in_diff": 30,
  "num_seeds": 143,
  "seeds": [
    {
      "path": "action/main.py",
      "version": "new",
      "line": 97,
      "kind": "function",
      "qualname": "action.main.find_black_version_in_array",
      "span": [
        97,
        118
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def find_black_version_in_array(array: object) -> str | None:\n    if not isinstance(array, list):\n        return None\n    try:\n        for item in array:\n            # Rudimentary PEP 508 parsing.\n            item = item.split(\";\")[0]\n            item = EXTRAS_RE.sub(\"\", item).strip()\n            if item == \"black\":\n                print(\n                    \"::error::Version specifier missing for 'black' dependency in \"\n                    \"pyproject.toml.\",\n                    file=sys.stderr,\n                    flush=True,\n                )\n                sys.exit(1)\n            elif m := BLACK_VERSION_RE.match(item):\n                return m.group(1).strip()\n    except TypeError:\n        pass\n\n    return None",
      "old_code": "def find_black_version_in_array(array: object) -> Union[str, None]:\n    if not isinstance(array, list):\n        return None\n    try:\n        for item in array:\n            # Rudimentary PEP 508 parsing.\n            item = item.split(\";\")[0]\n            item = EXTRAS_RE.sub(\"\", item).strip()\n            if item == \"black\":\n                print(\n                    \"::error::Version specifier missing for 'black' dependency in \"\n                    \"pyproject.toml.\",\n                    file=sys.stderr,\n                    flush=True,\n                )\n                sys.exit(1)\n            elif m := BLACK_VERSION_RE.match(item):\n                return m.group(1).strip()\n    except TypeError:\n        pass\n\n    return None"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 14,
      "kind": "module",
      "qualname": "gallery.gallery",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 31,
      "kind": "class",
      "qualname": "gallery.gallery.BlackVersion",
      "span": [
        29,
        31
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class BlackVersion(NamedTuple):\n    version: str\n    config: str | None = None",
      "old_code": "class BlackVersion(NamedTuple):\n    version: str\n    config: Optional[str] = None"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 34,
      "kind": "function",
      "qualname": "gallery.gallery.get_pypi_download_url",
      "span": [
        34,
        55
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def get_pypi_download_url(package: str, version: str | None) -> str:\n    with urlopen(PYPI_INSTANCE + f\"/{package}/json\") as page:\n        metadata = json.load(page)\n\n    if version is None:\n        sources = metadata[\"urls\"]\n    else:\n        if version in metadata[\"releases\"]:\n            sources = metadata[\"releases\"][version]\n        else:\n            raise ValueError(\n                f\"No releases found with version ('{version}') tag. \"\n                f\"Found releases: {metadata['releases'].keys()}\"\n            )\n\n    for source in sources:\n        if source[\"python_version\"] == \"source\":\n            break\n    else:\n        raise ValueError(f\"Couldn't find any sources for {package}\")\n\n    return cast(str, source[\"url\"])",
      "old_code": "def get_pypi_download_url(package: str, version: Optional[str]) -> str:\n    with urlopen(PYPI_INSTANCE + f\"/{package}/json\") as page:\n        metadata = json.load(page)\n\n    if version is None:\n        sources = metadata[\"urls\"]\n    else:\n        if version in metadata[\"releases\"]:\n            sources = metadata[\"releases\"][version]\n        else:\n            raise ValueError(\n                f\"No releases found with version ('{version}') tag. \"\n                f\"Found releases: {metadata['releases'].keys()}\"\n            )\n\n    for source in sources:\n        if source[\"python_version\"] == \"source\":\n            break\n    else:\n        raise ValueError(f\"Couldn't find any sources for {package}\")\n\n    return cast(str, source[\"url\"])"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 65,
      "kind": "function",
      "qualname": "gallery.gallery.get_package_source",
      "span": [
        65,
        77
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def get_package_source(package: str, version: str | None) -> str:\n    if package == \"cpython\":\n        if version is None:\n            version = \"main\"\n        return f\"https://github.com/python/cpython/archive/{version}.zip\"\n    elif package == \"pypy\":\n        if version is None:\n            version = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{version}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(package, version)",
      "old_code": "def get_package_source(package: str, version: Optional[str]) -> str:\n    if package == \"cpython\":\n        if version is None:\n            version = \"main\"\n        return f\"https://github.com/python/cpython/archive/{version}.zip\"\n    elif package == \"pypy\":\n        if version is None:\n            version = \"branch/default\"\n        return (\n            f\"https://foss.heptapod.net/pypy/pypy/repository/{version}/archive.tar.bz2\"\n        )\n    else:\n        return get_pypi_download_url(package, version)"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 96,
      "kind": "function",
      "qualname": "gallery.gallery.download_and_extract",
      "span": [
        96,
        103
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def download_and_extract(package: str, version: str | None, directory: Path) -> Path:\n    source = get_package_source(package, version)\n\n    local_file, _ = urlretrieve(source, directory / f\"{package}-src\")\n    with get_archive_manager(local_file) as archive:\n        archive.extractall(path=directory)\n        result_dir = get_first_archive_member(archive)\n    return directory / result_dir",
      "old_code": "def download_and_extract(package: str, version: Optional[str], directory: Path) -> Path:\n    source = get_package_source(package, version)\n\n    local_file, _ = urlretrieve(source, directory / f\"{package}-src\")\n    with get_archive_manager(local_file) as archive:\n        archive.extractall(path=directory)\n        result_dir = get_first_archive_member(archive)\n    return directory / result_dir"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 107,
      "kind": "function",
      "qualname": "gallery.gallery.get_package",
      "span": [
        106,
        114
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def get_package(\n    package: str, version: str | None, directory: Path\n) -> Path | None:\n    try:\n        return download_and_extract(package, version, directory)\n    except Exception:\n        print(f\"Caught an exception while downloading {package}.\")\n        traceback.print_exc()\n        return None",
      "old_code": "def get_package(\n    package: str, version: Optional[str], directory: Path\n) -> Optional[Path]:\n    try:\n        return download_and_extract(package, version, directory)\n    except Exception:\n        print(f\"Caught an exception while downloading {package}.\")\n        traceback.print_exc()\n        return None"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 143,
      "kind": "function",
      "qualname": "gallery.gallery.git_switch_branch",
      "span": [
        142,
        151
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def git_switch_branch(\n    branch: str, repo: Path, new: bool = False, from_branch: str | None = None\n) -> None:\n    args = [\"git\", \"checkout\"]\n    if new:\n        args.append(\"-b\")\n    args.append(branch)\n    if from_branch:\n        args.append(from_branch)\n    subprocess.run(args, cwd=repo)",
      "old_code": "def git_switch_branch(\n    branch: str, repo: Path, new: bool = False, from_branch: Optional[str] = None\n) -> None:\n    args = [\"git\", \"checkout\"]\n    if new:\n        args.append(\"-b\")\n    args.append(branch)\n    if from_branch:\n        args.append(from_branch)\n    subprocess.run(args, cwd=repo)"
    },
    {
      "path": "gallery/gallery.py",
      "version": "new",
      "line": 201,
      "kind": "function",
      "qualname": "gallery.gallery.format_repo_with_version",
      "span": [
        199,
        223
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def format_repo_with_version(\n    repo: Path,\n    from_branch: str | None,\n    black_repo: Path,\n    black_version: BlackVersion,\n    input_directory: Path,\n) -> str:\n    current_branch = f\"black-{black_version.version}\"\n    git_switch_branch(black_version.version, repo=black_repo)\n    git_switch_branch(current_branch, repo=repo, new=True, from_branch=from_branch)\n\n    format_cmd: list[Path | str] = [\n        black_runner(black_version.version, black_repo),\n        (black_repo / \"black.py\").resolve(),\n        \".\",\n    ]\n    if black_version.config:\n        format_cmd.extend([\"--config\", input_directory / black_version.config])\n\n    subprocess.run(format_cmd, cwd=repo, check=False)  # ensure the process\n    # continuess to run even it can't format some files. Reporting those\n    # should be enough\n    git_add_and_commit(f\"Format with black:{black_version.version}\", repo=repo)\n\n    return current_branch",
      "old_code": "def format_repo_with_version(\n    repo: Path,\n    from_branch: Optional[str],\n    black_repo: Path,\n    black_version: BlackVersion,\n    input_directory: Path,\n) -> str:\n    current_branch = f\"black-{black_version.version}\"\n    git_switch_branch(black_version.version, repo=black_repo)\n    git_switch_branch(current_branch, repo=repo, new=True, from_branch=from_branch)\n\n    format_cmd: list[Union[Path, str]] = [\n        black_runner(black_version.version, black_repo),\n        (black_repo / \"black.py\").resolve(),\n        \".\",\n    ]\n    if black_version.config:\n        format_cmd.extend([\"--config\", input_directory / black_version.config])\n\n    subprocess.run(format_cmd, cwd=repo, check=False)  # ensure the process\n    # continuess to run even it can't format some files. Reporting those\n    # should be enough\n    git_add_and_commit(f\"Format with black:{black_version.version}\", repo=repo)\n\n    return current_branch"
    },
    {
      "path": "scripts/check_pre_commit_rev_in_example.py",
      "version": "new",
      "line": 24,
      "kind": "function",
      "qualname": "scripts.check_pre_commit_rev_in_example.main",
      "span": [
        20,
        43
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def main(changes: str, source_version_control: str) -> None:\n    changes_html = commonmark.commonmark(changes)\n    changes_soup = BeautifulSoup(changes_html, \"html.parser\")\n    headers = changes_soup.find_all(\"h2\")\n    latest_tag, *_ = (\n        header.string for header in headers if header.string != \"Unreleased\"\n    )\n\n    source_version_control_html = commonmark.commonmark(source_version_control)\n    source_version_control_soup = BeautifulSoup(\n        source_version_control_html, \"html.parser\"\n    )\n    pre_commit_repos = yaml.safe_load(\n        source_version_control_soup.find(class_=\"language-yaml\").string\n    )[\"repos\"]\n\n    for repo in pre_commit_repos:\n        pre_commit_rev = repo[\"rev\"]\n        if not pre_commit_rev == latest_tag:\n            print(\n                \"Please set the rev in ``source_version_control.md`` to be the latest \"\n                f\"one.\\nExpected {latest_tag}, got {pre_commit_rev}.\\n\"\n            )\n            sys.exit(1)",
      "old_code": "def main(changes: str, source_version_control: str) -> None:\n    changes_html = commonmark.commonmark(changes)\n    changes_soup = BeautifulSoup(changes_html, \"html.parser\")\n    headers = changes_soup.find_all(\"h2\")\n    latest_tag, *_ = [\n        header.string for header in headers if header.string != \"Unreleased\"\n    ]\n\n    source_version_control_html = commonmark.commonmark(source_version_control)\n    source_version_control_soup = BeautifulSoup(\n        source_version_control_html, \"html.parser\"\n    )\n    pre_commit_repos = yaml.safe_load(\n        source_version_control_soup.find(class_=\"language-yaml\").string\n    )[\"repos\"]\n\n    for repo in pre_commit_repos:\n        pre_commit_rev = repo[\"rev\"]\n        if not pre_commit_rev == latest_tag:\n            print(\n                \"Please set the rev in ``source_version_control.md`` to be the latest \"\n                f\"one.\\nExpected {latest_tag}, got {pre_commit_rev}.\\n\"\n            )\n            sys.exit(1)"
    },
    {
      "path": "scripts/migrate-black.py",
      "version": "new",
      "line": 43,
      "kind": "function",
      "qualname": "scripts.migrate-black.blackify",
      "span": [
        15,
        82
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def blackify(base_branch: str, black_command: str, logger: logging.Logger) -> int:\n    current_branch = git(\"branch\", \"--show-current\")\n\n    if not current_branch or base_branch == current_branch:\n        logger.error(\"You need to check out a feature branch to work on\")\n        return 1\n\n    if not os.path.exists(\".git\"):\n        logger.error(\"Run me in the root of your repo\")\n        return 1\n\n    merge_base = git(\"merge-base\", \"HEAD\", base_branch)\n    if not merge_base:\n        logger.error(\n            f\"Could not find a common commit for current head and {base_branch}\"\n        )\n        return 1\n\n    commits = git(\n        \"log\", \"--reverse\", \"--pretty=format:%H\", f\"{merge_base}~1..HEAD\"\n    ).split()\n    for commit in commits:\n        git(\"checkout\", commit, f\"-b{commit}-black\")\n        check_output(black_command, shell=True)\n        git(\"commit\", \"-aqm\", \"blackify\")\n\n    git(\"checkout\", base_branch, f\"-b{current_branch}-black\")\n\n    for last_commit, commit in zip(commits, commits[1:], strict=False):\n        allow_empty = (\n            b\"--allow-empty\" in run([\"git\", \"apply\", \"-h\"], stdout=PIPE).stdout\n        )\n        quiet = b\"--quiet\" in run([\"git\", \"apply\", \"-h\"], stdout=PIPE).stdout\n        git_diff = Popen(\n            [\n                \"git\",\n                \"diff\",\n                \"--binary\",\n                \"--find-copies\",\n                f\"{last_commit}-black..{commit}-black\",\n            ],\n            stdout=PIPE,\n        )\n        git_apply = Popen(\n            [\n                \"git\",\n                \"apply\",\n            ]\n            + ([\"--quiet\"] if quiet else [])\n            + [\n                \"-3\",\n                \"--intent-to-add\",\n            ]\n            + ([\"--allow-empty\"] if allow_empty else [])\n            + [\n                \"-\",\n            ],\n            stdin=git_diff.stdout,\n        )\n        if git_diff.stdout is not None:\n            git_diff.stdout.close()\n        git_apply.communicate()\n        git(\"commit\", \"--allow-empty\", \"-aqC\", commit)\n\n    for commit in commits:\n        git(\"branch\", \"-qD\", f\"{commit}-black\")\n\n    return 0",
      "old_code": "def blackify(base_branch: str, black_command: str, logger: logging.Logger) -> int:\n    current_branch = git(\"branch\", \"--show-current\")\n\n    if not current_branch or base_branch == current_branch:\n        logger.error(\"You need to check out a feature branch to work on\")\n        return 1\n\n    if not os.path.exists(\".git\"):\n        logger.error(\"Run me in the root of your repo\")\n        return 1\n\n    merge_base = git(\"merge-base\", \"HEAD\", base_branch)\n    if not merge_base:\n        logger.error(\n            f\"Could not find a common commit for current head and {base_branch}\"\n        )\n        return 1\n\n    commits = git(\n        \"log\", \"--reverse\", \"--pretty=format:%H\", f\"{merge_base}~1..HEAD\"\n    ).split()\n    for commit in commits:\n        git(\"checkout\", commit, f\"-b{commit}-black\")\n        check_output(black_command, shell=True)\n        git(\"commit\", \"-aqm\", \"blackify\")\n\n    git(\"checkout\", base_branch, f\"-b{current_branch}-black\")\n\n    for last_commit, commit in zip(commits, commits[1:]):\n        allow_empty = (\n            b\"--allow-empty\" in run([\"git\", \"apply\", \"-h\"], stdout=PIPE).stdout\n        )\n        quiet = b\"--quiet\" in run([\"git\", \"apply\", \"-h\"], stdout=PIPE).stdout\n        git_diff = Popen(\n            [\n                \"git\",\n                \"diff\",\n                \"--binary\",\n                \"--find-copies\",\n                f\"{last_commit}-black..{commit}-black\",\n            ],\n            stdout=PIPE,\n        )\n        git_apply = Popen(\n            [\n                \"git\",\n                \"apply\",\n            ]\n            + ([\"--quiet\"] if quiet else [])\n            + [\n                \"-3\",\n                \"--intent-to-add\",\n            ]\n            + ([\"--allow-empty\"] if allow_empty else [])\n            + [\n                \"-\",\n            ],\n            stdin=git_diff.stdout,\n        )\n        if git_diff.stdout is not None:\n            git_diff.stdout.close()\n        git_apply.communicate()\n        git(\"commit\", \"--allow-empty\", \"-aqC\", commit)\n\n    for commit in commits:\n        git(\"branch\", \"-qD\", f\"{commit}-black\")\n\n    return 0"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 22,
      "kind": "module",
      "qualname": "src.black.__init__",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 117,
      "kind": "function",
      "qualname": "src.black.__init__.read_pyproject_toml",
      "span": [
        116,
        178
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def read_pyproject_toml(\n    ctx: click.Context, param: click.Parameter, value: str | None\n) -> str | None:\n    \"\"\"Inject Black configuration from \"pyproject.toml\" into defaults in `ctx`.\n\n    Returns the path to a successfully found and read configuration file, None\n    otherwise.\n    \"\"\"\n    if not value:\n        value = find_pyproject_toml(\n            ctx.params.get(\"src\", ()), ctx.params.get(\"stdin_filename\", None)\n        )\n        if value is None:\n            return None\n\n    try:\n        config = parse_pyproject_toml(value)\n    except (OSError, ValueError) as e:\n        raise click.FileError(\n            filename=value, hint=f\"Error reading configuration file: {e}\"\n        ) from None\n\n    if not config:\n        return None\n    else:\n        spellcheck_pyproject_toml_keys(ctx, list(config), value)\n        # Sanitize the values to be Click friendly. For more information please see:\n        # https://github.com/psf/black/issues/1458\n        # https://github.com/pallets/click/issues/1567\n        config = {\n            k: str(v) if not isinstance(v, (list, dict)) else v\n            for k, v in config.items()\n        }\n\n    target_version = config.get(\"target_version\")\n    if target_version is not None and not isinstance(target_version, list):\n        raise click.BadOptionUsage(\n            \"target-version\", \"Config key target-version must be a list\"\n        )\n\n    exclude = config.get(\"exclude\")\n    if exclude is not None and not isinstance(exclude, str):\n        raise click.BadOptionUsage(\"exclude\", \"Config key exclude must be a string\")\n\n    extend_exclude = config.get(\"extend_exclude\")\n    if extend_exclude is not None and not isinstance(extend_exclude, str):\n        raise click.BadOptionUsage(\n            \"extend-exclude\", \"Config key extend-exclude must be a string\"\n        )\n\n    line_ranges = config.get(\"line_ranges\")\n    if line_ranges is not None:\n        raise click.BadOptionUsage(\n            \"line-ranges\", \"Cannot use line-ranges in the pyproject.toml file.\"\n        )\n\n    default_map: dict[str, Any] = {}\n    if ctx.default_map:\n        default_map.update(ctx.default_map)\n    default_map.update(config)\n\n    ctx.default_map = default_map\n    return value",
      "old_code": "def read_pyproject_toml(\n    ctx: click.Context, param: click.Parameter, value: Optional[str]\n) -> Optional[str]:\n    \"\"\"Inject Black configuration from \"pyproject.toml\" into defaults in `ctx`.\n\n    Returns the path to a successfully found and read configuration file, None\n    otherwise.\n    \"\"\"\n    if not value:\n        value = find_pyproject_toml(\n            ctx.params.get(\"src\", ()), ctx.params.get(\"stdin_filename\", None)\n        )\n        if value is None:\n            return None\n\n    try:\n        config = parse_pyproject_toml(value)\n    except (OSError, ValueError) as e:\n        raise click.FileError(\n            filename=value, hint=f\"Error reading configuration file: {e}\"\n        ) from None\n\n    if not config:\n        return None\n    else:\n        spellcheck_pyproject_toml_keys(ctx, list(config), value)\n        # Sanitize the values to be Click friendly. For more information please see:\n        # https://github.com/psf/black/issues/1458\n        # https://github.com/pallets/click/issues/1567\n        config = {\n            k: str(v) if not isinstance(v, (list, dict)) else v\n            for k, v in config.items()\n        }\n\n    target_version = config.get(\"target_version\")\n    if target_version is not None and not isinstance(target_version, list):\n        raise click.BadOptionUsage(\n            \"target-version\", \"Config key target-version must be a list\"\n        )\n\n    exclude = config.get(\"exclude\")\n    if exclude is not None and not isinstance(exclude, str):\n        raise click.BadOptionUsage(\"exclude\", \"Config key exclude must be a string\")\n\n    extend_exclude = config.get(\"extend_exclude\")\n    if extend_exclude is not None and not isinstance(extend_exclude, str):\n        raise click.BadOptionUsage(\n            \"extend-exclude\", \"Config key extend-exclude must be a string\"\n        )\n\n    line_ranges = config.get(\"line_ranges\")\n    if line_ranges is not None:\n        raise click.BadOptionUsage(\n            \"line-ranges\", \"Cannot use line-ranges in the pyproject.toml file.\"\n        )\n\n    default_map: dict[str, Any] = {}\n    if ctx.default_map:\n        default_map.update(ctx.default_map)\n    default_map.update(config)\n\n    ctx.default_map = default_map\n    return value"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 196,
      "kind": "function",
      "qualname": "src.black.__init__.target_version_option_callback",
      "span": [
        195,
        203
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def target_version_option_callback(\n    c: click.Context, p: click.Option | click.Parameter, v: tuple[str, ...]\n) -> list[TargetVersion]:\n    \"\"\"Compute the target versions from a --target-version flag.\n\n    This is its own function because mypy couldn't infer the type correctly\n    when it was a lambda, causing mypyc trouble.\n    \"\"\"\n    return [TargetVersion[val.upper()] for val in v]",
      "old_code": "def target_version_option_callback(\n    c: click.Context, p: Union[click.Option, click.Parameter], v: tuple[str, ...]\n) -> list[TargetVersion]:\n    \"\"\"Compute the target versions from a --target-version flag.\n\n    This is its own function because mypy couldn't infer the type correctly\n    when it was a lambda, causing mypyc trouble.\n    \"\"\"\n    return [TargetVersion[val.upper()] for val in v]"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 207,
      "kind": "function",
      "qualname": "src.black.__init__.enable_unstable_feature_callback",
      "span": [
        206,
        210
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def enable_unstable_feature_callback(\n    c: click.Context, p: click.Option | click.Parameter, v: tuple[str, ...]\n) -> list[Preview]:\n    \"\"\"Compute the features from an --enable-unstable-feature flag.\"\"\"\n    return [Preview[val] for val in v]",
      "old_code": "def enable_unstable_feature_callback(\n    c: click.Context, p: Union[click.Option, click.Parameter], v: tuple[str, ...]\n) -> list[Preview]:\n    \"\"\"Compute the features from an --enable-unstable-feature flag.\"\"\"\n    return [Preview[val] for val in v]"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 227,
      "kind": "function",
      "qualname": "src.black.__init__.validate_regex",
      "span": [
        224,
        232
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def validate_regex(\n    ctx: click.Context,\n    param: click.Parameter,\n    value: str | None,\n) -> Pattern[str] | None:\n    try:\n        return re_compile_maybe_verbose(value) if value is not None else None\n    except re.error as e:\n        raise click.BadParameter(f\"Not a valid regular expression: {e}\") from None",
      "old_code": "def validate_regex(\n    ctx: click.Context,\n    param: click.Parameter,\n    value: Optional[str],\n) -> Optional[Pattern[str]]:\n    try:\n        return re_compile_maybe_verbose(value) if value is not None else None\n    except re.error as e:\n        raise click.BadParameter(f\"Not a valid regular expression: {e}\") from None"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 519,
      "kind": "function",
      "qualname": "src.black.__init__.main",
      "span": [
        517,
        731
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def main(  # noqa: C901\n    ctx: click.Context,\n    code: str | None,\n    line_length: int,\n    target_version: list[TargetVersion],\n    check: bool,\n    diff: bool,\n    line_ranges: Sequence[str],\n    color: bool,\n    fast: bool,\n    pyi: bool,\n    ipynb: bool,\n    python_cell_magics: Sequence[str],\n    skip_source_first_line: bool,\n    skip_string_normalization: bool,\n    skip_magic_trailing_comma: bool,\n    preview: bool,\n    unstable: bool,\n    enable_unstable_feature: list[Preview],\n    quiet: bool,\n    verbose: bool,\n    required_version: str | None,\n    include: Pattern[str],\n    exclude: Pattern[str] | None,\n    extend_exclude: Pattern[str] | None,\n    force_exclude: Pattern[str] | None,\n    stdin_filename: str | None,\n    workers: int | None,\n    src: tuple[str, ...],\n    config: str | None,\n    no_cache: bool,\n) -> None:\n    \"\"\"The uncompromising code formatter.\"\"\"\n    ctx.ensure_object(dict)\n\n    assert sys.version_info >= (3, 10), \"Black requires Python 3.10+\"\n    if sys.version_info[:3] == (3, 12, 5):\n        out(\n            \"Python 3.12.5 has a memory safety issue that can cause Black's \"\n            \"AST safety checks to fail. \"\n            \"Please upgrade to Python 3.12.6 or downgrade to Python 3.12.4\"\n        )\n        ctx.exit(1)\n\n    if src and code is not None:\n        out(\n            main.get_usage(ctx)\n            + \"\\n\\n'SRC' and 'code' cannot be passed simultaneously.\"\n        )\n        ctx.exit(1)\n    if not src and code is None:\n        out(main.get_usage(ctx) + \"\\n\\nOne of 'SRC' or 'code' is required.\")\n        ctx.exit(1)\n\n    # It doesn't do anything if --unstable is also passed, so just allow it.\n    if enable_unstable_feature and not (preview or unstable):\n        out(\n            main.get_usage(ctx)\n            + \"\\n\\n'--enable-unstable-feature' requires '--preview'.\"\n        )\n        ctx.exit(1)\n\n    root, method = (\n        find_project_root(src, stdin_filename) if code is None else (None, None)\n    )\n    ctx.obj[\"root\"] = root\n\n    if verbose:\n        if root:\n            out(\n                f\"Identified `{root}` as project root containing a {method}.\",\n                fg=\"blue\",\n            )\n\n        if config:\n            config_source = ctx.get_parameter_source(\"config\")\n            user_level_config = str(find_user_pyproject_toml())\n            if config == user_level_config:\n                out(\n                    \"Using configuration from user-level config at \"\n                    f\"'{user_level_config}'.\",\n                    fg=\"blue\",\n                )\n            elif config_source in (\n                ParameterSource.DEFAULT,\n                ParameterSource.DEFAULT_MAP,\n            ):\n                out(\"Using configuration from project root.\", fg=\"blue\")\n            else:\n                out(f\"Using configuration in '{config}'.\", fg=\"blue\")\n            if ctx.default_map:\n                for param, value in ctx.default_map.items():\n                    out(f\"{param}: {value}\")\n\n    error_msg = \"Oh no! ðŸ’¥ ðŸ’” ðŸ’¥\"\n    if (\n        required_version\n        and required_version != __version__\n        and required_version != __version__.split(\".\")[0]\n    ):\n        err(\n            f\"{error_msg} The required version `{required_version}` does not match\"\n            f\" the running version `{__version__}`!\"\n        )\n        ctx.exit(1)\n    if ipynb and pyi:\n        err(\"Cannot pass both `pyi` and `ipynb` flags!\")\n        ctx.exit(1)\n\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\n    if target_version:\n        versions = set(target_version)\n    else:\n        # We'll autodetect later.\n        versions = set()\n    mode = Mode(\n        target_versions=versions,\n        line_length=line_length,\n        is_pyi=pyi,\n        is_ipynb=ipynb,\n        skip_source_first_line=skip_source_first_line,\n        string_normalization=not skip_string_normalization,\n        magic_trailing_comma=not skip_magic_trailing_comma,\n        preview=preview,\n        unstable=unstable,\n        python_cell_magics=set(python_cell_magics),\n        enabled_features=set(enable_unstable_feature),\n    )\n\n    lines: list[tuple[int, int]] = []\n    if line_ranges:\n        if ipynb:\n            err(\"Cannot use --line-ranges with ipynb files.\")\n            ctx.exit(1)\n\n        try:\n            lines = parse_line_ranges(line_ranges)\n        except ValueError as e:\n            err(str(e))\n            ctx.exit(1)\n\n    if code is not None:\n        # Run in quiet mode by default with -c; the extra output isn't useful.\n        # You can still pass -v to get verbose output.\n        quiet = True\n\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\n\n    if code is not None:\n        reformat_code(\n            content=code,\n            fast=fast,\n            write_back=write_back,\n            mode=mode,\n            report=report,\n            lines=lines,\n        )\n    else:\n        assert root is not None  # root is only None if code is not None\n        try:\n            sources = get_sources(\n                root=root,\n                src=src,\n                quiet=quiet,\n                verbose=verbose,\n                include=include,\n                exclude=exclude,\n                extend_exclude=extend_exclude,\n                force_exclude=force_exclude,\n                report=report,\n                stdin_filename=stdin_filename,\n            )\n        except GitWildMatchPatternError:\n            ctx.exit(1)\n\n        if not sources:\n            if verbose or not quiet:\n                out(\"No Python files are present to be formatted. Nothing to do ðŸ˜´\")\n            if \"-\" in src:\n                sys.stdout.write(sys.stdin.read())\n            ctx.exit(0)\n\n        if len(sources) == 1:\n            reformat_one(\n                src=sources.pop(),\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                lines=lines,\n                no_cache=no_cache,\n            )\n        else:\n            from black.concurrency import reformat_many\n\n            if lines:\n                err(\"Cannot use --line-ranges to format multiple files.\")\n                ctx.exit(1)\n            reformat_many(\n                sources=sources,\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                workers=workers,\n                no_cache=no_cache,\n            )\n\n    if verbose or not quiet:\n        if code is None and (verbose or report.change_count or report.failure_count):\n            out()\n        out(error_msg if report.return_code else \"All done! âœ¨ ðŸ° âœ¨\")\n        if code is None:\n            click.echo(str(report), err=True)\n    ctx.exit(report.return_code)",
      "old_code": "def main(  # noqa: C901\n    ctx: click.Context,\n    code: Optional[str],\n    line_length: int,\n    target_version: list[TargetVersion],\n    check: bool,\n    diff: bool,\n    line_ranges: Sequence[str],\n    color: bool,\n    fast: bool,\n    pyi: bool,\n    ipynb: bool,\n    python_cell_magics: Sequence[str],\n    skip_source_first_line: bool,\n    skip_string_normalization: bool,\n    skip_magic_trailing_comma: bool,\n    preview: bool,\n    unstable: bool,\n    enable_unstable_feature: list[Preview],\n    quiet: bool,\n    verbose: bool,\n    required_version: Optional[str],\n    include: Pattern[str],\n    exclude: Optional[Pattern[str]],\n    extend_exclude: Optional[Pattern[str]],\n    force_exclude: Optional[Pattern[str]],\n    stdin_filename: Optional[str],\n    workers: Optional[int],\n    src: tuple[str, ...],\n    config: Optional[str],\n    no_cache: bool,\n) -> None:\n    \"\"\"The uncompromising code formatter.\"\"\"\n    ctx.ensure_object(dict)\n\n    assert sys.version_info >= (3, 9), \"Black requires Python 3.9+\"\n    if sys.version_info[:3] == (3, 12, 5):\n        out(\n            \"Python 3.12.5 has a memory safety issue that can cause Black's \"\n            \"AST safety checks to fail. \"\n            \"Please upgrade to Python 3.12.6 or downgrade to Python 3.12.4\"\n        )\n        ctx.exit(1)\n\n    if src and code is not None:\n        out(\n            main.get_usage(ctx)\n            + \"\\n\\n'SRC' and 'code' cannot be passed simultaneously.\"\n        )\n        ctx.exit(1)\n    if not src and code is None:\n        out(main.get_usage(ctx) + \"\\n\\nOne of 'SRC' or 'code' is required.\")\n        ctx.exit(1)\n\n    # It doesn't do anything if --unstable is also passed, so just allow it.\n    if enable_unstable_feature and not (preview or unstable):\n        out(\n            main.get_usage(ctx)\n            + \"\\n\\n'--enable-unstable-feature' requires '--preview'.\"\n        )\n        ctx.exit(1)\n\n    root, method = (\n        find_project_root(src, stdin_filename) if code is None else (None, None)\n    )\n    ctx.obj[\"root\"] = root\n\n    if verbose:\n        if root:\n            out(\n                f\"Identified `{root}` as project root containing a {method}.\",\n                fg=\"blue\",\n            )\n\n        if config:\n            config_source = ctx.get_parameter_source(\"config\")\n            user_level_config = str(find_user_pyproject_toml())\n            if config == user_level_config:\n                out(\n                    \"Using configuration from user-level config at \"\n                    f\"'{user_level_config}'.\",\n                    fg=\"blue\",\n                )\n            elif config_source in (\n                ParameterSource.DEFAULT,\n                ParameterSource.DEFAULT_MAP,\n            ):\n                out(\"Using configuration from project root.\", fg=\"blue\")\n            else:\n                out(f\"Using configuration in '{config}'.\", fg=\"blue\")\n            if ctx.default_map:\n                for param, value in ctx.default_map.items():\n                    out(f\"{param}: {value}\")\n\n    error_msg = \"Oh no! ðŸ’¥ ðŸ’” ðŸ’¥\"\n    if (\n        required_version\n        and required_version != __version__\n        and required_version != __version__.split(\".\")[0]\n    ):\n        err(\n            f\"{error_msg} The required version `{required_version}` does not match\"\n            f\" the running version `{__version__}`!\"\n        )\n        ctx.exit(1)\n    if ipynb and pyi:\n        err(\"Cannot pass both `pyi` and `ipynb` flags!\")\n        ctx.exit(1)\n\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\n    if target_version:\n        versions = set(target_version)\n    else:\n        # We'll autodetect later.\n        versions = set()\n    mode = Mode(\n        target_versions=versions,\n        line_length=line_length,\n        is_pyi=pyi,\n        is_ipynb=ipynb,\n        skip_source_first_line=skip_source_first_line,\n        string_normalization=not skip_string_normalization,\n        magic_trailing_comma=not skip_magic_trailing_comma,\n        preview=preview,\n        unstable=unstable,\n        python_cell_magics=set(python_cell_magics),\n        enabled_features=set(enable_unstable_feature),\n    )\n\n    lines: list[tuple[int, int]] = []\n    if line_ranges:\n        if ipynb:\n            err(\"Cannot use --line-ranges with ipynb files.\")\n            ctx.exit(1)\n\n        try:\n            lines = parse_line_ranges(line_ranges)\n        except ValueError as e:\n            err(str(e))\n            ctx.exit(1)\n\n    if code is not None:\n        # Run in quiet mode by default with -c; the extra output isn't useful.\n        # You can still pass -v to get verbose output.\n        quiet = True\n\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\n\n    if code is not None:\n        reformat_code(\n            content=code,\n            fast=fast,\n            write_back=write_back,\n            mode=mode,\n            report=report,\n            lines=lines,\n        )\n    else:\n        assert root is not None  # root is only None if code is not None\n        try:\n            sources = get_sources(\n                root=root,\n                src=src,\n                quiet=quiet,\n                verbose=verbose,\n                include=include,\n                exclude=exclude,\n                extend_exclude=extend_exclude,\n                force_exclude=force_exclude,\n                report=report,\n                stdin_filename=stdin_filename,\n            )\n        except GitWildMatchPatternError:\n            ctx.exit(1)\n\n        if not sources:\n            if verbose or not quiet:\n                out(\"No Python files are present to be formatted. Nothing to do ðŸ˜´\")\n            if \"-\" in src:\n                sys.stdout.write(sys.stdin.read())\n            ctx.exit(0)\n\n        if len(sources) == 1:\n            reformat_one(\n                src=sources.pop(),\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                lines=lines,\n                no_cache=no_cache,\n            )\n        else:\n            from black.concurrency import reformat_many\n\n            if lines:\n                err(\"Cannot use --line-ranges to format multiple files.\")\n                ctx.exit(1)\n            reformat_many(\n                sources=sources,\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                workers=workers,\n                no_cache=no_cache,\n            )\n\n    if verbose or not quiet:\n        if code is None and (verbose or report.change_count or report.failure_count):\n            out()\n        out(error_msg if report.return_code else \"All done! âœ¨ ðŸ° âœ¨\")\n        if code is None:\n            click.echo(str(report), err=True)\n    ctx.exit(report.return_code)"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 741,
      "kind": "function",
      "qualname": "src.black.__init__.get_sources",
      "span": [
        734,
        829
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def get_sources(\n    *,\n    root: Path,\n    src: tuple[str, ...],\n    quiet: bool,\n    verbose: bool,\n    include: Pattern[str],\n    exclude: Pattern[str] | None,\n    extend_exclude: Pattern[str] | None,\n    force_exclude: Pattern[str] | None,\n    report: \"Report\",\n    stdin_filename: str | None,\n) -> set[Path]:\n    \"\"\"Compute the set of files to be formatted.\"\"\"\n    sources: set[Path] = set()\n\n    assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n    using_default_exclude = exclude is None\n    exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES) if exclude is None else exclude\n    gitignore: dict[Path, PathSpec] | None = None\n    root_gitignore = get_gitignore(root)\n\n    for s in src:\n        if s == \"-\" and stdin_filename:\n            path = Path(stdin_filename)\n            if path_is_excluded(stdin_filename, force_exclude):\n                report.path_ignored(\n                    path,\n                    \"--stdin-filename matches the --force-exclude regular expression\",\n                )\n                continue\n            is_stdin = True\n        else:\n            path = Path(s)\n            is_stdin = False\n\n        # Compare the logic here to the logic in `gen_python_files`.\n        if is_stdin or path.is_file():\n            if resolves_outside_root_or_cannot_stat(path, root, report):\n                if verbose:\n                    out(f'Skipping invalid source: \"{path}\"', fg=\"red\")\n                continue\n\n            root_relative_path = best_effort_relative_path(path, root).as_posix()\n            root_relative_path = \"/\" + root_relative_path\n\n            # Hard-exclude any files that matches the `--force-exclude` regex.\n            if path_is_excluded(root_relative_path, force_exclude):\n                report.path_ignored(\n                    path, \"matches the --force-exclude regular expression\"\n                )\n                continue\n\n            if is_stdin:\n                path = Path(f\"{STDIN_PLACEHOLDER}{path}\")\n\n            if path.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\n                warn=verbose or not quiet\n            ):\n                continue\n\n            if verbose:\n                out(f'Found input source: \"{path}\"', fg=\"blue\")\n            sources.add(path)\n        elif path.is_dir():\n            path = root / (path.resolve().relative_to(root))\n            if verbose:\n                out(f'Found input source directory: \"{path}\"', fg=\"blue\")\n\n            if using_default_exclude:\n                gitignore = {\n                    root: root_gitignore,\n                    path: get_gitignore(path),\n                }\n            sources.update(\n                gen_python_files(\n                    path.iterdir(),\n                    root,\n                    include,\n                    exclude,\n                    extend_exclude,\n                    force_exclude,\n                    report,\n                    gitignore,\n                    verbose=verbose,\n                    quiet=quiet,\n                )\n            )\n        elif s == \"-\":\n            if verbose:\n                out(\"Found input source stdin\", fg=\"blue\")\n            sources.add(path)\n        else:\n            err(f\"invalid path: {s}\")\n\n    return sources",
      "old_code": "def get_sources(\n    *,\n    root: Path,\n    src: tuple[str, ...],\n    quiet: bool,\n    verbose: bool,\n    include: Pattern[str],\n    exclude: Optional[Pattern[str]],\n    extend_exclude: Optional[Pattern[str]],\n    force_exclude: Optional[Pattern[str]],\n    report: \"Report\",\n    stdin_filename: Optional[str],\n) -> set[Path]:\n    \"\"\"Compute the set of files to be formatted.\"\"\"\n    sources: set[Path] = set()\n\n    assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n    using_default_exclude = exclude is None\n    exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES) if exclude is None else exclude\n    gitignore: Optional[dict[Path, PathSpec]] = None\n    root_gitignore = get_gitignore(root)\n\n    for s in src:\n        if s == \"-\" and stdin_filename:\n            path = Path(stdin_filename)\n            if path_is_excluded(stdin_filename, force_exclude):\n                report.path_ignored(\n                    path,\n                    \"--stdin-filename matches the --force-exclude regular expression\",\n                )\n                continue\n            is_stdin = True\n        else:\n            path = Path(s)\n            is_stdin = False\n\n        # Compare the logic here to the logic in `gen_python_files`.\n        if is_stdin or path.is_file():\n            if resolves_outside_root_or_cannot_stat(path, root, report):\n                if verbose:\n                    out(f'Skipping invalid source: \"{path}\"', fg=\"red\")\n                continue\n\n            root_relative_path = best_effort_relative_path(path, root).as_posix()\n            root_relative_path = \"/\" + root_relative_path\n\n            # Hard-exclude any files that matches the `--force-exclude` regex.\n            if path_is_excluded(root_relative_path, force_exclude):\n                report.path_ignored(\n                    path, \"matches the --force-exclude regular expression\"\n                )\n                continue\n\n            if is_stdin:\n                path = Path(f\"{STDIN_PLACEHOLDER}{path}\")\n\n            if path.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\n                warn=verbose or not quiet\n            ):\n                continue\n\n            if verbose:\n                out(f'Found input source: \"{path}\"', fg=\"blue\")\n            sources.add(path)\n        elif path.is_dir():\n            path = root / (path.resolve().relative_to(root))\n            if verbose:\n                out(f'Found input source directory: \"{path}\"', fg=\"blue\")\n\n            if using_default_exclude:\n                gitignore = {\n                    root: root_gitignore,\n                    path: get_gitignore(path),\n                }\n            sources.update(\n                gen_python_files(\n                    path.iterdir(),\n                    root,\n                    include,\n                    exclude,\n                    extend_exclude,\n                    force_exclude,\n                    report,\n                    gitignore,\n                    verbose=verbose,\n                    quiet=quiet,\n                )\n            )\n        elif s == \"-\":\n            if verbose:\n                out(\"Found input source stdin\", fg=\"blue\")\n            sources.add(path)\n        else:\n            err(f\"invalid path: {s}\")\n\n    return sources"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 997,
      "kind": "function",
      "qualname": "src.black.__init__.format_stdin_to_stdout",
      "span": [
        994,
        1049
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def format_stdin_to_stdout(\n    fast: bool,\n    *,\n    content: str | None = None,\n    write_back: WriteBack = WriteBack.NO,\n    mode: Mode,\n    lines: Collection[tuple[int, int]] = (),\n) -> bool:\n    \"\"\"Format file on stdin. Return True if changed.\n\n    If content is None, it's read from sys.stdin.\n\n    If `write_back` is YES, write reformatted code back to stdout. If it is DIFF,\n    write a diff to stdout. The `mode` argument is passed to\n    :func:`format_file_contents`.\n    \"\"\"\n    then = datetime.now(timezone.utc)\n\n    if content is None:\n        src, encoding, newline = decode_bytes(sys.stdin.buffer.read(), mode)\n    elif Preview.normalize_cr_newlines in mode:\n        src, encoding, newline = content, \"utf-8\", \"\\n\"\n    else:\n        src, encoding, newline = content, \"utf-8\", \"\"\n\n    dst = src\n    try:\n        dst = format_file_contents(src, fast=fast, mode=mode, lines=lines)\n        return True\n\n    except NothingChanged:\n        return False\n\n    finally:\n        f = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=encoding, newline=newline, write_through=True\n        )\n        if write_back == WriteBack.YES:\n            # Make sure there's a newline after the content\n            if Preview.normalize_cr_newlines in mode:\n                if dst and dst[-1] != \"\\n\" and dst[-1] != \"\\r\":\n                    dst += newline\n            else:\n                if dst and dst[-1] != \"\\n\":\n                    dst += \"\\n\"\n            f.write(dst)\n        elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n            now = datetime.now(timezone.utc)\n            src_name = f\"STDIN\\t{then}\"\n            dst_name = f\"STDOUT\\t{now}\"\n            d = diff(src, dst, src_name, dst_name)\n            if write_back == WriteBack.COLOR_DIFF:\n                d = color_diff(d)\n                f = wrap_stream_for_windows(f)\n            f.write(d)\n        f.detach()",
      "old_code": "def format_stdin_to_stdout(\n    fast: bool,\n    *,\n    content: Optional[str] = None,\n    write_back: WriteBack = WriteBack.NO,\n    mode: Mode,\n    lines: Collection[tuple[int, int]] = (),\n) -> bool:\n    \"\"\"Format file on stdin. Return True if changed.\n\n    If content is None, it's read from sys.stdin.\n\n    If `write_back` is YES, write reformatted code back to stdout. If it is DIFF,\n    write a diff to stdout. The `mode` argument is passed to\n    :func:`format_file_contents`.\n    \"\"\"\n    then = datetime.now(timezone.utc)\n\n    if content is None:\n        src, encoding, newline = decode_bytes(sys.stdin.buffer.read(), mode)\n    elif Preview.normalize_cr_newlines in mode:\n        src, encoding, newline = content, \"utf-8\", \"\\n\"\n    else:\n        src, encoding, newline = content, \"utf-8\", \"\"\n\n    dst = src\n    try:\n        dst = format_file_contents(src, fast=fast, mode=mode, lines=lines)\n        return True\n\n    except NothingChanged:\n        return False\n\n    finally:\n        f = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=encoding, newline=newline, write_through=True\n        )\n        if write_back == WriteBack.YES:\n            # Make sure there's a newline after the content\n            if Preview.normalize_cr_newlines in mode:\n                if dst and dst[-1] != \"\\n\" and dst[-1] != \"\\r\":\n                    dst += newline\n            else:\n                if dst and dst[-1] != \"\\n\":\n                    dst += \"\\n\"\n            f.write(dst)\n        elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n            now = datetime.now(timezone.utc)\n            src_name = f\"STDIN\\t{then}\"\n            dst_name = f\"STDOUT\\t{now}\"\n            d = diff(src, dst, src_name, dst_name)\n            if write_back == WriteBack.COLOR_DIFF:\n                d = color_diff(d)\n                f = wrap_stream_for_windows(f)\n            f.write(d)\n        f.detach()"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 1270,
      "kind": "function",
      "qualname": "src.black.__init__._format_str_once",
      "span": [
        1225,
        1299
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _format_str_once(\n    src_contents: str, *, mode: Mode, lines: Collection[tuple[int, int]] = ()\n) -> str:\n    if Preview.normalize_cr_newlines in mode:\n        normalized_contents, _, newline_type = decode_bytes(\n            src_contents.encode(\"utf-8\"), mode\n        )\n\n        src_node = lib2to3_parse(\n            normalized_contents.lstrip(), target_versions=mode.target_versions\n        )\n    else:\n        src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)\n\n    dst_blocks: list[LinesBlock] = []\n    if mode.target_versions:\n        versions = mode.target_versions\n    else:\n        future_imports = get_future_imports(src_node)\n        versions = detect_target_versions(src_node, future_imports=future_imports)\n\n    line_generation_features = {\n        feature\n        for feature in {\n            Feature.PARENTHESIZED_CONTEXT_MANAGERS,\n            Feature.UNPARENTHESIZED_EXCEPT_TYPES,\n            Feature.T_STRINGS,\n        }\n        if supports_feature(versions, feature)\n    }\n    normalize_fmt_off(src_node, mode, lines)\n    if lines:\n        # This should be called after normalize_fmt_off.\n        convert_unchanged_lines(src_node, lines)\n\n    line_generator = LineGenerator(mode=mode, features=line_generation_features)\n    elt = EmptyLineTracker(mode=mode)\n    split_line_features = {\n        feature\n        for feature in {\n            Feature.TRAILING_COMMA_IN_CALL,\n            Feature.TRAILING_COMMA_IN_DEF,\n        }\n        if supports_feature(versions, feature)\n    }\n    block: LinesBlock | None = None\n    for current_line in line_generator.visit(src_node):\n        block = elt.maybe_empty_lines(current_line)\n        dst_blocks.append(block)\n        for line in transform_line(\n            current_line, mode=mode, features=split_line_features\n        ):\n            block.content_lines.append(str(line))\n    if dst_blocks:\n        dst_blocks[-1].after = 0\n    dst_contents = []\n    for block in dst_blocks:\n        dst_contents.extend(block.all_lines())\n    if not dst_contents:\n        if Preview.normalize_cr_newlines in mode:\n            if \"\\n\" in normalized_contents:\n                return newline_type\n        else:\n            # Use decode_bytes to retrieve the correct source newline (CRLF or LF),\n            # and check if normalized_content has more than one line\n            normalized_content, _, newline = decode_bytes(\n                src_contents.encode(\"utf-8\"), mode\n            )\n            if \"\\n\" in normalized_content:\n                return newline\n        return \"\"\n    if Preview.normalize_cr_newlines in mode:\n        return \"\".join(dst_contents).replace(\"\\n\", newline_type)\n    else:\n        return \"\".join(dst_contents)",
      "old_code": "def _format_str_once(\n    src_contents: str, *, mode: Mode, lines: Collection[tuple[int, int]] = ()\n) -> str:\n    if Preview.normalize_cr_newlines in mode:\n        normalized_contents, _, newline_type = decode_bytes(\n            src_contents.encode(\"utf-8\"), mode\n        )\n\n        src_node = lib2to3_parse(\n            normalized_contents.lstrip(), target_versions=mode.target_versions\n        )\n    else:\n        src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)\n\n    dst_blocks: list[LinesBlock] = []\n    if mode.target_versions:\n        versions = mode.target_versions\n    else:\n        future_imports = get_future_imports(src_node)\n        versions = detect_target_versions(src_node, future_imports=future_imports)\n\n    line_generation_features = {\n        feature\n        for feature in {\n            Feature.PARENTHESIZED_CONTEXT_MANAGERS,\n            Feature.UNPARENTHESIZED_EXCEPT_TYPES,\n            Feature.T_STRINGS,\n        }\n        if supports_feature(versions, feature)\n    }\n    normalize_fmt_off(src_node, mode, lines)\n    if lines:\n        # This should be called after normalize_fmt_off.\n        convert_unchanged_lines(src_node, lines)\n\n    line_generator = LineGenerator(mode=mode, features=line_generation_features)\n    elt = EmptyLineTracker(mode=mode)\n    split_line_features = {\n        feature\n        for feature in {\n            Feature.TRAILING_COMMA_IN_CALL,\n            Feature.TRAILING_COMMA_IN_DEF,\n        }\n        if supports_feature(versions, feature)\n    }\n    block: Optional[LinesBlock] = None\n    for current_line in line_generator.visit(src_node):\n        block = elt.maybe_empty_lines(current_line)\n        dst_blocks.append(block)\n        for line in transform_line(\n            current_line, mode=mode, features=split_line_features\n        ):\n            block.content_lines.append(str(line))\n    if dst_blocks:\n        dst_blocks[-1].after = 0\n    dst_contents = []\n    for block in dst_blocks:\n        dst_contents.extend(block.all_lines())\n    if not dst_contents:\n        if Preview.normalize_cr_newlines in mode:\n            if \"\\n\" in normalized_contents:\n                return newline_type\n        else:\n            # Use decode_bytes to retrieve the correct source newline (CRLF or LF),\n            # and check if normalized_content has more than one line\n            normalized_content, _, newline = decode_bytes(\n                src_contents.encode(\"utf-8\"), mode\n            )\n            if \"\\n\" in normalized_content:\n                return newline\n        return \"\"\n    if Preview.normalize_cr_newlines in mode:\n        return \"\".join(dst_contents).replace(\"\\n\", newline_type)\n    else:\n        return \"\".join(dst_contents)"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 1338,
      "kind": "function",
      "qualname": "src.black.__init__.get_features_used",
      "span": [
        1337,
        1496
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def get_features_used(  # noqa: C901\n    node: Node, *, future_imports: set[str] | None = None\n) -> set[Feature]:\n    \"\"\"Return a set of (relatively) new Python features used in this file.\n\n    Currently looking for:\n    - f-strings;\n    - self-documenting expressions in f-strings (f\"{x=}\");\n    - underscores in numeric literals;\n    - trailing commas after * or ** in function signatures and calls;\n    - positional only arguments in function signatures and lambdas;\n    - assignment expression;\n    - relaxed decorator syntax;\n    - usage of __future__ flags (annotations);\n    - print / exec statements;\n    - parenthesized context managers;\n    - match statements;\n    - except* clause;\n    - variadic generics;\n    \"\"\"\n    features: set[Feature] = set()\n    if future_imports:\n        features |= {\n            FUTURE_FLAG_TO_FEATURE[future_import]\n            for future_import in future_imports\n            if future_import in FUTURE_FLAG_TO_FEATURE\n        }\n\n    for n in node.pre_order():\n        if n.type == token.FSTRING_START:\n            features.add(Feature.F_STRINGS)\n        elif n.type == token.TSTRING_START:\n            features.add(Feature.T_STRINGS)\n        elif (\n            n.type == token.RBRACE\n            and n.parent is not None\n            and any(child.type == token.EQUAL for child in n.parent.children)\n        ):\n            features.add(Feature.DEBUG_F_STRINGS)\n\n        elif is_number_token(n):\n            if \"_\" in n.value:\n                features.add(Feature.NUMERIC_UNDERSCORES)\n\n        elif n.type == token.SLASH:\n            if n.parent and n.parent.type in {\n                syms.typedargslist,\n                syms.arglist,\n                syms.varargslist,\n            }:\n                features.add(Feature.POS_ONLY_ARGUMENTS)\n\n        elif n.type == token.COLONEQUAL:\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\n\n        elif n.type == syms.decorator:\n            if len(n.children) > 1 and not is_simple_decorator_expression(\n                n.children[1]\n            ):\n                features.add(Feature.RELAXED_DECORATORS)\n\n        elif (\n            n.type in {syms.typedargslist, syms.arglist}\n            and n.children\n            and n.children[-1].type == token.COMMA\n        ):\n            if n.type == syms.typedargslist:\n                feature = Feature.TRAILING_COMMA_IN_DEF\n            else:\n                feature = Feature.TRAILING_COMMA_IN_CALL\n\n            for ch in n.children:\n                if ch.type in STARS:\n                    features.add(feature)\n\n                if ch.type == syms.argument:\n                    for argch in ch.children:\n                        if argch.type in STARS:\n                            features.add(feature)\n\n        elif (\n            n.type in {syms.return_stmt, syms.yield_expr}\n            and len(n.children) >= 2\n            and n.children[1].type == syms.testlist_star_expr\n            and any(child.type == syms.star_expr for child in n.children[1].children)\n        ):\n            features.add(Feature.UNPACKING_ON_FLOW)\n\n        elif (\n            n.type == syms.annassign\n            and len(n.children) >= 4\n            and n.children[3].type == syms.testlist_star_expr\n        ):\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\n\n        elif (\n            n.type == syms.with_stmt\n            and len(n.children) > 2\n            and n.children[1].type == syms.atom\n        ):\n            atom_children = n.children[1].children\n            if (\n                len(atom_children) == 3\n                and atom_children[0].type == token.LPAR\n                and _contains_asexpr(atom_children[1])\n                and atom_children[2].type == token.RPAR\n            ):\n                features.add(Feature.PARENTHESIZED_CONTEXT_MANAGERS)\n\n        elif n.type == syms.match_stmt:\n            features.add(Feature.PATTERN_MATCHING)\n\n        elif n.type in {syms.subscriptlist, syms.trailer} and any(\n            child.type == syms.star_expr for child in n.children\n        ):\n            features.add(Feature.VARIADIC_GENERICS)\n\n        elif (\n            n.type == syms.tname_star\n            and len(n.children) == 3\n            and n.children[2].type == syms.star_expr\n        ):\n            features.add(Feature.VARIADIC_GENERICS)\n\n        elif n.type in (syms.type_stmt, syms.typeparams):\n            features.add(Feature.TYPE_PARAMS)\n\n        elif (\n            n.type in (syms.typevartuple, syms.paramspec, syms.typevar)\n            and n.children[-2].type == token.EQUAL\n        ):\n            features.add(Feature.TYPE_PARAM_DEFAULTS)\n\n        elif (\n            n.type == syms.except_clause\n            and len(n.children) >= 2\n            and (\n                n.children[1].type == token.STAR or n.children[1].type == syms.testlist\n            )\n        ):\n            is_star_except = n.children[1].type == token.STAR\n\n            if is_star_except:\n                features.add(Feature.EXCEPT_STAR)\n\n            # Presence of except* pushes as clause 1 index back\n            has_as_clause = (\n                len(n.children) >= is_star_except + 3\n                and n.children[is_star_except + 2].type == token.NAME\n                and n.children[is_star_except + 2].value == \"as\"  # type: ignore\n            )\n\n            # If there's no 'as' clause and the except expression is a testlist.\n            if not has_as_clause and (\n                (is_star_except and n.children[2].type == syms.testlist)\n                or (not is_star_except and n.children[1].type == syms.testlist)\n            ):\n                features.add(Feature.UNPARENTHESIZED_EXCEPT_TYPES)\n\n    return features",
      "old_code": "def get_features_used(  # noqa: C901\n    node: Node, *, future_imports: Optional[set[str]] = None\n) -> set[Feature]:\n    \"\"\"Return a set of (relatively) new Python features used in this file.\n\n    Currently looking for:\n    - f-strings;\n    - self-documenting expressions in f-strings (f\"{x=}\");\n    - underscores in numeric literals;\n    - trailing commas after * or ** in function signatures and calls;\n    - positional only arguments in function signatures and lambdas;\n    - assignment expression;\n    - relaxed decorator syntax;\n    - usage of __future__ flags (annotations);\n    - print / exec statements;\n    - parenthesized context managers;\n    - match statements;\n    - except* clause;\n    - variadic generics;\n    \"\"\"\n    features: set[Feature] = set()\n    if future_imports:\n        features |= {\n            FUTURE_FLAG_TO_FEATURE[future_import]\n            for future_import in future_imports\n            if future_import in FUTURE_FLAG_TO_FEATURE\n        }\n\n    for n in node.pre_order():\n        if n.type == token.FSTRING_START:\n            features.add(Feature.F_STRINGS)\n        elif n.type == token.TSTRING_START:\n            features.add(Feature.T_STRINGS)\n        elif (\n            n.type == token.RBRACE\n            and n.parent is not None\n            and any(child.type == token.EQUAL for child in n.parent.children)\n        ):\n            features.add(Feature.DEBUG_F_STRINGS)\n\n        elif is_number_token(n):\n            if \"_\" in n.value:\n                features.add(Feature.NUMERIC_UNDERSCORES)\n\n        elif n.type == token.SLASH:\n            if n.parent and n.parent.type in {\n                syms.typedargslist,\n                syms.arglist,\n                syms.varargslist,\n            }:\n                features.add(Feature.POS_ONLY_ARGUMENTS)\n\n        elif n.type == token.COLONEQUAL:\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\n\n        elif n.type == syms.decorator:\n            if len(n.children) > 1 and not is_simple_decorator_expression(\n                n.children[1]\n            ):\n                features.add(Feature.RELAXED_DECORATORS)\n\n        elif (\n            n.type in {syms.typedargslist, syms.arglist}\n            and n.children\n            and n.children[-1].type == token.COMMA\n        ):\n            if n.type == syms.typedargslist:\n                feature = Feature.TRAILING_COMMA_IN_DEF\n            else:\n                feature = Feature.TRAILING_COMMA_IN_CALL\n\n            for ch in n.children:\n                if ch.type in STARS:\n                    features.add(feature)\n\n                if ch.type == syms.argument:\n                    for argch in ch.children:\n                        if argch.type in STARS:\n                            features.add(feature)\n\n        elif (\n            n.type in {syms.return_stmt, syms.yield_expr}\n            and len(n.children) >= 2\n            and n.children[1].type == syms.testlist_star_expr\n            and any(child.type == syms.star_expr for child in n.children[1].children)\n        ):\n            features.add(Feature.UNPACKING_ON_FLOW)\n\n        elif (\n            n.type == syms.annassign\n            and len(n.children) >= 4\n            and n.children[3].type == syms.testlist_star_expr\n        ):\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\n\n        elif (\n            n.type == syms.with_stmt\n            and len(n.children) > 2\n            and n.children[1].type == syms.atom\n        ):\n            atom_children = n.children[1].children\n            if (\n                len(atom_children) == 3\n                and atom_children[0].type == token.LPAR\n                and _contains_asexpr(atom_children[1])\n                and atom_children[2].type == token.RPAR\n            ):\n                features.add(Feature.PARENTHESIZED_CONTEXT_MANAGERS)\n\n        elif n.type == syms.match_stmt:\n            features.add(Feature.PATTERN_MATCHING)\n\n        elif n.type in {syms.subscriptlist, syms.trailer} and any(\n            child.type == syms.star_expr for child in n.children\n        ):\n            features.add(Feature.VARIADIC_GENERICS)\n\n        elif (\n            n.type == syms.tname_star\n            and len(n.children) == 3\n            and n.children[2].type == syms.star_expr\n        ):\n            features.add(Feature.VARIADIC_GENERICS)\n\n        elif n.type in (syms.type_stmt, syms.typeparams):\n            features.add(Feature.TYPE_PARAMS)\n\n        elif (\n            n.type in (syms.typevartuple, syms.paramspec, syms.typevar)\n            and n.children[-2].type == token.EQUAL\n        ):\n            features.add(Feature.TYPE_PARAM_DEFAULTS)\n\n        elif (\n            n.type == syms.except_clause\n            and len(n.children) >= 2\n            and (\n                n.children[1].type == token.STAR or n.children[1].type == syms.testlist\n            )\n        ):\n            is_star_except = n.children[1].type == token.STAR\n\n            if is_star_except:\n                features.add(Feature.EXCEPT_STAR)\n\n            # Presence of except* pushes as clause 1 index back\n            has_as_clause = (\n                len(n.children) >= is_star_except + 3\n                and n.children[is_star_except + 2].type == token.NAME\n                and n.children[is_star_except + 2].value == \"as\"  # type: ignore\n            )\n\n            # If there's no 'as' clause and the except expression is a testlist.\n            if not has_as_clause and (\n                (is_star_except and n.children[2].type == syms.testlist)\n                or (not is_star_except and n.children[1].type == syms.testlist)\n            ):\n                features.add(Feature.UNPARENTHESIZED_EXCEPT_TYPES)\n\n    return features"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 1499,
      "kind": "function",
      "qualname": "src.black.__init__._contains_asexpr",
      "span": [
        1499,
        1512
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _contains_asexpr(node: Node | Leaf) -> bool:\n    \"\"\"Return True if `node` contains an as-pattern.\"\"\"\n    if node.type == syms.asexpr_test:\n        return True\n    elif node.type == syms.atom:\n        if (\n            len(node.children) == 3\n            and node.children[0].type == token.LPAR\n            and node.children[2].type == token.RPAR\n        ):\n            return _contains_asexpr(node.children[1])\n    elif node.type == syms.testlist_gexp:\n        return any(_contains_asexpr(child) for child in node.children)\n    return False",
      "old_code": "def _contains_asexpr(node: Union[Node, Leaf]) -> bool:\n    \"\"\"Return True if `node` contains an as-pattern.\"\"\"\n    if node.type == syms.asexpr_test:\n        return True\n    elif node.type == syms.atom:\n        if (\n            len(node.children) == 3\n            and node.children[0].type == token.LPAR\n            and node.children[2].type == token.RPAR\n        ):\n            return _contains_asexpr(node.children[1])\n    elif node.type == syms.testlist_gexp:\n        return any(_contains_asexpr(child) for child in node.children)\n    return False"
    },
    {
      "path": "src/black/__init__.py",
      "version": "new",
      "line": 1516,
      "kind": "function",
      "qualname": "src.black.__init__.detect_target_versions",
      "span": [
        1515,
        1522
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def detect_target_versions(\n    node: Node, *, future_imports: set[str] | None = None\n) -> set[TargetVersion]:\n    \"\"\"Detect the version to target based on the nodes used.\"\"\"\n    features = get_features_used(node, future_imports=future_imports)\n    return {\n        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]\n    }",
      "old_code": "def detect_target_versions(\n    node: Node, *, future_imports: Optional[set[str]] = None\n) -> set[TargetVersion]:\n    \"\"\"Detect the version to target based on the nodes used.\"\"\"\n    features = get_features_used(node, future_imports=future_imports)\n    return {\n        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]\n    }"
    },
    {
      "path": "src/black/brackets.py",
      "version": "new",
      "line": 5,
      "kind": "module",
      "qualname": "src.black.brackets",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/brackets.py",
      "version": "new",
      "line": 66,
      "kind": "class",
      "qualname": "src.black.brackets.BracketTracker",
      "span": [
        60,
        216
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class BracketTracker:\n    \"\"\"Keeps track of brackets on a line.\"\"\"\n\n    depth: int = 0\n    bracket_match: dict[tuple[Depth, NodeType], Leaf] = field(default_factory=dict)\n    delimiters: dict[LeafID, Priority] = field(default_factory=dict)\n    previous: Leaf | None = None\n    _for_loop_depths: list[int] = field(default_factory=list)\n    _lambda_argument_depths: list[int] = field(default_factory=list)\n    invisible: list[Leaf] = field(default_factory=list)\n\n    def mark(self, leaf: Leaf) -> None:\n        \"\"\"Mark `leaf` with bracket-related metadata. Keep track of delimiters.\n\n        All leaves receive an int `bracket_depth` field that stores how deep\n        within brackets a given leaf is. 0 means there are no enclosing brackets\n        that started on this line.\n\n        If a leaf is itself a closing bracket and there is a matching opening\n        bracket earlier, it receives an `opening_bracket` field with which it forms a\n        pair. This is a one-directional link to avoid reference cycles. Closing\n        bracket without opening happens on lines continued from previous\n        breaks, e.g. `) -> \"ReturnType\":` as part of a funcdef where we place\n        the return type annotation on its own line of the previous closing RPAR.\n\n        If a leaf is a delimiter (a token on which Black can split the line if\n        needed) and it's on depth 0, its `id()` is stored in the tracker's\n        `delimiters` field.\n        \"\"\"\n        if leaf.type == token.COMMENT:\n            return\n\n        if (\n            self.depth == 0\n            and leaf.type in CLOSING_BRACKETS\n            and (self.depth, leaf.type) not in self.bracket_match\n        ):\n            return\n\n        self.maybe_decrement_after_for_loop_variable(leaf)\n        self.maybe_decrement_after_lambda_arguments(leaf)\n        if leaf.type in CLOSING_BRACKETS:\n            self.depth -= 1\n            try:\n                opening_bracket = self.bracket_match.pop((self.depth, leaf.type))\n            except KeyError as e:\n                raise BracketMatchError(\n                    \"Unable to match a closing bracket to the following opening\"\n                    f\" bracket: {leaf}\"\n                ) from e\n            leaf.opening_bracket = opening_bracket\n            if not leaf.value:\n                self.invisible.append(leaf)\n        leaf.bracket_depth = self.depth\n        if self.depth == 0:\n            delim = is_split_before_delimiter(leaf, self.previous)\n            if delim and self.previous is not None:\n                self.delimiters[id(self.previous)] = delim\n            else:\n                delim = is_split_after_delimiter(leaf)\n                if delim:\n                    self.delimiters[id(leaf)] = delim\n        if leaf.type in OPENING_BRACKETS:\n            self.bracket_match[self.depth, BRACKET[leaf.type]] = leaf\n            self.depth += 1\n            if not leaf.value:\n                self.invisible.append(leaf)\n        self.previous = leaf\n        self.maybe_increment_lambda_arguments(leaf)\n        self.maybe_increment_for_loop_variable(leaf)\n\n    def any_open_for_or_lambda(self) -> bool:\n        \"\"\"Return True if there is an open for or lambda expression on the line.\n\n        See maybe_increment_for_loop_variable and maybe_increment_lambda_arguments\n        for details.\"\"\"\n        return bool(self._for_loop_depths or self._lambda_argument_depths)\n\n    def any_open_brackets(self) -> bool:\n        \"\"\"Return True if there is an yet unmatched open bracket on the line.\"\"\"\n        return bool(self.bracket_match)\n\n    def max_delimiter_priority(self, exclude: Iterable[LeafID] = ()) -> Priority:\n        \"\"\"Return the highest priority of a delimiter found on the line.\n\n        Values are consistent with what `is_split_*_delimiter()` return.\n        Raises ValueError on no delimiters.\n        \"\"\"\n        return max(v for k, v in self.delimiters.items() if k not in exclude)\n\n    def delimiter_count_with_priority(self, priority: Priority = 0) -> int:\n        \"\"\"Return the number of delimiters with the given `priority`.\n\n        If no `priority` is passed, defaults to max priority on the line.\n        \"\"\"\n        if not self.delimiters:\n            return 0\n\n        priority = priority or self.max_delimiter_priority()\n        return sum(1 for p in self.delimiters.values() if p == priority)\n\n    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"In a for loop, or comprehension, the variables are often unpacks.\n\n        To avoid splitting on the comma in this situation, increase the depth of\n        tokens between `for` and `in`.\n        \"\"\"\n        if leaf.type == token.NAME and leaf.value == \"for\":\n            self.depth += 1\n            self._for_loop_depths.append(self.depth)\n            return True\n\n        return False\n\n    def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"See `maybe_increment_for_loop_variable` above for explanation.\"\"\"\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n            self.depth -= 1\n            self._for_loop_depths.pop()\n            return True\n\n        return False\n\n    def maybe_increment_lambda_arguments(self, leaf: Leaf) -> bool:\n        \"\"\"In a lambda expression, there might be more than one argument.\n\n        To avoid splitting on the comma in this situation, increase the depth of\n        tokens between `lambda` and `:`.\n        \"\"\"\n        if leaf.type == token.NAME and leaf.value == \"lambda\":\n            self.depth += 1\n            self._lambda_argument_depths.append(self.depth)\n            return True\n\n        return False\n\n    def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\n        \"\"\"See `maybe_increment_lambda_arguments` above for explanation.\"\"\"\n        if (\n            self._lambda_argument_depths\n            and self._lambda_argument_depths[-1] == self.depth\n            and leaf.type == token.COLON\n        ):\n            self.depth -= 1\n            self._lambda_argument_depths.pop()\n            return True\n\n        return False\n\n    def get_open_lsqb(self) -> Leaf | None:\n        \"\"\"Return the most recent opening square bracket (if any).\"\"\"\n        return self.bracket_match.get((self.depth - 1, token.RSQB))",
      "old_code": "class BracketTracker:\n    \"\"\"Keeps track of brackets on a line.\"\"\"\n\n    depth: int = 0\n    bracket_match: dict[tuple[Depth, NodeType], Leaf] = field(default_factory=dict)\n    delimiters: dict[LeafID, Priority] = field(default_factory=dict)\n    previous: Optional[Leaf] = None\n    _for_loop_depths: list[int] = field(default_factory=list)\n    _lambda_argument_depths: list[int] = field(default_factory=list)\n    invisible: list[Leaf] = field(default_factory=list)\n\n    def mark(self, leaf: Leaf) -> None:\n        \"\"\"Mark `leaf` with bracket-related metadata. Keep track of delimiters.\n\n        All leaves receive an int `bracket_depth` field that stores how deep\n        within brackets a given leaf is. 0 means there are no enclosing brackets\n        that started on this line.\n\n        If a leaf is itself a closing bracket and there is a matching opening\n        bracket earlier, it receives an `opening_bracket` field with which it forms a\n        pair. This is a one-directional link to avoid reference cycles. Closing\n        bracket without opening happens on lines continued from previous\n        breaks, e.g. `) -> \"ReturnType\":` as part of a funcdef where we place\n        the return type annotation on its own line of the previous closing RPAR.\n\n        If a leaf is a delimiter (a token on which Black can split the line if\n        needed) and it's on depth 0, its `id()` is stored in the tracker's\n        `delimiters` field.\n        \"\"\"\n        if leaf.type == token.COMMENT:\n            return\n\n        if (\n            self.depth == 0\n            and leaf.type in CLOSING_BRACKETS\n            and (self.depth, leaf.type) not in self.bracket_match\n        ):\n            return\n\n        self.maybe_decrement_after_for_loop_variable(leaf)\n        self.maybe_decrement_after_lambda_arguments(leaf)\n        if leaf.type in CLOSING_BRACKETS:\n            self.depth -= 1\n            try:\n                opening_bracket = self.bracket_match.pop((self.depth, leaf.type))\n            except KeyError as e:\n                raise BracketMatchError(\n                    \"Unable to match a closing bracket to the following opening\"\n                    f\" bracket: {leaf}\"\n                ) from e\n            leaf.opening_bracket = opening_bracket\n            if not leaf.value:\n                self.invisible.append(leaf)\n        leaf.bracket_depth = self.depth\n        if self.depth == 0:\n            delim = is_split_before_delimiter(leaf, self.previous)\n            if delim and self.previous is not None:\n                self.delimiters[id(self.previous)] = delim\n            else:\n                delim = is_split_after_delimiter(leaf)\n                if delim:\n                    self.delimiters[id(leaf)] = delim\n        if leaf.type in OPENING_BRACKETS:\n            self.bracket_match[self.depth, BRACKET[leaf.type]] = leaf\n            self.depth += 1\n            if not leaf.value:\n                self.invisible.append(leaf)\n        self.previous = leaf\n        self.maybe_increment_lambda_arguments(leaf)\n        self.maybe_increment_for_loop_variable(leaf)\n\n    def any_open_for_or_lambda(self) -> bool:\n        \"\"\"Return True if there is an open for or lambda expression on the line.\n\n        See maybe_increment_for_loop_variable and maybe_increment_lambda_arguments\n        for details.\"\"\"\n        return bool(self._for_loop_depths or self._lambda_argument_depths)\n\n    def any_open_brackets(self) -> bool:\n        \"\"\"Return True if there is an yet unmatched open bracket on the line.\"\"\"\n        return bool(self.bracket_match)\n\n    def max_delimiter_priority(self, exclude: Iterable[LeafID] = ()) -> Priority:\n        \"\"\"Return the highest priority of a delimiter found on the line.\n\n        Values are consistent with what `is_split_*_delimiter()` return.\n        Raises ValueError on no delimiters.\n        \"\"\"\n        return max(v for k, v in self.delimiters.items() if k not in exclude)\n\n    def delimiter_count_with_priority(self, priority: Priority = 0) -> int:\n        \"\"\"Return the number of delimiters with the given `priority`.\n\n        If no `priority` is passed, defaults to max priority on the line.\n        \"\"\"\n        if not self.delimiters:\n            return 0\n\n        priority = priority or self.max_delimiter_priority()\n        return sum(1 for p in self.delimiters.values() if p == priority)\n\n    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"In a for loop, or comprehension, the variables are often unpacks.\n\n        To avoid splitting on the comma in this situation, increase the depth of\n        tokens between `for` and `in`.\n        \"\"\"\n        if leaf.type == token.NAME and leaf.value == \"for\":\n            self.depth += 1\n            self._for_loop_depths.append(self.depth)\n            return True\n\n        return False\n\n    def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"See `maybe_increment_for_loop_variable` above for explanation.\"\"\"\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n            self.depth -= 1\n            self._for_loop_depths.pop()\n            return True\n\n        return False\n\n    def maybe_increment_lambda_arguments(self, leaf: Leaf) -> bool:\n        \"\"\"In a lambda expression, there might be more than one argument.\n\n        To avoid splitting on the comma in this situation, increase the depth of\n        tokens between `lambda` and `:`.\n        \"\"\"\n        if leaf.type == token.NAME and leaf.value == \"lambda\":\n            self.depth += 1\n            self._lambda_argument_depths.append(self.depth)\n            return True\n\n        return False\n\n    def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\n        \"\"\"See `maybe_increment_lambda_arguments` above for explanation.\"\"\"\n        if (\n            self._lambda_argument_depths\n            and self._lambda_argument_depths[-1] == self.depth\n            and leaf.type == token.COLON\n        ):\n            self.depth -= 1\n            self._lambda_argument_depths.pop()\n            return True\n\n        return False\n\n    def get_open_lsqb(self) -> Optional[Leaf]:\n        \"\"\"Return the most recent opening square bracket (if any).\"\"\"\n        return self.bracket_match.get((self.depth - 1, token.RSQB))"
    },
    {
      "path": "src/black/brackets.py",
      "version": "new",
      "line": 214,
      "kind": "function",
      "qualname": "src.black.brackets.BracketTracker.get_open_lsqb",
      "span": [
        214,
        216
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def get_open_lsqb(self) -> Leaf | None:\n        \"\"\"Return the most recent opening square bracket (if any).\"\"\"\n        return self.bracket_match.get((self.depth - 1, token.RSQB))",
      "old_code": "    def get_open_lsqb(self) -> Optional[Leaf]:\n        \"\"\"Return the most recent opening square bracket (if any).\"\"\"\n        return self.bracket_match.get((self.depth - 1, token.RSQB))"
    },
    {
      "path": "src/black/brackets.py",
      "version": "new",
      "line": 233,
      "kind": "function",
      "qualname": "src.black.brackets.is_split_before_delimiter",
      "span": [
        233,
        326
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def is_split_before_delimiter(leaf: Leaf, previous: Leaf | None = None) -> Priority:\n    \"\"\"Return the priority of the `leaf` delimiter, given a line break before it.\n\n    The delimiter priorities returned here are from those delimiters that would\n    cause a line break before themselves.\n\n    Higher numbers are higher priority.\n    \"\"\"\n    if is_vararg(leaf, within=VARARGS_PARENTS | UNPACKING_PARENTS):\n        # * and ** might also be MATH_OPERATORS but in this case they are not.\n        # Don't treat them as a delimiter.\n        return 0\n\n    if (\n        leaf.type == token.DOT\n        and leaf.parent\n        and leaf.parent.type not in {syms.import_from, syms.dotted_name}\n        and (previous is None or previous.type in CLOSING_BRACKETS)\n    ):\n        return DOT_PRIORITY\n\n    if (\n        leaf.type in MATH_OPERATORS\n        and leaf.parent\n        and leaf.parent.type not in {syms.factor, syms.star_expr}\n    ):\n        return MATH_PRIORITIES[leaf.type]\n\n    if leaf.type in COMPARATORS:\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.type == token.STRING\n        and previous is not None\n        and previous.type == token.STRING\n    ):\n        return STRING_PRIORITY\n\n    if leaf.type not in {token.NAME, token.ASYNC}:\n        return 0\n\n    if (\n        leaf.value == \"for\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_for, syms.old_comp_for}\n        or leaf.type == token.ASYNC\n    ):\n        if (\n            not isinstance(leaf.prev_sibling, Leaf)\n            or leaf.prev_sibling.value != \"async\"\n        ):\n            return COMPREHENSION_PRIORITY\n\n    if (\n        leaf.value == \"if\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_if, syms.old_comp_if}\n    ):\n        return COMPREHENSION_PRIORITY\n\n    if leaf.value in {\"if\", \"else\"} and leaf.parent and leaf.parent.type == syms.test:\n        return TERNARY_PRIORITY\n\n    if leaf.value == \"is\":\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.value == \"in\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_op, syms.comparison}\n        and not (\n            previous is not None\n            and previous.type == token.NAME\n            and previous.value == \"not\"\n        )\n    ):\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.value == \"not\"\n        and leaf.parent\n        and leaf.parent.type == syms.comp_op\n        and not (\n            previous is not None\n            and previous.type == token.NAME\n            and previous.value == \"is\"\n        )\n    ):\n        return COMPARATOR_PRIORITY\n\n    if leaf.value in LOGIC_OPERATORS and leaf.parent:\n        return LOGIC_PRIORITY\n\n    return 0",
      "old_code": "def is_split_before_delimiter(leaf: Leaf, previous: Optional[Leaf] = None) -> Priority:\n    \"\"\"Return the priority of the `leaf` delimiter, given a line break before it.\n\n    The delimiter priorities returned here are from those delimiters that would\n    cause a line break before themselves.\n\n    Higher numbers are higher priority.\n    \"\"\"\n    if is_vararg(leaf, within=VARARGS_PARENTS | UNPACKING_PARENTS):\n        # * and ** might also be MATH_OPERATORS but in this case they are not.\n        # Don't treat them as a delimiter.\n        return 0\n\n    if (\n        leaf.type == token.DOT\n        and leaf.parent\n        and leaf.parent.type not in {syms.import_from, syms.dotted_name}\n        and (previous is None or previous.type in CLOSING_BRACKETS)\n    ):\n        return DOT_PRIORITY\n\n    if (\n        leaf.type in MATH_OPERATORS\n        and leaf.parent\n        and leaf.parent.type not in {syms.factor, syms.star_expr}\n    ):\n        return MATH_PRIORITIES[leaf.type]\n\n    if leaf.type in COMPARATORS:\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.type == token.STRING\n        and previous is not None\n        and previous.type == token.STRING\n    ):\n        return STRING_PRIORITY\n\n    if leaf.type not in {token.NAME, token.ASYNC}:\n        return 0\n\n    if (\n        leaf.value == \"for\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_for, syms.old_comp_for}\n        or leaf.type == token.ASYNC\n    ):\n        if (\n            not isinstance(leaf.prev_sibling, Leaf)\n            or leaf.prev_sibling.value != \"async\"\n        ):\n            return COMPREHENSION_PRIORITY\n\n    if (\n        leaf.value == \"if\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_if, syms.old_comp_if}\n    ):\n        return COMPREHENSION_PRIORITY\n\n    if leaf.value in {\"if\", \"else\"} and leaf.parent and leaf.parent.type == syms.test:\n        return TERNARY_PRIORITY\n\n    if leaf.value == \"is\":\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.value == \"in\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_op, syms.comparison}\n        and not (\n            previous is not None\n            and previous.type == token.NAME\n            and previous.value == \"not\"\n        )\n    ):\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.value == \"not\"\n        and leaf.parent\n        and leaf.parent.type == syms.comp_op\n        and not (\n            previous is not None\n            and previous.type == token.NAME\n            and previous.value == \"is\"\n        )\n    ):\n        return COMPARATOR_PRIORITY\n\n    if leaf.value in LOGIC_OPERATORS and leaf.parent:\n        return LOGIC_PRIORITY\n\n    return 0"
    },
    {
      "path": "src/black/comments.py",
      "version": "new",
      "line": 5,
      "kind": "module",
      "qualname": "src.black.comments",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/comments.py",
      "version": "new",
      "line": 417,
      "kind": "function",
      "qualname": "src.black.comments._handle_regular_fmt_block",
      "span": [
        373,
        434
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _handle_regular_fmt_block(\n    ignored_nodes: list[LN],\n    comment: ProtoComment,\n    previous_consumed: int,\n    is_fmt_skip: bool,\n    lines: Collection[tuple[int, int]],\n    leaf: Leaf,\n) -> None:\n    \"\"\"Handle fmt blocks with actual AST nodes.\"\"\"\n    first = ignored_nodes[0]  # Can be a container node with the `leaf`.\n    parent = first.parent\n    prefix = first.prefix\n\n    if comment.value in FMT_OFF:\n        first.prefix = prefix[comment.consumed :]\n    if is_fmt_skip:\n        first.prefix = \"\"\n        standalone_comment_prefix = prefix\n    else:\n        standalone_comment_prefix = prefix[:previous_consumed] + \"\\n\" * comment.newlines\n\n    hidden_value = \"\".join(str(n) for n in ignored_nodes)\n    comment_lineno = leaf.lineno - comment.newlines\n\n    if comment.value in FMT_OFF:\n        fmt_off_prefix = \"\"\n        if len(lines) > 0 and not any(\n            line[0] <= comment_lineno <= line[1] for line in lines\n        ):\n            # keeping indentation of comment by preserving original whitespaces.\n            fmt_off_prefix = prefix.split(comment.value)[0]\n            if \"\\n\" in fmt_off_prefix:\n                fmt_off_prefix = fmt_off_prefix.split(\"\\n\")[-1]\n        standalone_comment_prefix += fmt_off_prefix\n        hidden_value = comment.value + \"\\n\" + hidden_value\n\n    if is_fmt_skip:\n        hidden_value += comment.leading_whitespace + comment.value\n\n    if hidden_value.endswith(\"\\n\"):\n        # That happens when one of the `ignored_nodes` ended with a NEWLINE\n        # leaf (possibly followed by a DEDENT).\n        hidden_value = hidden_value[:-1]\n\n    first_idx: int | None = None\n    for ignored in ignored_nodes:\n        index = ignored.remove()\n        if first_idx is None:\n            first_idx = index\n\n    assert parent is not None, \"INTERNAL ERROR: fmt: on/off handling (1)\"\n    assert first_idx is not None, \"INTERNAL ERROR: fmt: on/off handling (2)\"\n\n    parent.insert_child(\n        first_idx,\n        Leaf(\n            STANDALONE_COMMENT,\n            hidden_value,\n            prefix=standalone_comment_prefix,\n            fmt_pass_converted_first_leaf=first_leaf_of(first),\n        ),\n    )",
      "old_code": "def _handle_regular_fmt_block(\n    ignored_nodes: list[LN],\n    comment: ProtoComment,\n    previous_consumed: int,\n    is_fmt_skip: bool,\n    lines: Collection[tuple[int, int]],\n    leaf: Leaf,\n) -> None:\n    \"\"\"Handle fmt blocks with actual AST nodes.\"\"\"\n    first = ignored_nodes[0]  # Can be a container node with the `leaf`.\n    parent = first.parent\n    prefix = first.prefix\n\n    if comment.value in FMT_OFF:\n        first.prefix = prefix[comment.consumed :]\n    if is_fmt_skip:\n        first.prefix = \"\"\n        standalone_comment_prefix = prefix\n    else:\n        standalone_comment_prefix = prefix[:previous_consumed] + \"\\n\" * comment.newlines\n\n    hidden_value = \"\".join(str(n) for n in ignored_nodes)\n    comment_lineno = leaf.lineno - comment.newlines\n\n    if comment.value in FMT_OFF:\n        fmt_off_prefix = \"\"\n        if len(lines) > 0 and not any(\n            line[0] <= comment_lineno <= line[1] for line in lines\n        ):\n            # keeping indentation of comment by preserving original whitespaces.\n            fmt_off_prefix = prefix.split(comment.value)[0]\n            if \"\\n\" in fmt_off_prefix:\n                fmt_off_prefix = fmt_off_prefix.split(\"\\n\")[-1]\n        standalone_comment_prefix += fmt_off_prefix\n        hidden_value = comment.value + \"\\n\" + hidden_value\n\n    if is_fmt_skip:\n        hidden_value += comment.leading_whitespace + comment.value\n\n    if hidden_value.endswith(\"\\n\"):\n        # That happens when one of the `ignored_nodes` ended with a NEWLINE\n        # leaf (possibly followed by a DEDENT).\n        hidden_value = hidden_value[:-1]\n\n    first_idx: Optional[int] = None\n    for ignored in ignored_nodes:\n        index = ignored.remove()\n        if first_idx is None:\n            first_idx = index\n\n    assert parent is not None, \"INTERNAL ERROR: fmt: on/off handling (1)\"\n    assert first_idx is not None, \"INTERNAL ERROR: fmt: on/off handling (2)\"\n\n    parent.insert_child(\n        first_idx,\n        Leaf(\n            STANDALONE_COMMENT,\n            hidden_value,\n            prefix=standalone_comment_prefix,\n            fmt_pass_converted_first_leaf=first_leaf_of(first),\n        ),\n    )"
    },
    {
      "path": "src/black/comments.py",
      "version": "new",
      "line": 448,
      "kind": "function",
      "qualname": "src.black.comments.generate_ignored_nodes",
      "span": [
        437,
        483
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def generate_ignored_nodes(\n    leaf: Leaf, comment: ProtoComment, mode: Mode\n) -> Iterator[LN]:\n    \"\"\"Starting from the container of `leaf`, generate all leaves until `# fmt: on`.\n\n    If comment is skip, returns leaf only.\n    Stops at the end of the block.\n    \"\"\"\n    if _contains_fmt_directive(comment.value, FMT_SKIP):\n        yield from _generate_ignored_nodes_from_fmt_skip(leaf, comment, mode)\n        return\n    container: LN | None = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n        if is_fmt_on(container, mode=mode):\n            return\n\n        # fix for fmt: on in children\n        if children_contains_fmt_on(container, mode=mode):\n            for index, child in enumerate(container.children):\n                if isinstance(child, Leaf) and is_fmt_on(child, mode=mode):\n                    if child.type in CLOSING_BRACKETS:\n                        # This means `# fmt: on` is placed at a different bracket level\n                        # than `# fmt: off`. This is an invalid use, but as a courtesy,\n                        # we include this closing bracket in the ignored nodes.\n                        # The alternative is to fail the formatting.\n                        yield child\n                    return\n                if (\n                    child.type == token.INDENT\n                    and index < len(container.children) - 1\n                    and children_contains_fmt_on(\n                        container.children[index + 1], mode=mode\n                    )\n                ):\n                    # This means `# fmt: on` is placed right after an indentation\n                    # level, and we shouldn't swallow the previous INDENT token.\n                    return\n                if children_contains_fmt_on(child, mode=mode):\n                    return\n                yield child\n        else:\n            if container.type == token.DEDENT and container.next_sibling is None:\n                # This can happen when there is no matching `# fmt: on` comment at the\n                # same level as `# fmt: on`. We need to keep this DEDENT.\n                return\n            yield container\n            container = container.next_sibling",
      "old_code": "def generate_ignored_nodes(\n    leaf: Leaf, comment: ProtoComment, mode: Mode\n) -> Iterator[LN]:\n    \"\"\"Starting from the container of `leaf`, generate all leaves until `# fmt: on`.\n\n    If comment is skip, returns leaf only.\n    Stops at the end of the block.\n    \"\"\"\n    if _contains_fmt_directive(comment.value, FMT_SKIP):\n        yield from _generate_ignored_nodes_from_fmt_skip(leaf, comment, mode)\n        return\n    container: Optional[LN] = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n        if is_fmt_on(container, mode=mode):\n            return\n\n        # fix for fmt: on in children\n        if children_contains_fmt_on(container, mode=mode):\n            for index, child in enumerate(container.children):\n                if isinstance(child, Leaf) and is_fmt_on(child, mode=mode):\n                    if child.type in CLOSING_BRACKETS:\n                        # This means `# fmt: on` is placed at a different bracket level\n                        # than `# fmt: off`. This is an invalid use, but as a courtesy,\n                        # we include this closing bracket in the ignored nodes.\n                        # The alternative is to fail the formatting.\n                        yield child\n                    return\n                if (\n                    child.type == token.INDENT\n                    and index < len(container.children) - 1\n                    and children_contains_fmt_on(\n                        container.children[index + 1], mode=mode\n                    )\n                ):\n                    # This means `# fmt: on` is placed right after an indentation\n                    # level, and we shouldn't swallow the previous INDENT token.\n                    return\n                if children_contains_fmt_on(child, mode=mode):\n                    return\n                yield child\n        else:\n            if container.type == token.DEDENT and container.next_sibling is None:\n                # This can happen when there is no matching `# fmt: on` comment at the\n                # same level as `# fmt: on`. We need to keep this DEDENT.\n                return\n            yield container\n            container = container.next_sibling"
    },
    {
      "path": "src/black/comments.py",
      "version": "new",
      "line": 486,
      "kind": "function",
      "qualname": "src.black.comments._find_compound_statement_context",
      "span": [
        486,
        524
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _find_compound_statement_context(parent: Node) -> Node | None:\n    \"\"\"Return the body node of a compound statement if we should respect fmt: skip.\n\n    This handles one-line compound statements like:\n        if condition: body  # fmt: skip\n\n    When Black expands such statements, they temporarily look like:\n        if condition:\n            body  # fmt: skip\n\n    In both cases, we want to return the body node (either the simple_stmt directly\n    or the suite containing it).\n    \"\"\"\n    if parent.type != syms.simple_stmt:\n        return None\n\n    if not isinstance(parent.parent, Node):\n        return None\n\n    # Case 1: Expanded form after Black's initial formatting pass.\n    # The one-liner has been split across multiple lines:\n    #     if True:\n    #         print(\"a\"); print(\"b\")  # fmt: skip\n    # Structure: compound_stmt -> suite -> simple_stmt\n    if (\n        parent.parent.type == syms.suite\n        and isinstance(parent.parent.parent, Node)\n        and parent.parent.parent.type in _COMPOUND_STATEMENTS\n    ):\n        return parent.parent\n\n    # Case 2: Original one-line form from the input source.\n    # The statement is still on a single line:\n    #     if True: print(\"a\"); print(\"b\")  # fmt: skip\n    # Structure: compound_stmt -> simple_stmt\n    if parent.parent.type in _COMPOUND_STATEMENTS:\n        return parent\n\n    return None",
      "old_code": "def _find_compound_statement_context(parent: Node) -> Optional[Node]:\n    \"\"\"Return the body node of a compound statement if we should respect fmt: skip.\n\n    This handles one-line compound statements like:\n        if condition: body  # fmt: skip\n\n    When Black expands such statements, they temporarily look like:\n        if condition:\n            body  # fmt: skip\n\n    In both cases, we want to return the body node (either the simple_stmt directly\n    or the suite containing it).\n    \"\"\"\n    if parent.type != syms.simple_stmt:\n        return None\n\n    if not isinstance(parent.parent, Node):\n        return None\n\n    # Case 1: Expanded form after Black's initial formatting pass.\n    # The one-liner has been split across multiple lines:\n    #     if True:\n    #         print(\"a\"); print(\"b\")  # fmt: skip\n    # Structure: compound_stmt -> suite -> simple_stmt\n    if (\n        parent.parent.type == syms.suite\n        and isinstance(parent.parent.parent, Node)\n        and parent.parent.parent.type in _COMPOUND_STATEMENTS\n    ):\n        return parent.parent\n\n    # Case 2: Original one-line form from the input source.\n    # The statement is still on a single line:\n    #     if True: print(\"a\"); print(\"b\")  # fmt: skip\n    # Structure: compound_stmt -> simple_stmt\n    if parent.parent.type in _COMPOUND_STATEMENTS:\n        return parent\n\n    return None"
    },
    {
      "path": "src/black/concurrency.py",
      "version": "new",
      "line": 19,
      "kind": "module",
      "qualname": "src.black.concurrency",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/concurrency.py",
      "version": "new",
      "line": 44,
      "kind": "function",
      "qualname": "src.black.concurrency.cancel",
      "span": [
        44,
        48
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def cancel(tasks: Iterable[asyncio.Future[Any]]) -> None:\n    \"\"\"asyncio signal handler that cancels all `tasks` and reports to stderr.\"\"\"\n    err(\"Aborted!\")\n    for task in tasks:\n        task.cancel()",
      "old_code": "def cancel(tasks: Iterable[\"asyncio.Future[Any]\"]) -> None:\n    \"\"\"asyncio signal handler that cancels all `tasks` and reports to stderr.\"\"\"\n    err(\"Aborted!\")\n    for task in tasks:\n        task.cancel()"
    },
    {
      "path": "src/black/concurrency.py",
      "version": "new",
      "line": 80,
      "kind": "function",
      "qualname": "src.black.concurrency.reformat_many",
      "span": [
        74,
        128
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def reformat_many(\n    sources: set[Path],\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: Report,\n    workers: int | None,\n    no_cache: bool = False,\n) -> None:\n    \"\"\"Reformat multiple files using a ProcessPoolExecutor.\"\"\"\n    maybe_install_uvloop()\n\n    if workers is None:\n        workers = int(os.environ.get(\"BLACK_NUM_WORKERS\", 0))\n        workers = workers or os.cpu_count() or 1\n    if sys.platform == \"win32\":\n        # Work around https://bugs.python.org/issue26903\n        workers = min(workers, 60)\n\n    executor: Executor | None = None\n    if workers > 1:\n        try:\n            executor = ProcessPoolExecutor(max_workers=workers)\n        except (ImportError, NotImplementedError, OSError):\n            # we arrive here if the underlying system does not support multi-processing\n            # like in AWS Lambda or Termux, in which case we gracefully fallback to\n            # a ThreadPoolExecutor with just a single worker (more workers would not do\n            # us any good due to the Global Interpreter Lock)\n            pass\n\n    if executor is None:\n        executor = ThreadPoolExecutor(max_workers=1)\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        loop.run_until_complete(\n            schedule_formatting(\n                sources=sources,\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                loop=loop,\n                executor=executor,\n                no_cache=no_cache,\n            )\n        )\n    finally:\n        try:\n            shutdown(loop)\n        finally:\n            asyncio.set_event_loop(None)\n        if executor is not None:\n            executor.shutdown()",
      "old_code": "def reformat_many(\n    sources: set[Path],\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: Report,\n    workers: Optional[int],\n    no_cache: bool = False,\n) -> None:\n    \"\"\"Reformat multiple files using a ProcessPoolExecutor.\"\"\"\n    maybe_install_uvloop()\n\n    if workers is None:\n        workers = int(os.environ.get(\"BLACK_NUM_WORKERS\", 0))\n        workers = workers or os.cpu_count() or 1\n    if sys.platform == \"win32\":\n        # Work around https://bugs.python.org/issue26903\n        workers = min(workers, 60)\n\n    executor: Executor | None = None\n    if workers > 1:\n        try:\n            executor = ProcessPoolExecutor(max_workers=workers)\n        except (ImportError, NotImplementedError, OSError):\n            # we arrive here if the underlying system does not support multi-processing\n            # like in AWS Lambda or Termux, in which case we gracefully fallback to\n            # a ThreadPoolExecutor with just a single worker (more workers would not do\n            # us any good due to the Global Interpreter Lock)\n            pass\n\n    if executor is None:\n        executor = ThreadPoolExecutor(max_workers=1)\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        loop.run_until_complete(\n            schedule_formatting(\n                sources=sources,\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                loop=loop,\n                executor=executor,\n                no_cache=no_cache,\n            )\n        )\n    finally:\n        try:\n            shutdown(loop)\n        finally:\n            asyncio.set_event_loop(None)\n        if executor is not None:\n            executor.shutdown()"
    },
    {
      "path": "src/black/concurrency.py",
      "version": "new",
      "line": 136,
      "kind": "async_function",
      "qualname": "src.black.concurrency.schedule_formatting",
      "span": [
        131,
        204
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "async def schedule_formatting(\n    sources: set[Path],\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: Report,\n    loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n    no_cache: bool = False,\n) -> None:\n    \"\"\"Run formatting of `sources` in parallel using the provided `executor`.\n\n    (Use ProcessPoolExecutors for actual parallelism.)\n\n    `write_back`, `fast`, and `mode` options are passed to\n    :func:`format_file_in_place`.\n    \"\"\"\n    cache = None if no_cache else Cache.read(mode)\n    if cache is not None and write_back not in (\n        WriteBack.DIFF,\n        WriteBack.COLOR_DIFF,\n    ):\n        sources, cached = cache.filtered_cached(sources)\n        for src in sorted(cached):\n            report.done(src, Changed.CACHED)\n    if not sources:\n        return\n\n    cancelled = []\n    sources_to_cache = []\n    lock = None\n    if write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n        # For diff output, we need locks to ensure we don't interleave output\n        # from different processes.\n        manager = Manager()\n        lock = manager.Lock()\n    tasks = {\n        asyncio.ensure_future(\n            loop.run_in_executor(\n                executor, format_file_in_place, src, fast, mode, write_back, lock\n            )\n        ): src\n        for src in sorted(sources)\n    }\n    pending = tasks.keys()\n    try:\n        loop.add_signal_handler(signal.SIGINT, cancel, pending)\n        loop.add_signal_handler(signal.SIGTERM, cancel, pending)\n    except NotImplementedError:\n        # There are no good alternatives for these on Windows.\n        pass\n    while pending:\n        done, _ = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n        for task in done:\n            src = tasks.pop(task)\n            if task.cancelled():\n                cancelled.append(task)\n            elif exc := task.exception():\n                if report.verbose:\n                    traceback.print_exception(type(exc), exc, exc.__traceback__)\n                report.failed(src, str(exc))\n            else:\n                changed = Changed.YES if task.result() else Changed.NO\n                # If the file was written back or was successfully checked as\n                # well-formatted, store this information in the cache.\n                if write_back is WriteBack.YES or (\n                    write_back is WriteBack.CHECK and changed is Changed.NO\n                ):\n                    sources_to_cache.append(src)\n                report.done(src, changed)\n    if cancelled:\n        await asyncio.gather(*cancelled, return_exceptions=True)\n    if sources_to_cache and not no_cache and cache is not None:\n        cache.write(sources_to_cache)",
      "old_code": "async def schedule_formatting(\n    sources: set[Path],\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: \"Report\",\n    loop: asyncio.AbstractEventLoop,\n    executor: \"Executor\",\n    no_cache: bool = False,\n) -> None:\n    \"\"\"Run formatting of `sources` in parallel using the provided `executor`.\n\n    (Use ProcessPoolExecutors for actual parallelism.)\n\n    `write_back`, `fast`, and `mode` options are passed to\n    :func:`format_file_in_place`.\n    \"\"\"\n    cache = None if no_cache else Cache.read(mode)\n    if cache is not None and write_back not in (\n        WriteBack.DIFF,\n        WriteBack.COLOR_DIFF,\n    ):\n        sources, cached = cache.filtered_cached(sources)\n        for src in sorted(cached):\n            report.done(src, Changed.CACHED)\n    if not sources:\n        return\n\n    cancelled = []\n    sources_to_cache = []\n    lock = None\n    if write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n        # For diff output, we need locks to ensure we don't interleave output\n        # from different processes.\n        manager = Manager()\n        lock = manager.Lock()\n    tasks = {\n        asyncio.ensure_future(\n            loop.run_in_executor(\n                executor, format_file_in_place, src, fast, mode, write_back, lock\n            )\n        ): src\n        for src in sorted(sources)\n    }\n    pending = tasks.keys()\n    try:\n        loop.add_signal_handler(signal.SIGINT, cancel, pending)\n        loop.add_signal_handler(signal.SIGTERM, cancel, pending)\n    except NotImplementedError:\n        # There are no good alternatives for these on Windows.\n        pass\n    while pending:\n        done, _ = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n        for task in done:\n            src = tasks.pop(task)\n            if task.cancelled():\n                cancelled.append(task)\n            elif exc := task.exception():\n                if report.verbose:\n                    traceback.print_exception(type(exc), exc, exc.__traceback__)\n                report.failed(src, str(exc))\n            else:\n                changed = Changed.YES if task.result() else Changed.NO\n                # If the file was written back or was successfully checked as\n                # well-formatted, store this information in the cache.\n                if write_back is WriteBack.YES or (\n                    write_back is WriteBack.CHECK and changed is Changed.NO\n                ):\n                    sources_to_cache.append(src)\n                report.done(src, changed)\n    if cancelled:\n        await asyncio.gather(*cancelled, return_exceptions=True)\n    if sources_to_cache and not no_cache and cache is not None:\n        cache.write(sources_to_cache)"
    },
    {
      "path": "src/black/debug.py",
      "version": "new",
      "line": 47,
      "kind": "function",
      "qualname": "src.black.debug.DebugVisitor.show",
      "span": [
        47,
        55
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def show(cls, code: str | Leaf | Node) -> None:\n        \"\"\"Pretty-print the lib2to3 AST of a given string of `code`.\n\n        Convenience method for debugging.\n        \"\"\"\n        v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))",
      "old_code": "    def show(cls, code: Union[str, Leaf, Node]) -> None:\n        \"\"\"Pretty-print the lib2to3 AST of a given string of `code`.\n\n        Convenience method for debugging.\n        \"\"\"\n        v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 8,
      "kind": "module",
      "qualname": "src.black.files",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 36,
      "kind": "function",
      "qualname": "src.black.files._load_toml",
      "span": [
        36,
        38
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _load_toml(path: Path | str) -> dict[str, Any]:\n    with open(path, \"rb\") as f:\n        return tomllib.load(f)",
      "old_code": "def _load_toml(path: Union[Path, str]) -> dict[str, Any]:\n    with open(path, \"rb\") as f:\n        return tomllib.load(f)"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 48,
      "kind": "function",
      "qualname": "src.black.files.find_project_root",
      "span": [
        47,
        95
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def find_project_root(\n    srcs: Sequence[str], stdin_filename: str | None = None\n) -> tuple[Path, str]:\n    \"\"\"Return a directory containing .git, .hg, or pyproject.toml.\n\n    pyproject.toml files are only considered if they contain a [tool.black]\n    section and are ignored otherwise.\n\n    That directory will be a common parent of all files and directories\n    passed in `srcs`.\n\n    If no directory in the tree contains a marker that would specify it's the\n    project root, the root of the file system is returned.\n\n    Returns a two-tuple with the first element as the project root path and\n    the second element as a string describing the method by which the\n    project root was discovered.\n    \"\"\"\n    if stdin_filename is not None:\n        srcs = tuple(stdin_filename if s == \"-\" else s for s in srcs)\n    if not srcs:\n        srcs = [str(_cached_resolve(Path.cwd()))]\n\n    path_srcs = [_cached_resolve(Path(Path.cwd(), src)) for src in srcs]\n\n    # A list of lists of parents for each 'src'. 'src' is included as a\n    # \"parent\" of itself if it is a directory\n    src_parents = [\n        list(path.parents) + ([path] if path.is_dir() else []) for path in path_srcs\n    ]\n\n    common_base = max(\n        set.intersection(*(set(parents) for parents in src_parents)),\n        key=lambda path: path.parts,\n    )\n\n    for directory in (common_base, *common_base.parents):\n        if (directory / \".git\").exists():\n            return directory, \".git directory\"\n\n        if (directory / \".hg\").is_dir():\n            return directory, \".hg directory\"\n\n        if (directory / \"pyproject.toml\").is_file():\n            pyproject_toml = _load_toml(directory / \"pyproject.toml\")\n            if \"black\" in pyproject_toml.get(\"tool\", {}):\n                return directory, \"pyproject.toml\"\n\n    return directory, \"file system root\"",
      "old_code": "def find_project_root(\n    srcs: Sequence[str], stdin_filename: Optional[str] = None\n) -> tuple[Path, str]:\n    \"\"\"Return a directory containing .git, .hg, or pyproject.toml.\n\n    pyproject.toml files are only considered if they contain a [tool.black]\n    section and are ignored otherwise.\n\n    That directory will be a common parent of all files and directories\n    passed in `srcs`.\n\n    If no directory in the tree contains a marker that would specify it's the\n    project root, the root of the file system is returned.\n\n    Returns a two-tuple with the first element as the project root path and\n    the second element as a string describing the method by which the\n    project root was discovered.\n    \"\"\"\n    if stdin_filename is not None:\n        srcs = tuple(stdin_filename if s == \"-\" else s for s in srcs)\n    if not srcs:\n        srcs = [str(_cached_resolve(Path.cwd()))]\n\n    path_srcs = [_cached_resolve(Path(Path.cwd(), src)) for src in srcs]\n\n    # A list of lists of parents for each 'src'. 'src' is included as a\n    # \"parent\" of itself if it is a directory\n    src_parents = [\n        list(path.parents) + ([path] if path.is_dir() else []) for path in path_srcs\n    ]\n\n    common_base = max(\n        set.intersection(*(set(parents) for parents in src_parents)),\n        key=lambda path: path.parts,\n    )\n\n    for directory in (common_base, *common_base.parents):\n        if (directory / \".git\").exists():\n            return directory, \".git directory\"\n\n        if (directory / \".hg\").is_dir():\n            return directory, \".hg directory\"\n\n        if (directory / \"pyproject.toml\").is_file():\n            pyproject_toml = _load_toml(directory / \"pyproject.toml\")\n            if \"black\" in pyproject_toml.get(\"tool\", {}):\n                return directory, \"pyproject.toml\"\n\n    return directory, \"file system root\""
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 99,
      "kind": "function",
      "qualname": "src.black.files.find_pyproject_toml",
      "span": [
        98,
        117
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def find_pyproject_toml(\n    path_search_start: tuple[str, ...], stdin_filename: str | None = None\n) -> str | None:\n    \"\"\"Find the absolute filepath to a pyproject.toml if it exists\"\"\"\n    path_project_root, _ = find_project_root(path_search_start, stdin_filename)\n    path_pyproject_toml = path_project_root / \"pyproject.toml\"\n    if path_pyproject_toml.is_file():\n        return str(path_pyproject_toml)\n\n    try:\n        path_user_pyproject_toml = find_user_pyproject_toml()\n        return (\n            str(path_user_pyproject_toml)\n            if path_user_pyproject_toml.is_file()\n            else None\n        )\n    except (PermissionError, RuntimeError) as e:\n        # We do not have access to the user-level config directory, so ignore it.\n        err(f\"Ignoring user configuration directory due to {e!r}\")\n        return None",
      "old_code": "def find_pyproject_toml(\n    path_search_start: tuple[str, ...], stdin_filename: Optional[str] = None\n) -> Optional[str]:\n    \"\"\"Find the absolute filepath to a pyproject.toml if it exists\"\"\"\n    path_project_root, _ = find_project_root(path_search_start, stdin_filename)\n    path_pyproject_toml = path_project_root / \"pyproject.toml\"\n    if path_pyproject_toml.is_file():\n        return str(path_pyproject_toml)\n\n    try:\n        path_user_pyproject_toml = find_user_pyproject_toml()\n        return (\n            str(path_user_pyproject_toml)\n            if path_user_pyproject_toml.is_file()\n            else None\n        )\n    except (PermissionError, RuntimeError) as e:\n        # We do not have access to the user-level config directory, so ignore it.\n        err(f\"Ignoring user configuration directory due to {e!r}\")\n        return None"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 140,
      "kind": "function",
      "qualname": "src.black.files.infer_target_version",
      "span": [
        138,
        160
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def infer_target_version(\n    pyproject_toml: dict[str, Any],\n) -> list[TargetVersion] | None:\n    \"\"\"Infer Black's target version from the project metadata in pyproject.toml.\n\n    Supports the PyPA standard format (PEP 621):\n    https://packaging.python.org/en/latest/specifications/declaring-project-metadata/#requires-python\n\n    If the target version cannot be inferred, returns None.\n    \"\"\"\n    project_metadata = pyproject_toml.get(\"project\", {})\n    requires_python = project_metadata.get(\"requires-python\", None)\n    if requires_python is not None:\n        try:\n            return parse_req_python_version(requires_python)\n        except InvalidVersion:\n            pass\n        try:\n            return parse_req_python_specifier(requires_python)\n        except (InvalidSpecifier, InvalidVersion):\n            pass\n\n    return None",
      "old_code": "def infer_target_version(\n    pyproject_toml: dict[str, Any],\n) -> Optional[list[TargetVersion]]:\n    \"\"\"Infer Black's target version from the project metadata in pyproject.toml.\n\n    Supports the PyPA standard format (PEP 621):\n    https://packaging.python.org/en/latest/specifications/declaring-project-metadata/#requires-python\n\n    If the target version cannot be inferred, returns None.\n    \"\"\"\n    project_metadata = pyproject_toml.get(\"project\", {})\n    requires_python = project_metadata.get(\"requires-python\", None)\n    if requires_python is not None:\n        try:\n            return parse_req_python_version(requires_python)\n        except InvalidVersion:\n            pass\n        try:\n            return parse_req_python_specifier(requires_python)\n        except (InvalidSpecifier, InvalidVersion):\n            pass\n\n    return None"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 163,
      "kind": "function",
      "qualname": "src.black.files.parse_req_python_version",
      "span": [
        163,
        175
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def parse_req_python_version(requires_python: str) -> list[TargetVersion] | None:\n    \"\"\"Parse a version string (i.e. ``\"3.7\"``) to a list of TargetVersion.\n\n    If parsing fails, will raise a packaging.version.InvalidVersion error.\n    If the parsed version cannot be mapped to a valid TargetVersion, returns None.\n    \"\"\"\n    version = Version(requires_python)\n    if version.release[0] != 3:\n        return None\n    try:\n        return [TargetVersion(version.release[1])]\n    except (IndexError, ValueError):\n        return None",
      "old_code": "def parse_req_python_version(requires_python: str) -> Optional[list[TargetVersion]]:\n    \"\"\"Parse a version string (i.e. ``\"3.7\"``) to a list of TargetVersion.\n\n    If parsing fails, will raise a packaging.version.InvalidVersion error.\n    If the parsed version cannot be mapped to a valid TargetVersion, returns None.\n    \"\"\"\n    version = Version(requires_python)\n    if version.release[0] != 3:\n        return None\n    try:\n        return [TargetVersion(version.release[1])]\n    except (IndexError, ValueError):\n        return None"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 178,
      "kind": "function",
      "qualname": "src.black.files.parse_req_python_specifier",
      "span": [
        178,
        192
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def parse_req_python_specifier(requires_python: str) -> list[TargetVersion] | None:\n    \"\"\"Parse a specifier string (i.e. ``\">=3.7,<3.10\"``) to a list of TargetVersion.\n\n    If parsing fails, will raise a packaging.specifiers.InvalidSpecifier error.\n    If the parsed specifier cannot be mapped to a valid TargetVersion, returns None.\n    \"\"\"\n    specifier_set = strip_specifier_set(SpecifierSet(requires_python))\n    if not specifier_set:\n        return None\n\n    target_version_map = {f\"3.{v.value}\": v for v in TargetVersion}\n    compatible_versions: list[str] = list(specifier_set.filter(target_version_map))\n    if compatible_versions:\n        return [target_version_map[v] for v in compatible_versions]\n    return None",
      "old_code": "def parse_req_python_specifier(requires_python: str) -> Optional[list[TargetVersion]]:\n    \"\"\"Parse a specifier string (i.e. ``\">=3.7,<3.10\"``) to a list of TargetVersion.\n\n    If parsing fails, will raise a packaging.specifiers.InvalidSpecifier error.\n    If the parsed specifier cannot be mapped to a valid TargetVersion, returns None.\n    \"\"\"\n    specifier_set = strip_specifier_set(SpecifierSet(requires_python))\n    if not specifier_set:\n        return None\n\n    target_version_map = {f\"3.{v.value}\": v for v in TargetVersion}\n    compatible_versions: list[str] = list(specifier_set.filter(target_version_map))\n    if compatible_versions:\n        return [target_version_map[v] for v in compatible_versions]\n    return None"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 258,
      "kind": "function",
      "qualname": "src.black.files.resolves_outside_root_or_cannot_stat",
      "span": [
        255,
        276
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def resolves_outside_root_or_cannot_stat(\n    path: Path,\n    root: Path,\n    report: Report | None = None,\n) -> bool:\n    \"\"\"\n    Returns whether the path is a symbolic link that points outside the\n    root directory. Also returns True if we failed to resolve the path.\n    \"\"\"\n    try:\n        resolved_path = _cached_resolve(path)\n    except OSError as e:\n        if report:\n            report.path_ignored(path, f\"cannot be read because {e}\")\n        return True\n    try:\n        resolved_path.relative_to(root)\n    except ValueError:\n        if report:\n            report.path_ignored(path, f\"is a symbolic link that points outside {root}\")\n        return True\n    return False",
      "old_code": "def resolves_outside_root_or_cannot_stat(\n    path: Path,\n    root: Path,\n    report: Optional[Report] = None,\n) -> bool:\n    \"\"\"\n    Returns whether the path is a symbolic link that points outside the\n    root directory. Also returns True if we failed to resolve the path.\n    \"\"\"\n    try:\n        resolved_path = _cached_resolve(path)\n    except OSError as e:\n        if report:\n            report.path_ignored(path, f\"cannot be read because {e}\")\n        return True\n    try:\n        resolved_path.relative_to(root)\n    except ValueError:\n        if report:\n            report.path_ignored(path, f\"is a symbolic link that points outside {root}\")\n        return True\n    return False"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 314,
      "kind": "function",
      "qualname": "src.black.files.path_is_excluded",
      "span": [
        312,
        317
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def path_is_excluded(\n    normalized_path: str,\n    pattern: Pattern[str] | None,\n) -> bool:\n    match = pattern.search(normalized_path) if pattern else None\n    return bool(match and match.group(0))",
      "old_code": "def path_is_excluded(\n    normalized_path: str,\n    pattern: Optional[Pattern[str]],\n) -> bool:\n    match = pattern.search(normalized_path) if pattern else None\n    return bool(match and match.group(0))"
    },
    {
      "path": "src/black/files.py",
      "version": "new",
      "line": 325,
      "kind": "function",
      "qualname": "src.black.files.gen_python_files",
      "span": [
        320,
        406
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def gen_python_files(\n    paths: Iterable[Path],\n    root: Path,\n    include: Pattern[str],\n    exclude: Pattern[str],\n    extend_exclude: Pattern[str] | None,\n    force_exclude: Pattern[str] | None,\n    report: Report,\n    gitignore_dict: dict[Path, PathSpec] | None,\n    *,\n    verbose: bool,\n    quiet: bool,\n) -> Iterator[Path]:\n    \"\"\"Generate all files under `path` whose paths are not excluded by the\n    `exclude_regex`, `extend_exclude`, or `force_exclude` regexes,\n    but are included by the `include` regex.\n\n    Symbolic links pointing outside of the `root` directory are ignored.\n\n    `report` is where output about exclusions goes.\n    \"\"\"\n\n    assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n    for child in paths:\n        assert child.is_absolute()\n        root_relative_path = child.relative_to(root).as_posix()\n\n        # First ignore files matching .gitignore, if passed\n        if gitignore_dict and _path_is_ignored(\n            root_relative_path, root, gitignore_dict\n        ):\n            report.path_ignored(child, \"matches a .gitignore file content\")\n            continue\n\n        # Then ignore with `--exclude` `--extend-exclude` and `--force-exclude` options.\n        root_relative_path = \"/\" + root_relative_path\n        if child.is_dir():\n            root_relative_path += \"/\"\n\n        if path_is_excluded(root_relative_path, exclude):\n            report.path_ignored(child, \"matches the --exclude regular expression\")\n            continue\n\n        if path_is_excluded(root_relative_path, extend_exclude):\n            report.path_ignored(\n                child, \"matches the --extend-exclude regular expression\"\n            )\n            continue\n\n        if path_is_excluded(root_relative_path, force_exclude):\n            report.path_ignored(child, \"matches the --force-exclude regular expression\")\n            continue\n\n        if resolves_outside_root_or_cannot_stat(child, root, report):\n            continue\n\n        if child.is_dir():\n            # If gitignore is None, gitignore usage is disabled, while a Falsey\n            # gitignore is when the directory doesn't have a .gitignore file.\n            if gitignore_dict is not None:\n                new_gitignore_dict = {\n                    **gitignore_dict,\n                    root / child: get_gitignore(child),\n                }\n            else:\n                new_gitignore_dict = None\n            yield from gen_python_files(\n                child.iterdir(),\n                root,\n                include,\n                exclude,\n                extend_exclude,\n                force_exclude,\n                report,\n                new_gitignore_dict,\n                verbose=verbose,\n                quiet=quiet,\n            )\n\n        elif child.is_file():\n            if child.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\n                warn=verbose or not quiet\n            ):\n                continue\n            include_match = include.search(root_relative_path) if include else True\n            if include_match:\n                yield child",
      "old_code": "def gen_python_files(\n    paths: Iterable[Path],\n    root: Path,\n    include: Pattern[str],\n    exclude: Pattern[str],\n    extend_exclude: Optional[Pattern[str]],\n    force_exclude: Optional[Pattern[str]],\n    report: Report,\n    gitignore_dict: Optional[dict[Path, PathSpec]],\n    *,\n    verbose: bool,\n    quiet: bool,\n) -> Iterator[Path]:\n    \"\"\"Generate all files under `path` whose paths are not excluded by the\n    `exclude_regex`, `extend_exclude`, or `force_exclude` regexes,\n    but are included by the `include` regex.\n\n    Symbolic links pointing outside of the `root` directory are ignored.\n\n    `report` is where output about exclusions goes.\n    \"\"\"\n\n    assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n    for child in paths:\n        assert child.is_absolute()\n        root_relative_path = child.relative_to(root).as_posix()\n\n        # First ignore files matching .gitignore, if passed\n        if gitignore_dict and _path_is_ignored(\n            root_relative_path, root, gitignore_dict\n        ):\n            report.path_ignored(child, \"matches a .gitignore file content\")\n            continue\n\n        # Then ignore with `--exclude` `--extend-exclude` and `--force-exclude` options.\n        root_relative_path = \"/\" + root_relative_path\n        if child.is_dir():\n            root_relative_path += \"/\"\n\n        if path_is_excluded(root_relative_path, exclude):\n            report.path_ignored(child, \"matches the --exclude regular expression\")\n            continue\n\n        if path_is_excluded(root_relative_path, extend_exclude):\n            report.path_ignored(\n                child, \"matches the --extend-exclude regular expression\"\n            )\n            continue\n\n        if path_is_excluded(root_relative_path, force_exclude):\n            report.path_ignored(child, \"matches the --force-exclude regular expression\")\n            continue\n\n        if resolves_outside_root_or_cannot_stat(child, root, report):\n            continue\n\n        if child.is_dir():\n            # If gitignore is None, gitignore usage is disabled, while a Falsey\n            # gitignore is when the directory doesn't have a .gitignore file.\n            if gitignore_dict is not None:\n                new_gitignore_dict = {\n                    **gitignore_dict,\n                    root / child: get_gitignore(child),\n                }\n            else:\n                new_gitignore_dict = None\n            yield from gen_python_files(\n                child.iterdir(),\n                root,\n                include,\n                exclude,\n                extend_exclude,\n                force_exclude,\n                report,\n                new_gitignore_dict,\n                verbose=verbose,\n                quiet=quiet,\n            )\n\n        elif child.is_file():\n            if child.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\n                warn=verbose or not quiet\n            ):\n                continue\n            include_match = include.search(root_relative_path) if include else True\n            if include_match:\n                yield child"
    },
    {
      "path": "src/black/handle_ipynb_magics.py",
      "version": "new",
      "line": 10,
      "kind": "module",
      "qualname": "src.black.handle_ipynb_magics",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/handle_ipynb_magics.py",
      "version": "new",
      "line": 355,
      "kind": "class",
      "qualname": "src.black.handle_ipynb_magics.CellMagic",
      "span": [
        353,
        362
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class CellMagic:\n    name: str\n    params: str | None\n    body: str\n\n    @property\n    def header(self) -> str:\n        if self.params:\n            return f\"%%{self.name} {self.params}\"\n        return f\"%%{self.name}\"",
      "old_code": "class CellMagic:\n    name: str\n    params: Optional[str]\n    body: str\n\n    @property\n    def header(self) -> str:\n        if self.params:\n            return f\"%%{self.name} {self.params}\"\n        return f\"%%{self.name}\""
    },
    {
      "path": "src/black/handle_ipynb_magics.py",
      "version": "new",
      "line": 385,
      "kind": "function",
      "qualname": "src.black.handle_ipynb_magics.CellMagicFinder.__init__",
      "span": [
        385,
        386
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(self, cell_magic: CellMagic | None = None) -> None:\n        self.cell_magic = cell_magic",
      "old_code": "    def __init__(self, cell_magic: Optional[CellMagic] = None) -> None:\n        self.cell_magic = cell_magic"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 11,
      "kind": "module",
      "qualname": "src.black.linegen",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 306,
      "kind": "function",
      "qualname": "src.black.linegen.LineGenerator.visit_simple_stmt",
      "span": [
        304,
        326
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def visit_simple_stmt(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit a statement without nested statements.\"\"\"\n        prev_type: int | None = None\n        for child in node.children:\n            if (prev_type is None or prev_type == token.SEMI) and is_arith_like(child):\n                wrap_in_parentheses(node, child, visible=False)\n            prev_type = child.type\n\n        if node.parent and node.parent.type in STATEMENT:\n            if is_parent_function_or_class(node) and is_stub_body(node):\n                yield from self.visit_default(node)\n            else:\n                yield from self.line(+1)\n                yield from self.visit_default(node)\n                yield from self.line(-1)\n\n        else:\n            if node.parent and is_stub_suite(node.parent):\n                node.prefix = \"\"\n                yield from self.visit_default(node)\n                return\n            yield from self.line()\n            yield from self.visit_default(node)",
      "old_code": "    def visit_simple_stmt(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit a statement without nested statements.\"\"\"\n        prev_type: Optional[int] = None\n        for child in node.children:\n            if (prev_type is None or prev_type == token.SEMI) and is_arith_like(child):\n                wrap_in_parentheses(node, child, visible=False)\n            prev_type = child.type\n\n        if node.parent and node.parent.type in STATEMENT:\n            if is_parent_function_or_class(node) and is_stub_body(node):\n                yield from self.visit_default(node)\n            else:\n                yield from self.line(+1)\n                yield from self.visit_default(node)\n                yield from self.line(-1)\n\n        else:\n            if node.parent and is_stub_suite(node.parent):\n                node.prefix = \"\"\n                yield from self.visit_default(node)\n                return\n            yield from self.line()\n            yield from self.visit_default(node)"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 670,
      "kind": "function",
      "qualname": "src.black.linegen._hugging_power_ops_line_to_string",
      "span": [
        666,
        674
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _hugging_power_ops_line_to_string(\n    line: Line,\n    features: Collection[Feature],\n    mode: Mode,\n) -> str | None:\n    try:\n        return line_to_string(next(hug_power_op(line, features, mode)))\n    except CannotTransform:\n        return None",
      "old_code": "def _hugging_power_ops_line_to_string(\n    line: Line,\n    features: Collection[Feature],\n    mode: Mode,\n) -> Optional[str]:\n    try:\n        return line_to_string(next(hug_power_op(line, features, mode)))\n    except CannotTransform:\n        return None"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 852,
      "kind": "function",
      "qualname": "src.black.linegen.left_hand_split",
      "span": [
        838,
        902
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def left_hand_split(\n    line: Line, _features: Collection[Feature], mode: Mode\n) -> Iterator[Line]:\n    \"\"\"Split line into many lines, starting with the first matching bracket pair.\n\n    Note: this usually looks weird, only use this for function definitions.\n    Prefer RHS otherwise.  This is why this function is not symmetrical with\n    :func:`right_hand_split` which also handles optional parentheses.\n    \"\"\"\n    for leaf_type in [token.LPAR, token.LSQB]:\n        tail_leaves: list[Leaf] = []\n        body_leaves: list[Leaf] = []\n        head_leaves: list[Leaf] = []\n        current_leaves = head_leaves\n        matching_bracket: Leaf | None = None\n        depth = 0\n        for index, leaf in enumerate(line.leaves):\n            if index == 2 and leaf.type == token.LSQB:\n                # A [ at index 2 means this is a type param, so start\n                # tracking the depth\n                depth += 1\n            elif depth > 0:\n                if leaf.type == token.LSQB:\n                    depth += 1\n                elif leaf.type == token.RSQB:\n                    depth -= 1\n            if (\n                current_leaves is body_leaves\n                and leaf.type in CLOSING_BRACKETS\n                and leaf.opening_bracket is matching_bracket\n                and isinstance(matching_bracket, Leaf)\n                # If the code is still on LPAR and we are inside a type\n                # param, ignore the match since this is searching\n                # for the function arguments\n                and not (leaf_type == token.LPAR and depth > 0)\n            ):\n                ensure_visible(leaf)\n                ensure_visible(matching_bracket)\n                current_leaves = tail_leaves if body_leaves else head_leaves\n            current_leaves.append(leaf)\n            if current_leaves is head_leaves:\n                if leaf.type == leaf_type and (\n                    Preview.fix_type_expansion_split not in mode\n                    or not (leaf_type == token.LPAR and depth > 0)\n                ):\n                    matching_bracket = leaf\n                    current_leaves = body_leaves\n        if matching_bracket and tail_leaves:\n            break\n    if not matching_bracket or not tail_leaves:\n        raise CannotSplit(\"No brackets found\")\n\n    head = bracket_split_build_line(\n        head_leaves, line, matching_bracket, component=_BracketSplitComponent.head\n    )\n    body = bracket_split_build_line(\n        body_leaves, line, matching_bracket, component=_BracketSplitComponent.body\n    )\n    tail = bracket_split_build_line(\n        tail_leaves, line, matching_bracket, component=_BracketSplitComponent.tail\n    )\n    bracket_split_succeeded_or_raise(head, body, tail)\n    for result in (head, body, tail):\n        if result:\n            yield result",
      "old_code": "def left_hand_split(\n    line: Line, _features: Collection[Feature], mode: Mode\n) -> Iterator[Line]:\n    \"\"\"Split line into many lines, starting with the first matching bracket pair.\n\n    Note: this usually looks weird, only use this for function definitions.\n    Prefer RHS otherwise.  This is why this function is not symmetrical with\n    :func:`right_hand_split` which also handles optional parentheses.\n    \"\"\"\n    for leaf_type in [token.LPAR, token.LSQB]:\n        tail_leaves: list[Leaf] = []\n        body_leaves: list[Leaf] = []\n        head_leaves: list[Leaf] = []\n        current_leaves = head_leaves\n        matching_bracket: Optional[Leaf] = None\n        depth = 0\n        for index, leaf in enumerate(line.leaves):\n            if index == 2 and leaf.type == token.LSQB:\n                # A [ at index 2 means this is a type param, so start\n                # tracking the depth\n                depth += 1\n            elif depth > 0:\n                if leaf.type == token.LSQB:\n                    depth += 1\n                elif leaf.type == token.RSQB:\n                    depth -= 1\n            if (\n                current_leaves is body_leaves\n                and leaf.type in CLOSING_BRACKETS\n                and leaf.opening_bracket is matching_bracket\n                and isinstance(matching_bracket, Leaf)\n                # If the code is still on LPAR and we are inside a type\n                # param, ignore the match since this is searching\n                # for the function arguments\n                and not (leaf_type == token.LPAR and depth > 0)\n            ):\n                ensure_visible(leaf)\n                ensure_visible(matching_bracket)\n                current_leaves = tail_leaves if body_leaves else head_leaves\n            current_leaves.append(leaf)\n            if current_leaves is head_leaves:\n                if leaf.type == leaf_type and (\n                    Preview.fix_type_expansion_split not in mode\n                    or not (leaf_type == token.LPAR and depth > 0)\n                ):\n                    matching_bracket = leaf\n                    current_leaves = body_leaves\n        if matching_bracket and tail_leaves:\n            break\n    if not matching_bracket or not tail_leaves:\n        raise CannotSplit(\"No brackets found\")\n\n    head = bracket_split_build_line(\n        head_leaves, line, matching_bracket, component=_BracketSplitComponent.head\n    )\n    body = bracket_split_build_line(\n        body_leaves, line, matching_bracket, component=_BracketSplitComponent.body\n    )\n    tail = bracket_split_build_line(\n        tail_leaves, line, matching_bracket, component=_BracketSplitComponent.tail\n    )\n    bracket_split_succeeded_or_raise(head, body, tail)\n    for result in (head, body, tail):\n        if result:\n            yield result"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 939,
      "kind": "function",
      "qualname": "src.black.linegen._first_right_hand_split",
      "span": [
        925,
        1025
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _first_right_hand_split(\n    line: Line,\n    omit: Collection[LeafID] = (),\n) -> RHSResult:\n    \"\"\"Split the line into head, body, tail starting with the last bracket pair.\n\n    Note: this function should not have side effects. It's relied upon by\n    _maybe_split_omitting_optional_parens to get an opinion whether to prefer\n    splitting on the right side of an assignment statement.\n    \"\"\"\n    tail_leaves: list[Leaf] = []\n    body_leaves: list[Leaf] = []\n    head_leaves: list[Leaf] = []\n    current_leaves = tail_leaves\n    opening_bracket: Leaf | None = None\n    closing_bracket: Leaf | None = None\n    for leaf in reversed(line.leaves):\n        if current_leaves is body_leaves:\n            if leaf is opening_bracket:\n                current_leaves = head_leaves if body_leaves else tail_leaves\n        current_leaves.append(leaf)\n        if current_leaves is tail_leaves:\n            if leaf.type in CLOSING_BRACKETS and id(leaf) not in omit:\n                opening_bracket = leaf.opening_bracket\n                closing_bracket = leaf\n                current_leaves = body_leaves\n    if not (opening_bracket and closing_bracket and head_leaves):\n        # If there is no opening or closing_bracket that means the split failed and\n        # all content is in the tail.  Otherwise, if `head_leaves` are empty, it means\n        # the matching `opening_bracket` wasn't available on `line` anymore.\n        raise CannotSplit(\"No brackets found\")\n\n    tail_leaves.reverse()\n    body_leaves.reverse()\n    head_leaves.reverse()\n\n    body: Line | None = None\n    if (\n        Preview.hug_parens_with_braces_and_square_brackets in line.mode\n        and tail_leaves[0].value\n        and tail_leaves[0].opening_bracket is head_leaves[-1]\n    ):\n        inner_body_leaves = list(body_leaves)\n        hugged_opening_leaves: list[Leaf] = []\n        hugged_closing_leaves: list[Leaf] = []\n        is_unpacking = body_leaves[0].type in [token.STAR, token.DOUBLESTAR]\n        unpacking_offset: int = 1 if is_unpacking else 0\n        while (\n            len(inner_body_leaves) >= 2 + unpacking_offset\n            and inner_body_leaves[-1].type in CLOSING_BRACKETS\n            and inner_body_leaves[-1].opening_bracket\n            is inner_body_leaves[unpacking_offset]\n        ):\n            if unpacking_offset:\n                hugged_opening_leaves.append(inner_body_leaves.pop(0))\n                unpacking_offset = 0\n            hugged_opening_leaves.append(inner_body_leaves.pop(0))\n            hugged_closing_leaves.insert(0, inner_body_leaves.pop())\n\n        if hugged_opening_leaves and inner_body_leaves:\n            inner_body = bracket_split_build_line(\n                inner_body_leaves,\n                line,\n                hugged_opening_leaves[-1],\n                component=_BracketSplitComponent.body,\n            )\n            if (\n                line.mode.magic_trailing_comma\n                and inner_body_leaves[-1].type == token.COMMA\n            ):\n                should_hug = True\n            else:\n                line_length = line.mode.line_length - sum(\n                    len(str(leaf))\n                    for leaf in hugged_opening_leaves + hugged_closing_leaves\n                )\n                if is_line_short_enough(\n                    inner_body, mode=replace(line.mode, line_length=line_length)\n                ):\n                    # Do not hug if it fits on a single line.\n                    should_hug = False\n                else:\n                    should_hug = True\n            if should_hug:\n                body_leaves = inner_body_leaves\n                head_leaves.extend(hugged_opening_leaves)\n                tail_leaves = hugged_closing_leaves + tail_leaves\n                body = inner_body  # No need to re-calculate the body again later.\n\n    head = bracket_split_build_line(\n        head_leaves, line, opening_bracket, component=_BracketSplitComponent.head\n    )\n    if body is None:\n        body = bracket_split_build_line(\n            body_leaves, line, opening_bracket, component=_BracketSplitComponent.body\n        )\n    tail = bracket_split_build_line(\n        tail_leaves, line, opening_bracket, component=_BracketSplitComponent.tail\n    )\n    bracket_split_succeeded_or_raise(head, body, tail)\n    return RHSResult(head, body, tail, opening_bracket, closing_bracket)",
      "old_code": "def _first_right_hand_split(\n    line: Line,\n    omit: Collection[LeafID] = (),\n) -> RHSResult:\n    \"\"\"Split the line into head, body, tail starting with the last bracket pair.\n\n    Note: this function should not have side effects. It's relied upon by\n    _maybe_split_omitting_optional_parens to get an opinion whether to prefer\n    splitting on the right side of an assignment statement.\n    \"\"\"\n    tail_leaves: list[Leaf] = []\n    body_leaves: list[Leaf] = []\n    head_leaves: list[Leaf] = []\n    current_leaves = tail_leaves\n    opening_bracket: Optional[Leaf] = None\n    closing_bracket: Optional[Leaf] = None\n    for leaf in reversed(line.leaves):\n        if current_leaves is body_leaves:\n            if leaf is opening_bracket:\n                current_leaves = head_leaves if body_leaves else tail_leaves\n        current_leaves.append(leaf)\n        if current_leaves is tail_leaves:\n            if leaf.type in CLOSING_BRACKETS and id(leaf) not in omit:\n                opening_bracket = leaf.opening_bracket\n                closing_bracket = leaf\n                current_leaves = body_leaves\n    if not (opening_bracket and closing_bracket and head_leaves):\n        # If there is no opening or closing_bracket that means the split failed and\n        # all content is in the tail.  Otherwise, if `head_leaves` are empty, it means\n        # the matching `opening_bracket` wasn't available on `line` anymore.\n        raise CannotSplit(\"No brackets found\")\n\n    tail_leaves.reverse()\n    body_leaves.reverse()\n    head_leaves.reverse()\n\n    body: Optional[Line] = None\n    if (\n        Preview.hug_parens_with_braces_and_square_brackets in line.mode\n        and tail_leaves[0].value\n        and tail_leaves[0].opening_bracket is head_leaves[-1]\n    ):\n        inner_body_leaves = list(body_leaves)\n        hugged_opening_leaves: list[Leaf] = []\n        hugged_closing_leaves: list[Leaf] = []\n        is_unpacking = body_leaves[0].type in [token.STAR, token.DOUBLESTAR]\n        unpacking_offset: int = 1 if is_unpacking else 0\n        while (\n            len(inner_body_leaves) >= 2 + unpacking_offset\n            and inner_body_leaves[-1].type in CLOSING_BRACKETS\n            and inner_body_leaves[-1].opening_bracket\n            is inner_body_leaves[unpacking_offset]\n        ):\n            if unpacking_offset:\n                hugged_opening_leaves.append(inner_body_leaves.pop(0))\n                unpacking_offset = 0\n            hugged_opening_leaves.append(inner_body_leaves.pop(0))\n            hugged_closing_leaves.insert(0, inner_body_leaves.pop())\n\n        if hugged_opening_leaves and inner_body_leaves:\n            inner_body = bracket_split_build_line(\n                inner_body_leaves,\n                line,\n                hugged_opening_leaves[-1],\n                component=_BracketSplitComponent.body,\n            )\n            if (\n                line.mode.magic_trailing_comma\n                and inner_body_leaves[-1].type == token.COMMA\n            ):\n                should_hug = True\n            else:\n                line_length = line.mode.line_length - sum(\n                    len(str(leaf))\n                    for leaf in hugged_opening_leaves + hugged_closing_leaves\n                )\n                if is_line_short_enough(\n                    inner_body, mode=replace(line.mode, line_length=line_length)\n                ):\n                    # Do not hug if it fits on a single line.\n                    should_hug = False\n                else:\n                    should_hug = True\n            if should_hug:\n                body_leaves = inner_body_leaves\n                head_leaves.extend(hugged_opening_leaves)\n                tail_leaves = hugged_closing_leaves + tail_leaves\n                body = inner_body  # No need to re-calculate the body again later.\n\n    head = bracket_split_build_line(\n        head_leaves, line, opening_bracket, component=_BracketSplitComponent.head\n    )\n    if body is None:\n        body = bracket_split_build_line(\n            body_leaves, line, opening_bracket, component=_BracketSplitComponent.body\n        )\n    tail = bracket_split_build_line(\n        tail_leaves, line, opening_bracket, component=_BracketSplitComponent.tail\n    )\n    bracket_split_succeeded_or_raise(head, body, tail)\n    return RHSResult(head, body, tail, opening_bracket, closing_bracket)"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 1299,
      "kind": "function",
      "qualname": "src.black.linegen._get_last_non_comment_leaf",
      "span": [
        1299,
        1303
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _get_last_non_comment_leaf(line: Line) -> int | None:\n    for leaf_idx in range(len(line.leaves) - 1, 0, -1):\n        if line.leaves[leaf_idx].type != STANDALONE_COMMENT:\n            return leaf_idx\n    return None",
      "old_code": "def _get_last_non_comment_leaf(line: Line) -> Optional[int]:\n    for leaf_idx in range(len(line.leaves) - 1, 0, -1):\n        if line.leaves[leaf_idx].type != STANDALONE_COMMENT:\n            return leaf_idx\n    return None"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 1635,
      "kind": "function",
      "qualname": "src.black.linegen._maybe_wrap_cms_in_parens",
      "span": [
        1621,
        1660
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _maybe_wrap_cms_in_parens(\n    node: Node, mode: Mode, features: Collection[Feature]\n) -> None:\n    \"\"\"When enabled and safe, wrap the multiple context managers in invisible parens.\n\n    It is only safe when `features` contain Feature.PARENTHESIZED_CONTEXT_MANAGERS.\n    \"\"\"\n    if (\n        Feature.PARENTHESIZED_CONTEXT_MANAGERS not in features\n        or len(node.children) <= 2\n        # If it's an atom, it's already wrapped in parens.\n        or node.children[1].type == syms.atom\n    ):\n        return\n    colon_index: int | None = None\n    for i in range(2, len(node.children)):\n        if node.children[i].type == token.COLON:\n            colon_index = i\n            break\n    if colon_index is not None:\n        lpar = Leaf(token.LPAR, \"\")\n        rpar = Leaf(token.RPAR, \"\")\n        context_managers = node.children[1:colon_index]\n        for child in context_managers:\n            child.remove()\n        # After wrapping, the with_stmt will look like this:\n        #   with_stmt\n        #     NAME 'with'\n        #     atom\n        #       LPAR ''\n        #       testlist_gexp\n        #         ... <-- context_managers\n        #       /testlist_gexp\n        #       RPAR ''\n        #     /atom\n        #     COLON ':'\n        new_child = Node(\n            syms.atom, [lpar, Node(syms.testlist_gexp, context_managers), rpar]\n        )\n        node.insert_child(1, new_child)",
      "old_code": "def _maybe_wrap_cms_in_parens(\n    node: Node, mode: Mode, features: Collection[Feature]\n) -> None:\n    \"\"\"When enabled and safe, wrap the multiple context managers in invisible parens.\n\n    It is only safe when `features` contain Feature.PARENTHESIZED_CONTEXT_MANAGERS.\n    \"\"\"\n    if (\n        Feature.PARENTHESIZED_CONTEXT_MANAGERS not in features\n        or len(node.children) <= 2\n        # If it's an atom, it's already wrapped in parens.\n        or node.children[1].type == syms.atom\n    ):\n        return\n    colon_index: Optional[int] = None\n    for i in range(2, len(node.children)):\n        if node.children[i].type == token.COLON:\n            colon_index = i\n            break\n    if colon_index is not None:\n        lpar = Leaf(token.LPAR, \"\")\n        rpar = Leaf(token.RPAR, \"\")\n        context_managers = node.children[1:colon_index]\n        for child in context_managers:\n            child.remove()\n        # After wrapping, the with_stmt will look like this:\n        #   with_stmt\n        #     NAME 'with'\n        #     atom\n        #       LPAR ''\n        #       testlist_gexp\n        #         ... <-- context_managers\n        #       /testlist_gexp\n        #       RPAR ''\n        #     /atom\n        #     COLON ':'\n        new_child = Node(\n            syms.atom, [lpar, Node(syms.testlist_gexp, context_managers), rpar]\n        )\n        node.insert_child(1, new_child)"
    },
    {
      "path": "src/black/linegen.py",
      "version": "new",
      "line": 1868,
      "kind": "function",
      "qualname": "src.black.linegen.generate_trailers_to_omit",
      "span": [
        1852,
        1925
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[set[LeafID]]:\n    \"\"\"Generate sets of closing bracket IDs that should be omitted in a RHS.\n\n    Brackets can be omitted if the entire trailer up to and including\n    a preceding closing bracket fits in one line.\n\n    Yielded sets are cumulative (contain results of previous yields, too).  First\n    set is empty, unless the line should explode, in which case bracket pairs until\n    the one that needs to explode are omitted.\n    \"\"\"\n\n    omit: set[LeafID] = set()\n    if not line.magic_trailing_comma:\n        yield omit\n\n    length = 4 * line.depth\n    opening_bracket: Leaf | None = None\n    closing_bracket: Leaf | None = None\n    inner_brackets: set[LeafID] = set()\n    for index, leaf, leaf_length in line.enumerate_with_length(is_reversed=True):\n        length += leaf_length\n        if length > line_length:\n            break\n\n        has_inline_comment = leaf_length > len(leaf.value) + len(leaf.prefix)\n        if leaf.type == STANDALONE_COMMENT or has_inline_comment:\n            break\n\n        if opening_bracket:\n            if leaf is opening_bracket:\n                opening_bracket = None\n            elif leaf.type in CLOSING_BRACKETS:\n                prev = line.leaves[index - 1] if index > 0 else None\n                if (\n                    prev\n                    and prev.type == token.COMMA\n                    and leaf.opening_bracket is not None\n                    and not is_one_sequence_between(\n                        leaf.opening_bracket, leaf, line.leaves\n                    )\n                ):\n                    # Never omit bracket pairs with trailing commas.\n                    # We need to explode on those.\n                    break\n\n                inner_brackets.add(id(leaf))\n        elif leaf.type in CLOSING_BRACKETS:\n            prev = line.leaves[index - 1] if index > 0 else None\n            if prev and prev.type in OPENING_BRACKETS:\n                # Empty brackets would fail a split so treat them as \"inner\"\n                # brackets (e.g. only add them to the `omit` set if another\n                # pair of brackets was good enough.\n                inner_brackets.add(id(leaf))\n                continue\n\n            if closing_bracket:\n                omit.add(id(closing_bracket))\n                omit.update(inner_brackets)\n                inner_brackets.clear()\n                yield omit\n\n            if (\n                prev\n                and prev.type == token.COMMA\n                and leaf.opening_bracket is not None\n                and not is_one_sequence_between(leaf.opening_bracket, leaf, line.leaves)\n            ):\n                # Never omit bracket pairs with trailing commas.\n                # We need to explode on those.\n                break\n\n            if leaf.value:\n                opening_bracket = leaf.opening_bracket\n                closing_bracket = leaf",
      "old_code": "def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[set[LeafID]]:\n    \"\"\"Generate sets of closing bracket IDs that should be omitted in a RHS.\n\n    Brackets can be omitted if the entire trailer up to and including\n    a preceding closing bracket fits in one line.\n\n    Yielded sets are cumulative (contain results of previous yields, too).  First\n    set is empty, unless the line should explode, in which case bracket pairs until\n    the one that needs to explode are omitted.\n    \"\"\"\n\n    omit: set[LeafID] = set()\n    if not line.magic_trailing_comma:\n        yield omit\n\n    length = 4 * line.depth\n    opening_bracket: Optional[Leaf] = None\n    closing_bracket: Optional[Leaf] = None\n    inner_brackets: set[LeafID] = set()\n    for index, leaf, leaf_length in line.enumerate_with_length(is_reversed=True):\n        length += leaf_length\n        if length > line_length:\n            break\n\n        has_inline_comment = leaf_length > len(leaf.value) + len(leaf.prefix)\n        if leaf.type == STANDALONE_COMMENT or has_inline_comment:\n            break\n\n        if opening_bracket:\n            if leaf is opening_bracket:\n                opening_bracket = None\n            elif leaf.type in CLOSING_BRACKETS:\n                prev = line.leaves[index - 1] if index > 0 else None\n                if (\n                    prev\n                    and prev.type == token.COMMA\n                    and leaf.opening_bracket is not None\n                    and not is_one_sequence_between(\n                        leaf.opening_bracket, leaf, line.leaves\n                    )\n                ):\n                    # Never omit bracket pairs with trailing commas.\n                    # We need to explode on those.\n                    break\n\n                inner_brackets.add(id(leaf))\n        elif leaf.type in CLOSING_BRACKETS:\n            prev = line.leaves[index - 1] if index > 0 else None\n            if prev and prev.type in OPENING_BRACKETS:\n                # Empty brackets would fail a split so treat them as \"inner\"\n                # brackets (e.g. only add them to the `omit` set if another\n                # pair of brackets was good enough.\n                inner_brackets.add(id(leaf))\n                continue\n\n            if closing_bracket:\n                omit.add(id(closing_bracket))\n                omit.update(inner_brackets)\n                inner_brackets.clear()\n                yield omit\n\n            if (\n                prev\n                and prev.type == token.COMMA\n                and leaf.opening_bracket is not None\n                and not is_one_sequence_between(leaf.opening_bracket, leaf, line.leaves)\n            ):\n                # Never omit bracket pairs with trailing commas.\n                # We need to explode on those.\n                break\n\n            if leaf.value:\n                opening_bracket = leaf.opening_bracket\n                closing_bracket = leaf"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 51,
      "kind": "class",
      "qualname": "src.black.lines.Line",
      "span": [
        40,
        496
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class Line:\n    \"\"\"Holds leaves and comments. Can be printed with `str(line)`.\"\"\"\n\n    mode: Mode = field(repr=False)\n    depth: int = 0\n    leaves: list[Leaf] = field(default_factory=list)\n    # keys ordered like `leaves`\n    comments: dict[LeafID, list[Leaf]] = field(default_factory=dict)\n    bracket_tracker: BracketTracker = field(default_factory=BracketTracker)\n    inside_brackets: bool = False\n    should_split_rhs: bool = False\n    magic_trailing_comma: Leaf | None = None\n\n    def append(\n        self, leaf: Leaf, preformatted: bool = False, track_bracket: bool = False\n    ) -> None:\n        \"\"\"Add a new `leaf` to the end of the line.\n\n        Unless `preformatted` is True, the `leaf` will receive a new consistent\n        whitespace prefix and metadata applied by :class:`BracketTracker`.\n        Trailing commas are maybe removed, unpacked for loop variables are\n        demoted from being delimiters.\n\n        Inline comments are put aside.\n        \"\"\"\n        has_value = (\n            leaf.type in BRACKETS\n            # empty fstring and tstring middles must not be truncated\n            or leaf.type in (token.FSTRING_MIDDLE, token.TSTRING_MIDDLE)\n            or bool(leaf.value.strip())\n        )\n        if not has_value:\n            return\n\n        if leaf.type == token.COLON and self.is_class_paren_empty:\n            del self.leaves[-2:]\n        if self.leaves and not preformatted:\n            # Note: at this point leaf.prefix should be empty except for\n            # imports, for which we only preserve newlines.\n            leaf.prefix += whitespace(\n                leaf,\n                complex_subscript=self.is_complex_subscript(leaf),\n                mode=self.mode,\n            )\n        if self.inside_brackets or not preformatted or track_bracket:\n            self.bracket_tracker.mark(leaf)\n            if self.mode.magic_trailing_comma:\n                if self.has_magic_trailing_comma(leaf):\n                    self.magic_trailing_comma = leaf\n            elif self.has_magic_trailing_comma(leaf):\n                self.remove_trailing_comma()\n        if not self.append_comment(leaf):\n            self.leaves.append(leaf)\n\n    def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:\n        \"\"\"Like :func:`append()` but disallow invalid standalone comment structure.\n\n        Raises ValueError when any `leaf` is appended after a standalone comment\n        or when a standalone comment is not the first leaf on the line.\n        \"\"\"\n        if (\n            self.bracket_tracker.depth == 0\n            or self.bracket_tracker.any_open_for_or_lambda()\n        ):\n            if self.is_comment:\n                raise ValueError(\"cannot append to standalone comments\")\n\n            if self.leaves and leaf.type == STANDALONE_COMMENT:\n                raise ValueError(\n                    \"cannot append standalone comments to a populated line\"\n                )\n\n        self.append(leaf, preformatted=preformatted)\n\n    @property\n    def is_comment(self) -> bool:\n        \"\"\"Is this line a standalone comment?\"\"\"\n        return len(self.leaves) == 1 and self.leaves[0].type == STANDALONE_COMMENT\n\n    @property\n    def is_decorator(self) -> bool:\n        \"\"\"Is this line a decorator?\"\"\"\n        return bool(self) and self.leaves[0].type == token.AT\n\n    @property\n    def is_import(self) -> bool:\n        \"\"\"Is this an import line?\"\"\"\n        return bool(self) and is_import(self.leaves[0])\n\n    @property\n    def is_with_or_async_with_stmt(self) -> bool:\n        \"\"\"Is this a with_stmt line?\"\"\"\n        return bool(self) and is_with_or_async_with_stmt(self.leaves[0])\n\n    @property\n    def is_class(self) -> bool:\n        \"\"\"Is this line a class definition?\"\"\"\n        return (\n            bool(self)\n            and self.leaves[0].type == token.NAME\n            and self.leaves[0].value == \"class\"\n        )\n\n    @property\n    def is_stub_class(self) -> bool:\n        \"\"\"Is this line a class definition with a body consisting only of \"...\"?\"\"\"\n        return self.is_class and self.leaves[-3:] == [\n            Leaf(token.DOT, \".\") for _ in range(3)\n        ]\n\n    @property\n    def is_def(self) -> bool:\n        \"\"\"Is this a function definition? (Also returns True for async defs.)\"\"\"\n        try:\n            first_leaf = self.leaves[0]\n        except IndexError:\n            return False\n\n        try:\n            second_leaf: Leaf | None = self.leaves[1]\n        except IndexError:\n            second_leaf = None\n        return (first_leaf.type == token.NAME and first_leaf.value == \"def\") or (\n            first_leaf.type == token.ASYNC\n            and second_leaf is not None\n            and second_leaf.type == token.NAME\n            and second_leaf.value == \"def\"\n        )\n\n    @property\n    def is_stub_def(self) -> bool:\n        \"\"\"Is this line a function definition with a body consisting only of \"...\"?\"\"\"\n        return self.is_def and self.leaves[-4:] == [Leaf(token.COLON, \":\")] + [\n            Leaf(token.DOT, \".\") for _ in range(3)\n        ]\n\n    @property\n    def is_class_paren_empty(self) -> bool:\n        \"\"\"Is this a class with no base classes but using parentheses?\n\n        Those are unnecessary and should be removed.\n        \"\"\"\n        return (\n            bool(self)\n            and len(self.leaves) == 4\n            and self.is_class\n            and self.leaves[2].type == token.LPAR\n            and self.leaves[2].value == \"(\"\n            and self.leaves[3].type == token.RPAR\n            and self.leaves[3].value == \")\"\n        )\n\n    @property\n    def _is_triple_quoted_string(self) -> bool:\n        \"\"\"Is the line a triple quoted string?\"\"\"\n        if not self or self.leaves[0].type != token.STRING:\n            return False\n        value = self.leaves[0].value\n        if value.startswith(('\"\"\"', \"'''\")):\n            return True\n        if value.startswith((\"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"')):\n            return True\n        return False\n\n    @property\n    def is_docstring(self) -> bool:\n        \"\"\"Is the line a docstring?\"\"\"\n        return bool(self) and is_docstring(self.leaves[0])\n\n    @property\n    def is_chained_assignment(self) -> bool:\n        \"\"\"Is the line a chained assignment\"\"\"\n        return [leaf.type for leaf in self.leaves].count(token.EQUAL) > 1\n\n    @property\n    def opens_block(self) -> bool:\n        \"\"\"Does this line open a new level of indentation.\"\"\"\n        if len(self.leaves) == 0:\n            return False\n        return self.leaves[-1].type == token.COLON\n\n    def is_fmt_pass_converted(\n        self, *, first_leaf_matches: Callable[[Leaf], bool] | None = None\n    ) -> bool:\n        \"\"\"Is this line converted from fmt off/skip code?\n\n        If first_leaf_matches is not None, it only returns True if the first\n        leaf of converted code matches.\n        \"\"\"\n        if len(self.leaves) != 1:\n            return False\n        leaf = self.leaves[0]\n        if (\n            leaf.type != STANDALONE_COMMENT\n            or leaf.fmt_pass_converted_first_leaf is None\n        ):\n            return False\n        return first_leaf_matches is None or first_leaf_matches(\n            leaf.fmt_pass_converted_first_leaf\n        )\n\n    def contains_standalone_comments(self) -> bool:\n        \"\"\"If so, needs to be split before emitting.\"\"\"\n        for leaf in self.leaves:\n            if leaf.type == STANDALONE_COMMENT:\n                return True\n\n        return False\n\n    def contains_implicit_multiline_string_with_comments(self) -> bool:\n        \"\"\"Chck if we have an implicit multiline string with comments on the line\"\"\"\n        for leaf_type, leaf_group_iterator in itertools.groupby(\n            self.leaves, lambda leaf: leaf.type\n        ):\n            if leaf_type != token.STRING:\n                continue\n            leaf_list = list(leaf_group_iterator)\n            if len(leaf_list) == 1:\n                continue\n            for leaf in leaf_list:\n                if self.comments_after(leaf):\n                    return True\n        return False\n\n    def contains_uncollapsable_type_comments(self) -> bool:\n        ignored_ids = set()\n        try:\n            last_leaf = self.leaves[-1]\n            ignored_ids.add(id(last_leaf))\n            if last_leaf.type == token.COMMA or (\n                last_leaf.type == token.RPAR and not last_leaf.value\n            ):\n                # When trailing commas or optional parens are inserted by Black for\n                # consistency, comments after the previous last element are not moved\n                # (they don't have to, rendering will still be correct).  So we ignore\n                # trailing commas and invisible.\n                last_leaf = self.leaves[-2]\n                ignored_ids.add(id(last_leaf))\n        except IndexError:\n            return False\n\n        # A type comment is uncollapsable if it is attached to a leaf\n        # that isn't at the end of the line (since that could cause it\n        # to get associated to a different argument) or if there are\n        # comments before it (since that could cause it to get hidden\n        # behind a comment.\n        comment_seen = False\n        for leaf_id, comments in self.comments.items():\n            for comment in comments:\n                if is_type_comment(comment, mode=self.mode):\n                    if comment_seen or (\n                        not is_type_ignore_comment(comment, mode=self.mode)\n                        and leaf_id not in ignored_ids\n                    ):\n                        return True\n\n                comment_seen = True\n\n        return False\n\n    def contains_unsplittable_type_ignore(self) -> bool:\n        if not self.leaves:\n            return False\n\n        # If a 'type: ignore' is attached to the end of a line, we\n        # can't split the line, because we can't know which of the\n        # subexpressions the ignore was meant to apply to.\n        #\n        # We only want this to apply to actual physical lines from the\n        # original source, though: we don't want the presence of a\n        # 'type: ignore' at the end of a multiline expression to\n        # justify pushing it all onto one line. Thus we\n        # (unfortunately) need to check the actual source lines and\n        # only report an unsplittable 'type: ignore' if this line was\n        # one line in the original code.\n\n        # Grab the first and last line numbers, skipping generated leaves\n        first_line = next((leaf.lineno for leaf in self.leaves if leaf.lineno != 0), 0)\n        last_line = next(\n            (leaf.lineno for leaf in reversed(self.leaves) if leaf.lineno != 0), 0\n        )\n\n        if first_line == last_line:\n            # We look at the last two leaves since a comma or an\n            # invisible paren could have been added at the end of the\n            # line.\n            for node in self.leaves[-2:]:\n                for comment in self.comments.get(id(node), []):\n                    if is_type_ignore_comment(comment, mode=self.mode):\n                        return True\n\n        return False\n\n    def contains_multiline_strings(self) -> bool:\n        return any(is_multiline_string(leaf) for leaf in self.leaves)\n\n    def has_magic_trailing_comma(self, closing: Leaf) -> bool:\n        \"\"\"Return True if we have a magic trailing comma, that is when:\n        - there's a trailing comma here\n        - it's not from single-element square bracket indexing\n        - it's not a one-tuple\n        \"\"\"\n        if not (\n            closing.type in CLOSING_BRACKETS\n            and self.leaves\n            and self.leaves[-1].type == token.COMMA\n        ):\n            return False\n\n        if closing.type == token.RBRACE:\n            return True\n\n        if closing.type == token.RSQB:\n            if (\n                closing.parent is not None\n                and closing.parent.type == syms.trailer\n                and closing.opening_bracket is not None\n                and is_one_sequence_between(\n                    closing.opening_bracket,\n                    closing,\n                    self.leaves,\n                    brackets=(token.LSQB, token.RSQB),\n                )\n            ):\n                assert closing.prev_sibling is not None\n                assert closing.prev_sibling.type == syms.subscriptlist\n                return False\n\n            return True\n\n        if self.is_import:\n            return True\n\n        if closing.opening_bracket is not None and not is_one_sequence_between(\n            closing.opening_bracket, closing, self.leaves\n        ):\n            return True\n\n        return False\n\n    def append_comment(self, comment: Leaf) -> bool:\n        \"\"\"Add an inline or standalone comment to the line.\"\"\"\n        if (\n            comment.type == STANDALONE_COMMENT\n            and self.bracket_tracker.any_open_brackets()\n        ):\n            comment.prefix = \"\"\n            return False\n\n        if comment.type != token.COMMENT:\n            return False\n\n        if not self.leaves:\n            comment.type = STANDALONE_COMMENT\n            comment.prefix = \"\"\n            return False\n\n        last_leaf = self.leaves[-1]\n        if (\n            last_leaf.type == token.RPAR\n            and not last_leaf.value\n            and last_leaf.parent\n            and len(list(last_leaf.parent.leaves())) <= 3\n            and not is_type_comment(comment, mode=self.mode)\n        ):\n            # Comments on an optional parens wrapping a single leaf should belong to\n            # the wrapped node except if it's a type comment. Pinning the comment like\n            # this avoids unstable formatting caused by comment migration.\n            if len(self.leaves) < 2:\n                comment.type = STANDALONE_COMMENT\n                comment.prefix = \"\"\n                return False\n\n            last_leaf = self.leaves[-2]\n        self.comments.setdefault(id(last_leaf), []).append(comment)\n        return True\n\n    def comments_after(self, leaf: Leaf) -> list[Leaf]:\n        \"\"\"Generate comments that should appear directly after `leaf`.\"\"\"\n        return self.comments.get(id(leaf), [])\n\n    def remove_trailing_comma(self) -> None:\n        \"\"\"Remove the trailing comma and moves the comments attached to it.\"\"\"\n        trailing_comma = self.leaves.pop()\n        trailing_comma_comments = self.comments.pop(id(trailing_comma), [])\n        self.comments.setdefault(id(self.leaves[-1]), []).extend(\n            trailing_comma_comments\n        )\n\n    def is_complex_subscript(self, leaf: Leaf) -> bool:\n        \"\"\"Return True iff `leaf` is part of a slice with non-trivial exprs.\"\"\"\n        open_lsqb = self.bracket_tracker.get_open_lsqb()\n        if open_lsqb is None:\n            return False\n\n        subscript_start = open_lsqb.next_sibling\n\n        if isinstance(subscript_start, Node):\n            if subscript_start.type == syms.listmaker:\n                return False\n\n            if subscript_start.type == syms.subscriptlist:\n                subscript_start = child_towards(subscript_start, leaf)\n\n        return subscript_start is not None and any(\n            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()\n        )\n\n    def enumerate_with_length(\n        self, is_reversed: bool = False\n    ) -> Iterator[tuple[Index, Leaf, int]]:\n        \"\"\"Return an enumeration of leaves with their length.\n\n        Stops prematurely on multiline strings and standalone comments.\n        \"\"\"\n        op = cast(\n            Callable[[Sequence[Leaf]], Iterator[tuple[Index, Leaf]]],\n            enumerate_reversed if is_reversed else enumerate,\n        )\n        for index, leaf in op(self.leaves):\n            length = len(leaf.prefix) + len(leaf.value)\n            if \"\\n\" in leaf.value:\n                return  # Multiline strings, we can't continue.\n\n            for comment in self.comments_after(leaf):\n                length += len(comment.value)\n\n            yield index, leaf, length\n\n    def clone(self) -> \"Line\":\n        return Line(\n            mode=self.mode,\n            depth=self.depth,\n            inside_brackets=self.inside_brackets,\n            should_split_rhs=self.should_split_rhs,\n            magic_trailing_comma=self.magic_trailing_comma,\n        )\n\n    def __str__(self) -> str:\n        \"\"\"Render the line.\"\"\"\n        if not self:\n            return \"\\n\"\n\n        indent = \"    \" * self.depth\n        leaves = iter(self.leaves)\n        first = next(leaves)\n        res = f\"{first.prefix}{indent}{first.value}\"\n        res += \"\".join(str(leaf) for leaf in leaves)\n        comments_iter = itertools.chain.from_iterable(self.comments.values())\n        comments = [str(comment) for comment in comments_iter]\n        res += \"\".join(comments)\n\n        return res + \"\\n\"\n\n    def __bool__(self) -> bool:\n        \"\"\"Return True if the line has leaves or comments.\"\"\"\n        return bool(self.leaves or self.comments)",
      "old_code": "class Line:\n    \"\"\"Holds leaves and comments. Can be printed with `str(line)`.\"\"\"\n\n    mode: Mode = field(repr=False)\n    depth: int = 0\n    leaves: list[Leaf] = field(default_factory=list)\n    # keys ordered like `leaves`\n    comments: dict[LeafID, list[Leaf]] = field(default_factory=dict)\n    bracket_tracker: BracketTracker = field(default_factory=BracketTracker)\n    inside_brackets: bool = False\n    should_split_rhs: bool = False\n    magic_trailing_comma: Optional[Leaf] = None\n\n    def append(\n        self, leaf: Leaf, preformatted: bool = False, track_bracket: bool = False\n    ) -> None:\n        \"\"\"Add a new `leaf` to the end of the line.\n\n        Unless `preformatted` is True, the `leaf` will receive a new consistent\n        whitespace prefix and metadata applied by :class:`BracketTracker`.\n        Trailing commas are maybe removed, unpacked for loop variables are\n        demoted from being delimiters.\n\n        Inline comments are put aside.\n        \"\"\"\n        has_value = (\n            leaf.type in BRACKETS\n            # empty fstring and tstring middles must not be truncated\n            or leaf.type in (token.FSTRING_MIDDLE, token.TSTRING_MIDDLE)\n            or bool(leaf.value.strip())\n        )\n        if not has_value:\n            return\n\n        if leaf.type == token.COLON and self.is_class_paren_empty:\n            del self.leaves[-2:]\n        if self.leaves and not preformatted:\n            # Note: at this point leaf.prefix should be empty except for\n            # imports, for which we only preserve newlines.\n            leaf.prefix += whitespace(\n                leaf,\n                complex_subscript=self.is_complex_subscript(leaf),\n                mode=self.mode,\n            )\n        if self.inside_brackets or not preformatted or track_bracket:\n            self.bracket_tracker.mark(leaf)\n            if self.mode.magic_trailing_comma:\n                if self.has_magic_trailing_comma(leaf):\n                    self.magic_trailing_comma = leaf\n            elif self.has_magic_trailing_comma(leaf):\n                self.remove_trailing_comma()\n        if not self.append_comment(leaf):\n            self.leaves.append(leaf)\n\n    def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:\n        \"\"\"Like :func:`append()` but disallow invalid standalone comment structure.\n\n        Raises ValueError when any `leaf` is appended after a standalone comment\n        or when a standalone comment is not the first leaf on the line.\n        \"\"\"\n        if (\n            self.bracket_tracker.depth == 0\n            or self.bracket_tracker.any_open_for_or_lambda()\n        ):\n            if self.is_comment:\n                raise ValueError(\"cannot append to standalone comments\")\n\n            if self.leaves and leaf.type == STANDALONE_COMMENT:\n                raise ValueError(\n                    \"cannot append standalone comments to a populated line\"\n                )\n\n        self.append(leaf, preformatted=preformatted)\n\n    @property\n    def is_comment(self) -> bool:\n        \"\"\"Is this line a standalone comment?\"\"\"\n        return len(self.leaves) == 1 and self.leaves[0].type == STANDALONE_COMMENT\n\n    @property\n    def is_decorator(self) -> bool:\n        \"\"\"Is this line a decorator?\"\"\"\n        return bool(self) and self.leaves[0].type == token.AT\n\n    @property\n    def is_import(self) -> bool:\n        \"\"\"Is this an import line?\"\"\"\n        return bool(self) and is_import(self.leaves[0])\n\n    @property\n    def is_with_or_async_with_stmt(self) -> bool:\n        \"\"\"Is this a with_stmt line?\"\"\"\n        return bool(self) and is_with_or_async_with_stmt(self.leaves[0])\n\n    @property\n    def is_class(self) -> bool:\n        \"\"\"Is this line a class definition?\"\"\"\n        return (\n            bool(self)\n            and self.leaves[0].type == token.NAME\n            and self.leaves[0].value == \"class\"\n        )\n\n    @property\n    def is_stub_class(self) -> bool:\n        \"\"\"Is this line a class definition with a body consisting only of \"...\"?\"\"\"\n        return self.is_class and self.leaves[-3:] == [\n            Leaf(token.DOT, \".\") for _ in range(3)\n        ]\n\n    @property\n    def is_def(self) -> bool:\n        \"\"\"Is this a function definition? (Also returns True for async defs.)\"\"\"\n        try:\n            first_leaf = self.leaves[0]\n        except IndexError:\n            return False\n\n        try:\n            second_leaf: Optional[Leaf] = self.leaves[1]\n        except IndexError:\n            second_leaf = None\n        return (first_leaf.type == token.NAME and first_leaf.value == \"def\") or (\n            first_leaf.type == token.ASYNC\n            and second_leaf is not None\n            and second_leaf.type == token.NAME\n            and second_leaf.value == \"def\"\n        )\n\n    @property\n    def is_stub_def(self) -> bool:\n        \"\"\"Is this line a function definition with a body consisting only of \"...\"?\"\"\"\n        return self.is_def and self.leaves[-4:] == [Leaf(token.COLON, \":\")] + [\n            Leaf(token.DOT, \".\") for _ in range(3)\n        ]\n\n    @property\n    def is_class_paren_empty(self) -> bool:\n        \"\"\"Is this a class with no base classes but using parentheses?\n\n        Those are unnecessary and should be removed.\n        \"\"\"\n        return (\n            bool(self)\n            and len(self.leaves) == 4\n            and self.is_class\n            and self.leaves[2].type == token.LPAR\n            and self.leaves[2].value == \"(\"\n            and self.leaves[3].type == token.RPAR\n            and self.leaves[3].value == \")\"\n        )\n\n    @property\n    def _is_triple_quoted_string(self) -> bool:\n        \"\"\"Is the line a triple quoted string?\"\"\"\n        if not self or self.leaves[0].type != token.STRING:\n            return False\n        value = self.leaves[0].value\n        if value.startswith(('\"\"\"', \"'''\")):\n            return True\n        if value.startswith((\"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"')):\n            return True\n        return False\n\n    @property\n    def is_docstring(self) -> bool:\n        \"\"\"Is the line a docstring?\"\"\"\n        return bool(self) and is_docstring(self.leaves[0])\n\n    @property\n    def is_chained_assignment(self) -> bool:\n        \"\"\"Is the line a chained assignment\"\"\"\n        return [leaf.type for leaf in self.leaves].count(token.EQUAL) > 1\n\n    @property\n    def opens_block(self) -> bool:\n        \"\"\"Does this line open a new level of indentation.\"\"\"\n        if len(self.leaves) == 0:\n            return False\n        return self.leaves[-1].type == token.COLON\n\n    def is_fmt_pass_converted(\n        self, *, first_leaf_matches: Optional[Callable[[Leaf], bool]] = None\n    ) -> bool:\n        \"\"\"Is this line converted from fmt off/skip code?\n\n        If first_leaf_matches is not None, it only returns True if the first\n        leaf of converted code matches.\n        \"\"\"\n        if len(self.leaves) != 1:\n            return False\n        leaf = self.leaves[0]\n        if (\n            leaf.type != STANDALONE_COMMENT\n            or leaf.fmt_pass_converted_first_leaf is None\n        ):\n            return False\n        return first_leaf_matches is None or first_leaf_matches(\n            leaf.fmt_pass_converted_first_leaf\n        )\n\n    def contains_standalone_comments(self) -> bool:\n        \"\"\"If so, needs to be split before emitting.\"\"\"\n        for leaf in self.leaves:\n            if leaf.type == STANDALONE_COMMENT:\n                return True\n\n        return False\n\n    def contains_implicit_multiline_string_with_comments(self) -> bool:\n        \"\"\"Chck if we have an implicit multiline string with comments on the line\"\"\"\n        for leaf_type, leaf_group_iterator in itertools.groupby(\n            self.leaves, lambda leaf: leaf.type\n        ):\n            if leaf_type != token.STRING:\n                continue\n            leaf_list = list(leaf_group_iterator)\n            if len(leaf_list) == 1:\n                continue\n            for leaf in leaf_list:\n                if self.comments_after(leaf):\n                    return True\n        return False\n\n    def contains_uncollapsable_type_comments(self) -> bool:\n        ignored_ids = set()\n        try:\n            last_leaf = self.leaves[-1]\n            ignored_ids.add(id(last_leaf))\n            if last_leaf.type == token.COMMA or (\n                last_leaf.type == token.RPAR and not last_leaf.value\n            ):\n                # When trailing commas or optional parens are inserted by Black for\n                # consistency, comments after the previous last element are not moved\n                # (they don't have to, rendering will still be correct).  So we ignore\n                # trailing commas and invisible.\n                last_leaf = self.leaves[-2]\n                ignored_ids.add(id(last_leaf))\n        except IndexError:\n            return False\n\n        # A type comment is uncollapsable if it is attached to a leaf\n        # that isn't at the end of the line (since that could cause it\n        # to get associated to a different argument) or if there are\n        # comments before it (since that could cause it to get hidden\n        # behind a comment.\n        comment_seen = False\n        for leaf_id, comments in self.comments.items():\n            for comment in comments:\n                if is_type_comment(comment, mode=self.mode):\n                    if comment_seen or (\n                        not is_type_ignore_comment(comment, mode=self.mode)\n                        and leaf_id not in ignored_ids\n                    ):\n                        return True\n\n                comment_seen = True\n\n        return False\n\n    def contains_unsplittable_type_ignore(self) -> bool:\n        if not self.leaves:\n            return False\n\n        # If a 'type: ignore' is attached to the end of a line, we\n        # can't split the line, because we can't know which of the\n        # subexpressions the ignore was meant to apply to.\n        #\n        # We only want this to apply to actual physical lines from the\n        # original source, though: we don't want the presence of a\n        # 'type: ignore' at the end of a multiline expression to\n        # justify pushing it all onto one line. Thus we\n        # (unfortunately) need to check the actual source lines and\n        # only report an unsplittable 'type: ignore' if this line was\n        # one line in the original code.\n\n        # Grab the first and last line numbers, skipping generated leaves\n        first_line = next((leaf.lineno for leaf in self.leaves if leaf.lineno != 0), 0)\n        last_line = next(\n            (leaf.lineno for leaf in reversed(self.leaves) if leaf.lineno != 0), 0\n        )\n\n        if first_line == last_line:\n            # We look at the last two leaves since a comma or an\n            # invisible paren could have been added at the end of the\n            # line.\n            for node in self.leaves[-2:]:\n                for comment in self.comments.get(id(node), []):\n                    if is_type_ignore_comment(comment, mode=self.mode):\n                        return True\n\n        return False\n\n    def contains_multiline_strings(self) -> bool:\n        return any(is_multiline_string(leaf) for leaf in self.leaves)\n\n    def has_magic_trailing_comma(self, closing: Leaf) -> bool:\n        \"\"\"Return True if we have a magic trailing comma, that is when:\n        - there's a trailing comma here\n        - it's not from single-element square bracket indexing\n        - it's not a one-tuple\n        \"\"\"\n        if not (\n            closing.type in CLOSING_BRACKETS\n            and self.leaves\n            and self.leaves[-1].type == token.COMMA\n        ):\n            return False\n\n        if closing.type == token.RBRACE:\n            return True\n\n        if closing.type == token.RSQB:\n            if (\n                closing.parent is not None\n                and closing.parent.type == syms.trailer\n                and closing.opening_bracket is not None\n                and is_one_sequence_between(\n                    closing.opening_bracket,\n                    closing,\n                    self.leaves,\n                    brackets=(token.LSQB, token.RSQB),\n                )\n            ):\n                assert closing.prev_sibling is not None\n                assert closing.prev_sibling.type == syms.subscriptlist\n                return False\n\n            return True\n\n        if self.is_import:\n            return True\n\n        if closing.opening_bracket is not None and not is_one_sequence_between(\n            closing.opening_bracket, closing, self.leaves\n        ):\n            return True\n\n        return False\n\n    def append_comment(self, comment: Leaf) -> bool:\n        \"\"\"Add an inline or standalone comment to the line.\"\"\"\n        if (\n            comment.type == STANDALONE_COMMENT\n            and self.bracket_tracker.any_open_brackets()\n        ):\n            comment.prefix = \"\"\n            return False\n\n        if comment.type != token.COMMENT:\n            return False\n\n        if not self.leaves:\n            comment.type = STANDALONE_COMMENT\n            comment.prefix = \"\"\n            return False\n\n        last_leaf = self.leaves[-1]\n        if (\n            last_leaf.type == token.RPAR\n            and not last_leaf.value\n            and last_leaf.parent\n            and len(list(last_leaf.parent.leaves())) <= 3\n            and not is_type_comment(comment, mode=self.mode)\n        ):\n            # Comments on an optional parens wrapping a single leaf should belong to\n            # the wrapped node except if it's a type comment. Pinning the comment like\n            # this avoids unstable formatting caused by comment migration.\n            if len(self.leaves) < 2:\n                comment.type = STANDALONE_COMMENT\n                comment.prefix = \"\"\n                return False\n\n            last_leaf = self.leaves[-2]\n        self.comments.setdefault(id(last_leaf), []).append(comment)\n        return True\n\n    def comments_after(self, leaf: Leaf) -> list[Leaf]:\n        \"\"\"Generate comments that should appear directly after `leaf`.\"\"\"\n        return self.comments.get(id(leaf), [])\n\n    def remove_trailing_comma(self) -> None:\n        \"\"\"Remove the trailing comma and moves the comments attached to it.\"\"\"\n        trailing_comma = self.leaves.pop()\n        trailing_comma_comments = self.comments.pop(id(trailing_comma), [])\n        self.comments.setdefault(id(self.leaves[-1]), []).extend(\n            trailing_comma_comments\n        )\n\n    def is_complex_subscript(self, leaf: Leaf) -> bool:\n        \"\"\"Return True iff `leaf` is part of a slice with non-trivial exprs.\"\"\"\n        open_lsqb = self.bracket_tracker.get_open_lsqb()\n        if open_lsqb is None:\n            return False\n\n        subscript_start = open_lsqb.next_sibling\n\n        if isinstance(subscript_start, Node):\n            if subscript_start.type == syms.listmaker:\n                return False\n\n            if subscript_start.type == syms.subscriptlist:\n                subscript_start = child_towards(subscript_start, leaf)\n\n        return subscript_start is not None and any(\n            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()\n        )\n\n    def enumerate_with_length(\n        self, is_reversed: bool = False\n    ) -> Iterator[tuple[Index, Leaf, int]]:\n        \"\"\"Return an enumeration of leaves with their length.\n\n        Stops prematurely on multiline strings and standalone comments.\n        \"\"\"\n        op = cast(\n            Callable[[Sequence[Leaf]], Iterator[tuple[Index, Leaf]]],\n            enumerate_reversed if is_reversed else enumerate,\n        )\n        for index, leaf in op(self.leaves):\n            length = len(leaf.prefix) + len(leaf.value)\n            if \"\\n\" in leaf.value:\n                return  # Multiline strings, we can't continue.\n\n            for comment in self.comments_after(leaf):\n                length += len(comment.value)\n\n            yield index, leaf, length\n\n    def clone(self) -> \"Line\":\n        return Line(\n            mode=self.mode,\n            depth=self.depth,\n            inside_brackets=self.inside_brackets,\n            should_split_rhs=self.should_split_rhs,\n            magic_trailing_comma=self.magic_trailing_comma,\n        )\n\n    def __str__(self) -> str:\n        \"\"\"Render the line.\"\"\"\n        if not self:\n            return \"\\n\"\n\n        indent = \"    \" * self.depth\n        leaves = iter(self.leaves)\n        first = next(leaves)\n        res = f\"{first.prefix}{indent}{first.value}\"\n        res += \"\".join(str(leaf) for leaf in leaves)\n        comments_iter = itertools.chain.from_iterable(self.comments.values())\n        comments = [str(comment) for comment in comments_iter]\n        res += \"\".join(comments)\n\n        return res + \"\\n\"\n\n    def __bool__(self) -> bool:\n        \"\"\"Return True if the line has leaves or comments.\"\"\"\n        return bool(self.leaves or self.comments)"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 159,
      "kind": "function",
      "qualname": "src.black.lines.Line.is_def",
      "span": [
        151,
        167
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def is_def(self) -> bool:\n        \"\"\"Is this a function definition? (Also returns True for async defs.)\"\"\"\n        try:\n            first_leaf = self.leaves[0]\n        except IndexError:\n            return False\n\n        try:\n            second_leaf: Leaf | None = self.leaves[1]\n        except IndexError:\n            second_leaf = None\n        return (first_leaf.type == token.NAME and first_leaf.value == \"def\") or (\n            first_leaf.type == token.ASYNC\n            and second_leaf is not None\n            and second_leaf.type == token.NAME\n            and second_leaf.value == \"def\"\n        )",
      "old_code": "    def is_def(self) -> bool:\n        \"\"\"Is this a function definition? (Also returns True for async defs.)\"\"\"\n        try:\n            first_leaf = self.leaves[0]\n        except IndexError:\n            return False\n\n        try:\n            second_leaf: Optional[Leaf] = self.leaves[1]\n        except IndexError:\n            second_leaf = None\n        return (first_leaf.type == token.NAME and first_leaf.value == \"def\") or (\n            first_leaf.type == token.ASYNC\n            and second_leaf is not None\n            and second_leaf.type == token.NAME\n            and second_leaf.value == \"def\"\n        )"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 222,
      "kind": "function",
      "qualname": "src.black.lines.Line.is_fmt_pass_converted",
      "span": [
        221,
        239
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def is_fmt_pass_converted(\n        self, *, first_leaf_matches: Callable[[Leaf], bool] | None = None\n    ) -> bool:\n        \"\"\"Is this line converted from fmt off/skip code?\n\n        If first_leaf_matches is not None, it only returns True if the first\n        leaf of converted code matches.\n        \"\"\"\n        if len(self.leaves) != 1:\n            return False\n        leaf = self.leaves[0]\n        if (\n            leaf.type != STANDALONE_COMMENT\n            or leaf.fmt_pass_converted_first_leaf is None\n        ):\n            return False\n        return first_leaf_matches is None or first_leaf_matches(\n            leaf.fmt_pass_converted_first_leaf\n        )",
      "old_code": "    def is_fmt_pass_converted(\n        self, *, first_leaf_matches: Optional[Callable[[Leaf], bool]] = None\n    ) -> bool:\n        \"\"\"Is this line converted from fmt off/skip code?\n\n        If first_leaf_matches is not None, it only returns True if the first\n        leaf of converted code matches.\n        \"\"\"\n        if len(self.leaves) != 1:\n            return False\n        leaf = self.leaves[0]\n        if (\n            leaf.type != STANDALONE_COMMENT\n            or leaf.fmt_pass_converted_first_leaf is None\n        ):\n            return False\n        return first_leaf_matches is None or first_leaf_matches(\n            leaf.fmt_pass_converted_first_leaf\n        )"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 543,
      "kind": "class",
      "qualname": "src.black.lines.EmptyLineTracker",
      "span": [
        533,
        790
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class EmptyLineTracker:\n    \"\"\"Provides a stateful method that returns the number of potential extra\n    empty lines needed before and after the currently processed line.\n\n    Note: this tracker works on lines that haven't been split yet.  It assumes\n    the prefix of the first leaf consists of optional newlines.  Those newlines\n    are consumed by `maybe_empty_lines()` and included in the computation.\n    \"\"\"\n\n    mode: Mode\n    previous_line: Line | None = None\n    previous_block: LinesBlock | None = None\n    previous_defs: list[Line] = field(default_factory=list)\n    semantic_leading_comment: LinesBlock | None = None\n\n    def maybe_empty_lines(self, current_line: Line) -> LinesBlock:\n        \"\"\"Return the number of extra empty lines before and after the `current_line`.\n\n        This is for separating `def`, `async def` and `class` with extra empty\n        lines (two on module-level).\n        \"\"\"\n        form_feed = (\n            current_line.depth == 0\n            and bool(current_line.leaves)\n            and \"\\f\\n\" in current_line.leaves[0].prefix\n        )\n        before, after = self._maybe_empty_lines(current_line)\n        previous_after = self.previous_block.after if self.previous_block else 0\n        before = max(0, before - previous_after)\n        if Preview.fix_module_docstring_detection in self.mode:\n            # Always have one empty line after a module docstring\n            if self._line_is_module_docstring(current_line):\n                before = 1\n        else:\n            if (\n                # Always have one empty line after a module docstring\n                self.previous_block\n                and self.previous_block.previous_block is None\n                and len(self.previous_block.original_line.leaves) == 1\n                and self.previous_block.original_line.is_docstring\n                and not (current_line.is_class or current_line.is_def)\n            ):\n                before = 1\n\n        block = LinesBlock(\n            mode=self.mode,\n            previous_block=self.previous_block,\n            original_line=current_line,\n            before=before,\n            after=after,\n            form_feed=form_feed,\n        )\n\n        # Maintain the semantic_leading_comment state.\n        if current_line.is_comment:\n            if self.previous_line is None or (\n                not self.previous_line.is_decorator\n                # `or before` means this comment already has an empty line before\n                and (not self.previous_line.is_comment or before)\n                and (self.semantic_leading_comment is None or before)\n            ):\n                self.semantic_leading_comment = block\n        # `or before` means this decorator already has an empty line before\n        elif not current_line.is_decorator or before:\n            self.semantic_leading_comment = None\n\n        self.previous_line = current_line\n        self.previous_block = block\n        return block\n\n    def _line_is_module_docstring(self, current_line: Line) -> bool:\n        previous_block = self.previous_block\n        if not previous_block:\n            return False\n        if (\n            len(previous_block.original_line.leaves) != 1\n            or not previous_block.original_line.is_docstring\n            or current_line.is_class\n            or current_line.is_def\n        ):\n            return False\n        while previous_block := previous_block.previous_block:\n            if not previous_block.original_line.is_comment:\n                return False\n        return True\n\n    def _maybe_empty_lines(self, current_line: Line) -> tuple[int, int]:  # noqa: C901\n        max_allowed = 1\n        if current_line.depth == 0:\n            max_allowed = 1 if self.mode.is_pyi else 2\n\n        if current_line.leaves:\n            # Consume the first leaf's extra newlines.\n            first_leaf = current_line.leaves[0]\n            before = first_leaf.prefix.count(\"\\n\")\n            before = min(before, max_allowed)\n            first_leaf.prefix = \"\"\n        else:\n            before = 0\n\n        user_had_newline = bool(before)\n        depth = current_line.depth\n\n        # Mutate self.previous_defs, remainder of this function should be pure\n        previous_def = None\n        while self.previous_defs and self.previous_defs[-1].depth >= depth:\n            previous_def = self.previous_defs.pop()\n        if current_line.is_def or current_line.is_class:\n            self.previous_defs.append(current_line)\n\n        if self.previous_line is None:\n            # Don't insert empty lines before the first line in the file.\n            return 0, 0\n\n        if current_line.is_docstring:\n            if self.previous_line.is_class:\n                return 0, 1\n            if self.previous_line.opens_block and self.previous_line.is_def:\n                return 0, 0\n\n        if previous_def is not None:\n            assert self.previous_line is not None\n            if self.mode.is_pyi:\n                if previous_def.is_class and not previous_def.is_stub_class:\n                    before = 1\n                elif depth and not current_line.is_def and self.previous_line.is_def:\n                    # Empty lines between attributes and methods should be preserved.\n                    before = 1 if user_had_newline else 0\n                elif depth:\n                    before = 0\n                else:\n                    before = 1\n            else:\n                if depth:\n                    before = 1\n                elif (\n                    not depth\n                    and previous_def.depth\n                    and current_line.leaves[-1].type == token.COLON\n                    and (\n                        current_line.leaves[0].value\n                        not in (\"with\", \"try\", \"for\", \"while\", \"if\", \"match\")\n                    )\n                ):\n                    # We shouldn't add two newlines between an indented function and\n                    # a dependent non-indented clause. This is to avoid issues with\n                    # conditional function definitions that are technically top-level\n                    # and therefore get two trailing newlines, but look weird and\n                    # inconsistent when they're followed by elif, else, etc. This is\n                    # worse because these functions only get *one* preceding newline\n                    # already.\n                    before = 1\n                else:\n                    before = 2\n\n        if current_line.is_decorator or current_line.is_def or current_line.is_class:\n            return self._maybe_empty_lines_for_class_or_def(\n                current_line, before, user_had_newline\n            )\n\n        if (\n            self.previous_line.is_import\n            and self.previous_line.depth == 0\n            and current_line.depth == 0\n            and not current_line.is_import\n            and Preview.always_one_newline_after_import in self.mode\n        ):\n            return 1, 0\n\n        if (\n            self.previous_line.is_import\n            and not current_line.is_import\n            and not current_line.is_fmt_pass_converted(first_leaf_matches=is_import)\n            and depth == self.previous_line.depth\n        ):\n            return (before or 1), 0\n\n        return before, 0\n\n    def _maybe_empty_lines_for_class_or_def(  # noqa: C901\n        self, current_line: Line, before: int, user_had_newline: bool\n    ) -> tuple[int, int]:\n        assert self.previous_line is not None\n\n        if self.previous_line.is_decorator:\n            if self.mode.is_pyi and current_line.is_stub_class:\n                # Insert an empty line after a decorated stub class\n                return 0, 1\n            return 0, 0\n\n        if self.previous_line.depth < current_line.depth and (\n            self.previous_line.is_class or self.previous_line.is_def\n        ):\n            if self.mode.is_pyi:\n                return 0, 0\n            return 1 if user_had_newline else 0, 0\n\n        comment_to_add_newlines: LinesBlock | None = None\n        if (\n            self.previous_line.is_comment\n            and self.previous_line.depth == current_line.depth\n            and before == 0\n        ):\n            slc = self.semantic_leading_comment\n            if (\n                slc is not None\n                and slc.previous_block is not None\n                and not slc.previous_block.original_line.is_class\n                and not slc.previous_block.original_line.opens_block\n                and slc.before <= 1\n            ):\n                comment_to_add_newlines = slc\n            else:\n                return 0, 0\n\n        if self.mode.is_pyi:\n            if current_line.is_class or self.previous_line.is_class:\n                if self.previous_line.depth < current_line.depth:\n                    newlines = 0\n                elif self.previous_line.depth > current_line.depth:\n                    newlines = 1\n                elif current_line.is_stub_class and self.previous_line.is_stub_class:\n                    # No blank line between classes with an empty body\n                    newlines = 0\n                else:\n                    newlines = 1\n            # Don't inspect the previous line if it's part of the body of the previous\n            # statement in the same level, we always want a blank line if there's\n            # something with a body preceding.\n            elif self.previous_line.depth > current_line.depth:\n                newlines = 1\n            elif (\n                current_line.is_def or current_line.is_decorator\n            ) and not self.previous_line.is_def:\n                if current_line.depth:\n                    # In classes empty lines between attributes and methods should\n                    # be preserved.\n                    newlines = min(1, before)\n                else:\n                    # Blank line between a block of functions (maybe with preceding\n                    # decorators) and a block of non-functions\n                    newlines = 1\n            else:\n                newlines = 0\n        else:\n            newlines = 1 if current_line.depth else 2\n            # If a user has left no space after a dummy implementation, don't insert\n            # new lines. This is useful for instance for @overload or Protocols.\n            if self.previous_line.is_stub_def and not user_had_newline:\n                newlines = 0\n        if comment_to_add_newlines is not None:\n            previous_block = comment_to_add_newlines.previous_block\n            if previous_block is not None:\n                comment_to_add_newlines.before = (\n                    max(comment_to_add_newlines.before, newlines) - previous_block.after\n                )\n                newlines = 0\n        return newlines, 0",
      "old_code": "class EmptyLineTracker:\n    \"\"\"Provides a stateful method that returns the number of potential extra\n    empty lines needed before and after the currently processed line.\n\n    Note: this tracker works on lines that haven't been split yet.  It assumes\n    the prefix of the first leaf consists of optional newlines.  Those newlines\n    are consumed by `maybe_empty_lines()` and included in the computation.\n    \"\"\"\n\n    mode: Mode\n    previous_line: Optional[Line] = None\n    previous_block: Optional[LinesBlock] = None\n    previous_defs: list[Line] = field(default_factory=list)\n    semantic_leading_comment: Optional[LinesBlock] = None\n\n    def maybe_empty_lines(self, current_line: Line) -> LinesBlock:\n        \"\"\"Return the number of extra empty lines before and after the `current_line`.\n\n        This is for separating `def`, `async def` and `class` with extra empty\n        lines (two on module-level).\n        \"\"\"\n        form_feed = (\n            current_line.depth == 0\n            and bool(current_line.leaves)\n            and \"\\f\\n\" in current_line.leaves[0].prefix\n        )\n        before, after = self._maybe_empty_lines(current_line)\n        previous_after = self.previous_block.after if self.previous_block else 0\n        before = max(0, before - previous_after)\n        if Preview.fix_module_docstring_detection in self.mode:\n            # Always have one empty line after a module docstring\n            if self._line_is_module_docstring(current_line):\n                before = 1\n        else:\n            if (\n                # Always have one empty line after a module docstring\n                self.previous_block\n                and self.previous_block.previous_block is None\n                and len(self.previous_block.original_line.leaves) == 1\n                and self.previous_block.original_line.is_docstring\n                and not (current_line.is_class or current_line.is_def)\n            ):\n                before = 1\n\n        block = LinesBlock(\n            mode=self.mode,\n            previous_block=self.previous_block,\n            original_line=current_line,\n            before=before,\n            after=after,\n            form_feed=form_feed,\n        )\n\n        # Maintain the semantic_leading_comment state.\n        if current_line.is_comment:\n            if self.previous_line is None or (\n                not self.previous_line.is_decorator\n                # `or before` means this comment already has an empty line before\n                and (not self.previous_line.is_comment or before)\n                and (self.semantic_leading_comment is None or before)\n            ):\n                self.semantic_leading_comment = block\n        # `or before` means this decorator already has an empty line before\n        elif not current_line.is_decorator or before:\n            self.semantic_leading_comment = None\n\n        self.previous_line = current_line\n        self.previous_block = block\n        return block\n\n    def _line_is_module_docstring(self, current_line: Line) -> bool:\n        previous_block = self.previous_block\n        if not previous_block:\n            return False\n        if (\n            len(previous_block.original_line.leaves) != 1\n            or not previous_block.original_line.is_docstring\n            or current_line.is_class\n            or current_line.is_def\n        ):\n            return False\n        while previous_block := previous_block.previous_block:\n            if not previous_block.original_line.is_comment:\n                return False\n        return True\n\n    def _maybe_empty_lines(self, current_line: Line) -> tuple[int, int]:  # noqa: C901\n        max_allowed = 1\n        if current_line.depth == 0:\n            max_allowed = 1 if self.mode.is_pyi else 2\n\n        if current_line.leaves:\n            # Consume the first leaf's extra newlines.\n            first_leaf = current_line.leaves[0]\n            before = first_leaf.prefix.count(\"\\n\")\n            before = min(before, max_allowed)\n            first_leaf.prefix = \"\"\n        else:\n            before = 0\n\n        user_had_newline = bool(before)\n        depth = current_line.depth\n\n        # Mutate self.previous_defs, remainder of this function should be pure\n        previous_def = None\n        while self.previous_defs and self.previous_defs[-1].depth >= depth:\n            previous_def = self.previous_defs.pop()\n        if current_line.is_def or current_line.is_class:\n            self.previous_defs.append(current_line)\n\n        if self.previous_line is None:\n            # Don't insert empty lines before the first line in the file.\n            return 0, 0\n\n        if current_line.is_docstring:\n            if self.previous_line.is_class:\n                return 0, 1\n            if self.previous_line.opens_block and self.previous_line.is_def:\n                return 0, 0\n\n        if previous_def is not None:\n            assert self.previous_line is not None\n            if self.mode.is_pyi:\n                if previous_def.is_class and not previous_def.is_stub_class:\n                    before = 1\n                elif depth and not current_line.is_def and self.previous_line.is_def:\n                    # Empty lines between attributes and methods should be preserved.\n                    before = 1 if user_had_newline else 0\n                elif depth:\n                    before = 0\n                else:\n                    before = 1\n            else:\n                if depth:\n                    before = 1\n                elif (\n                    not depth\n                    and previous_def.depth\n                    and current_line.leaves[-1].type == token.COLON\n                    and (\n                        current_line.leaves[0].value\n                        not in (\"with\", \"try\", \"for\", \"while\", \"if\", \"match\")\n                    )\n                ):\n                    # We shouldn't add two newlines between an indented function and\n                    # a dependent non-indented clause. This is to avoid issues with\n                    # conditional function definitions that are technically top-level\n                    # and therefore get two trailing newlines, but look weird and\n                    # inconsistent when they're followed by elif, else, etc. This is\n                    # worse because these functions only get *one* preceding newline\n                    # already.\n                    before = 1\n                else:\n                    before = 2\n\n        if current_line.is_decorator or current_line.is_def or current_line.is_class:\n            return self._maybe_empty_lines_for_class_or_def(\n                current_line, before, user_had_newline\n            )\n\n        if (\n            self.previous_line.is_import\n            and self.previous_line.depth == 0\n            and current_line.depth == 0\n            and not current_line.is_import\n            and Preview.always_one_newline_after_import in self.mode\n        ):\n            return 1, 0\n\n        if (\n            self.previous_line.is_import\n            and not current_line.is_import\n            and not current_line.is_fmt_pass_converted(first_leaf_matches=is_import)\n            and depth == self.previous_line.depth\n        ):\n            return (before or 1), 0\n\n        return before, 0\n\n    def _maybe_empty_lines_for_class_or_def(  # noqa: C901\n        self, current_line: Line, before: int, user_had_newline: bool\n    ) -> tuple[int, int]:\n        assert self.previous_line is not None\n\n        if self.previous_line.is_decorator:\n            if self.mode.is_pyi and current_line.is_stub_class:\n                # Insert an empty line after a decorated stub class\n                return 0, 1\n            return 0, 0\n\n        if self.previous_line.depth < current_line.depth and (\n            self.previous_line.is_class or self.previous_line.is_def\n        ):\n            if self.mode.is_pyi:\n                return 0, 0\n            return 1 if user_had_newline else 0, 0\n\n        comment_to_add_newlines: Optional[LinesBlock] = None\n        if (\n            self.previous_line.is_comment\n            and self.previous_line.depth == current_line.depth\n            and before == 0\n        ):\n            slc = self.semantic_leading_comment\n            if (\n                slc is not None\n                and slc.previous_block is not None\n                and not slc.previous_block.original_line.is_class\n                and not slc.previous_block.original_line.opens_block\n                and slc.before <= 1\n            ):\n                comment_to_add_newlines = slc\n            else:\n                return 0, 0\n\n        if self.mode.is_pyi:\n            if current_line.is_class or self.previous_line.is_class:\n                if self.previous_line.depth < current_line.depth:\n                    newlines = 0\n                elif self.previous_line.depth > current_line.depth:\n                    newlines = 1\n                elif current_line.is_stub_class and self.previous_line.is_stub_class:\n                    # No blank line between classes with an empty body\n                    newlines = 0\n                else:\n                    newlines = 1\n            # Don't inspect the previous line if it's part of the body of the previous\n            # statement in the same level, we always want a blank line if there's\n            # something with a body preceding.\n            elif self.previous_line.depth > current_line.depth:\n                newlines = 1\n            elif (\n                current_line.is_def or current_line.is_decorator\n            ) and not self.previous_line.is_def:\n                if current_line.depth:\n                    # In classes empty lines between attributes and methods should\n                    # be preserved.\n                    newlines = min(1, before)\n                else:\n                    # Blank line between a block of functions (maybe with preceding\n                    # decorators) and a block of non-functions\n                    newlines = 1\n            else:\n                newlines = 0\n        else:\n            newlines = 1 if current_line.depth else 2\n            # If a user has left no space after a dummy implementation, don't insert\n            # new lines. This is useful for instance for @overload or Protocols.\n            if self.previous_line.is_stub_def and not user_had_newline:\n                newlines = 0\n        if comment_to_add_newlines is not None:\n            previous_block = comment_to_add_newlines.previous_block\n            if previous_block is not None:\n                comment_to_add_newlines.before = (\n                    max(comment_to_add_newlines.before, newlines) - previous_block.after\n                )\n                newlines = 0\n        return newlines, 0"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 730,
      "kind": "function",
      "qualname": "src.black.lines.EmptyLineTracker._maybe_empty_lines_for_class_or_def",
      "span": [
        712,
        790
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _maybe_empty_lines_for_class_or_def(  # noqa: C901\n        self, current_line: Line, before: int, user_had_newline: bool\n    ) -> tuple[int, int]:\n        assert self.previous_line is not None\n\n        if self.previous_line.is_decorator:\n            if self.mode.is_pyi and current_line.is_stub_class:\n                # Insert an empty line after a decorated stub class\n                return 0, 1\n            return 0, 0\n\n        if self.previous_line.depth < current_line.depth and (\n            self.previous_line.is_class or self.previous_line.is_def\n        ):\n            if self.mode.is_pyi:\n                return 0, 0\n            return 1 if user_had_newline else 0, 0\n\n        comment_to_add_newlines: LinesBlock | None = None\n        if (\n            self.previous_line.is_comment\n            and self.previous_line.depth == current_line.depth\n            and before == 0\n        ):\n            slc = self.semantic_leading_comment\n            if (\n                slc is not None\n                and slc.previous_block is not None\n                and not slc.previous_block.original_line.is_class\n                and not slc.previous_block.original_line.opens_block\n                and slc.before <= 1\n            ):\n                comment_to_add_newlines = slc\n            else:\n                return 0, 0\n\n        if self.mode.is_pyi:\n            if current_line.is_class or self.previous_line.is_class:\n                if self.previous_line.depth < current_line.depth:\n                    newlines = 0\n                elif self.previous_line.depth > current_line.depth:\n                    newlines = 1\n                elif current_line.is_stub_class and self.previous_line.is_stub_class:\n                    # No blank line between classes with an empty body\n                    newlines = 0\n                else:\n                    newlines = 1\n            # Don't inspect the previous line if it's part of the body of the previous\n            # statement in the same level, we always want a blank line if there's\n            # something with a body preceding.\n            elif self.previous_line.depth > current_line.depth:\n                newlines = 1\n            elif (\n                current_line.is_def or current_line.is_decorator\n            ) and not self.previous_line.is_def:\n                if current_line.depth:\n                    # In classes empty lines between attributes and methods should\n                    # be preserved.\n                    newlines = min(1, before)\n                else:\n                    # Blank line between a block of functions (maybe with preceding\n                    # decorators) and a block of non-functions\n                    newlines = 1\n            else:\n                newlines = 0\n        else:\n            newlines = 1 if current_line.depth else 2\n            # If a user has left no space after a dummy implementation, don't insert\n            # new lines. This is useful for instance for @overload or Protocols.\n            if self.previous_line.is_stub_def and not user_had_newline:\n                newlines = 0\n        if comment_to_add_newlines is not None:\n            previous_block = comment_to_add_newlines.previous_block\n            if previous_block is not None:\n                comment_to_add_newlines.before = (\n                    max(comment_to_add_newlines.before, newlines) - previous_block.after\n                )\n                newlines = 0\n        return newlines, 0",
      "old_code": "    def _maybe_empty_lines_for_class_or_def(  # noqa: C901\n        self, current_line: Line, before: int, user_had_newline: bool\n    ) -> tuple[int, int]:\n        assert self.previous_line is not None\n\n        if self.previous_line.is_decorator:\n            if self.mode.is_pyi and current_line.is_stub_class:\n                # Insert an empty line after a decorated stub class\n                return 0, 1\n            return 0, 0\n\n        if self.previous_line.depth < current_line.depth and (\n            self.previous_line.is_class or self.previous_line.is_def\n        ):\n            if self.mode.is_pyi:\n                return 0, 0\n            return 1 if user_had_newline else 0, 0\n\n        comment_to_add_newlines: Optional[LinesBlock] = None\n        if (\n            self.previous_line.is_comment\n            and self.previous_line.depth == current_line.depth\n            and before == 0\n        ):\n            slc = self.semantic_leading_comment\n            if (\n                slc is not None\n                and slc.previous_block is not None\n                and not slc.previous_block.original_line.is_class\n                and not slc.previous_block.original_line.opens_block\n                and slc.before <= 1\n            ):\n                comment_to_add_newlines = slc\n            else:\n                return 0, 0\n\n        if self.mode.is_pyi:\n            if current_line.is_class or self.previous_line.is_class:\n                if self.previous_line.depth < current_line.depth:\n                    newlines = 0\n                elif self.previous_line.depth > current_line.depth:\n                    newlines = 1\n                elif current_line.is_stub_class and self.previous_line.is_stub_class:\n                    # No blank line between classes with an empty body\n                    newlines = 0\n                else:\n                    newlines = 1\n            # Don't inspect the previous line if it's part of the body of the previous\n            # statement in the same level, we always want a blank line if there's\n            # something with a body preceding.\n            elif self.previous_line.depth > current_line.depth:\n                newlines = 1\n            elif (\n                current_line.is_def or current_line.is_decorator\n            ) and not self.previous_line.is_def:\n                if current_line.depth:\n                    # In classes empty lines between attributes and methods should\n                    # be preserved.\n                    newlines = min(1, before)\n                else:\n                    # Blank line between a block of functions (maybe with preceding\n                    # decorators) and a block of non-functions\n                    newlines = 1\n            else:\n                newlines = 0\n        else:\n            newlines = 1 if current_line.depth else 2\n            # If a user has left no space after a dummy implementation, don't insert\n            # new lines. This is useful for instance for @overload or Protocols.\n            if self.previous_line.is_stub_def and not user_had_newline:\n                newlines = 0\n        if comment_to_add_newlines is not None:\n            previous_block = comment_to_add_newlines.previous_block\n            if previous_block is not None:\n                comment_to_add_newlines.before = (\n                    max(comment_to_add_newlines.before, newlines) - previous_block.after\n                )\n                newlines = 0\n        return newlines, 0"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 860,
      "kind": "function",
      "qualname": "src.black.lines.is_line_short_enough",
      "span": [
        825,
        919
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def is_line_short_enough(  # noqa: C901\n    line: Line, *, mode: Mode, line_str: str = \"\"\n) -> bool:\n    \"\"\"For non-multiline strings, return True if `line` is no longer than `line_length`.\n    For multiline strings, looks at the context around `line` to determine\n    if it should be inlined or split up.\n    Uses the provided `line_str` rendering, if any, otherwise computes a new one.\n    \"\"\"\n    if not line_str:\n        line_str = line_to_string(line)\n\n    if Preview.multiline_string_handling not in mode:\n        return (\n            str_width(line_str) <= mode.line_length\n            and \"\\n\" not in line_str  # multiline strings\n            and not line.contains_standalone_comments()\n        )\n\n    if line.contains_standalone_comments():\n        return False\n    if \"\\n\" not in line_str:\n        # No multiline strings (MLS) present\n        return str_width(line_str) <= mode.line_length\n\n    first, *_, last = line_str.split(\"\\n\")\n    if str_width(first) > mode.line_length or str_width(last) > mode.line_length:\n        return False\n\n    # Traverse the AST to examine the context of the multiline string (MLS),\n    # tracking aspects such as depth and comma existence,\n    # to determine whether to split the MLS or keep it together.\n    # Depth (which is based on the existing bracket_depth concept)\n    # is needed to determine nesting level of the MLS.\n    # Includes special case for trailing commas.\n    commas: list[int] = []  # tracks number of commas per depth level\n    multiline_string: Leaf | None = None\n    # store the leaves that contain parts of the MLS\n    multiline_string_contexts: list[LN] = []\n\n    max_level_to_update: int | float = math.inf  # track the depth of the MLS\n    for i, leaf in enumerate(line.leaves):\n        if max_level_to_update == math.inf:\n            had_comma: int | None = None\n            if leaf.bracket_depth + 1 > len(commas):\n                commas.append(0)\n            elif leaf.bracket_depth + 1 < len(commas):\n                had_comma = commas.pop()\n            if (\n                had_comma is not None\n                and multiline_string is not None\n                and multiline_string.bracket_depth == leaf.bracket_depth + 1\n            ):\n                # Have left the level with the MLS, stop tracking commas\n                max_level_to_update = leaf.bracket_depth\n                if had_comma > 0:\n                    # MLS was in parens with at least one comma - force split\n                    return False\n\n        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:\n            # Inside brackets, ignore trailing comma\n            # directly after MLS/MLS-containing expression\n            ignore_ctxs: list[LN | None] = [None]\n            ignore_ctxs += multiline_string_contexts\n            if (line.inside_brackets or leaf.bracket_depth > 0) and (\n                i != len(line.leaves) - 1 or leaf.prev_sibling not in ignore_ctxs\n            ):\n                commas[leaf.bracket_depth] += 1\n        if max_level_to_update != math.inf:\n            max_level_to_update = min(max_level_to_update, leaf.bracket_depth)\n\n        if is_multiline_string(leaf):\n            if leaf.parent and (\n                leaf.parent.type == syms.test\n                or (leaf.parent.parent and leaf.parent.parent.type == syms.dictsetmaker)\n            ):\n                # Keep ternary and dictionary values parenthesized\n                return False\n            if len(multiline_string_contexts) > 0:\n                # >1 multiline string cannot fit on a single line - force split\n                return False\n            multiline_string = leaf\n            ctx: LN = leaf\n            # fetch the leaf components of the MLS in the AST\n            while str(ctx) in line_str:\n                multiline_string_contexts.append(ctx)\n                if ctx.parent is None:\n                    break\n                ctx = ctx.parent\n\n    # May not have a triple-quoted multiline string at all,\n    # in case of a regular string with embedded newlines and line continuations\n    if len(multiline_string_contexts) == 0:\n        return True\n\n    return all(val == 0 for val in commas)",
      "old_code": "def is_line_short_enough(  # noqa: C901\n    line: Line, *, mode: Mode, line_str: str = \"\"\n) -> bool:\n    \"\"\"For non-multiline strings, return True if `line` is no longer than `line_length`.\n    For multiline strings, looks at the context around `line` to determine\n    if it should be inlined or split up.\n    Uses the provided `line_str` rendering, if any, otherwise computes a new one.\n    \"\"\"\n    if not line_str:\n        line_str = line_to_string(line)\n\n    if Preview.multiline_string_handling not in mode:\n        return (\n            str_width(line_str) <= mode.line_length\n            and \"\\n\" not in line_str  # multiline strings\n            and not line.contains_standalone_comments()\n        )\n\n    if line.contains_standalone_comments():\n        return False\n    if \"\\n\" not in line_str:\n        # No multiline strings (MLS) present\n        return str_width(line_str) <= mode.line_length\n\n    first, *_, last = line_str.split(\"\\n\")\n    if str_width(first) > mode.line_length or str_width(last) > mode.line_length:\n        return False\n\n    # Traverse the AST to examine the context of the multiline string (MLS),\n    # tracking aspects such as depth and comma existence,\n    # to determine whether to split the MLS or keep it together.\n    # Depth (which is based on the existing bracket_depth concept)\n    # is needed to determine nesting level of the MLS.\n    # Includes special case for trailing commas.\n    commas: list[int] = []  # tracks number of commas per depth level\n    multiline_string: Optional[Leaf] = None\n    # store the leaves that contain parts of the MLS\n    multiline_string_contexts: list[LN] = []\n\n    max_level_to_update: Union[int, float] = math.inf  # track the depth of the MLS\n    for i, leaf in enumerate(line.leaves):\n        if max_level_to_update == math.inf:\n            had_comma: Optional[int] = None\n            if leaf.bracket_depth + 1 > len(commas):\n                commas.append(0)\n            elif leaf.bracket_depth + 1 < len(commas):\n                had_comma = commas.pop()\n            if (\n                had_comma is not None\n                and multiline_string is not None\n                and multiline_string.bracket_depth == leaf.bracket_depth + 1\n            ):\n                # Have left the level with the MLS, stop tracking commas\n                max_level_to_update = leaf.bracket_depth\n                if had_comma > 0:\n                    # MLS was in parens with at least one comma - force split\n                    return False\n\n        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:\n            # Inside brackets, ignore trailing comma\n            # directly after MLS/MLS-containing expression\n            ignore_ctxs: list[Optional[LN]] = [None]\n            ignore_ctxs += multiline_string_contexts\n            if (line.inside_brackets or leaf.bracket_depth > 0) and (\n                i != len(line.leaves) - 1 or leaf.prev_sibling not in ignore_ctxs\n            ):\n                commas[leaf.bracket_depth] += 1\n        if max_level_to_update != math.inf:\n            max_level_to_update = min(max_level_to_update, leaf.bracket_depth)\n\n        if is_multiline_string(leaf):\n            if leaf.parent and (\n                leaf.parent.type == syms.test\n                or (leaf.parent.parent and leaf.parent.parent.type == syms.dictsetmaker)\n            ):\n                # Keep ternary and dictionary values parenthesized\n                return False\n            if len(multiline_string_contexts) > 0:\n                # >1 multiline string cannot fit on a single line - force split\n                return False\n            multiline_string = leaf\n            ctx: LN = leaf\n            # fetch the leaf components of the MLS in the AST\n            while str(ctx) in line_str:\n                multiline_string_contexts.append(ctx)\n                if ctx.parent is None:\n                    break\n                ctx = ctx.parent\n\n    # May not have a triple-quoted multiline string at all,\n    # in case of a regular string with embedded newlines and line continuations\n    if len(multiline_string_contexts) == 0:\n        return True\n\n    return all(val == 0 for val in commas)"
    },
    {
      "path": "src/black/lines.py",
      "version": "new",
      "line": 972,
      "kind": "function",
      "qualname": "src.black.lines.can_omit_invisible_parens",
      "span": [
        958,
        1052
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def can_omit_invisible_parens(\n    rhs: RHSResult,\n    line_length: int,\n) -> bool:\n    \"\"\"Does `rhs.body` have a shape safe to reformat without optional parens around it?\n\n    Returns True for only a subset of potentially nice looking formattings but\n    the point is to not return false positives that end up producing lines that\n    are too long.\n    \"\"\"\n    line = rhs.body\n\n    # We need optional parens in order to split standalone comments to their own lines\n    # if there are no nested parens around the standalone comments\n    closing_bracket: Leaf | None = None\n    for leaf in reversed(line.leaves):\n        if closing_bracket and leaf is closing_bracket.opening_bracket:\n            closing_bracket = None\n        if leaf.type == STANDALONE_COMMENT and not closing_bracket:\n            return False\n        if (\n            not closing_bracket\n            and leaf.type in CLOSING_BRACKETS\n            and leaf.opening_bracket in line.leaves\n            and leaf.value\n        ):\n            closing_bracket = leaf\n\n    bt = line.bracket_tracker\n    if not bt.delimiters:\n        # Without delimiters the optional parentheses are useless.\n        return True\n\n    max_priority = bt.max_delimiter_priority()\n    delimiter_count = bt.delimiter_count_with_priority(max_priority)\n    if delimiter_count > 1:\n        # With more than one delimiter of a kind the optional parentheses read better.\n        return False\n\n    if delimiter_count == 1:\n        if max_priority == COMMA_PRIORITY and rhs.head.is_with_or_async_with_stmt:\n            # For two context manager with statements, the optional parentheses read\n            # better. In this case, `rhs.body` is the context managers part of\n            # the with statement. `rhs.head` is the `with (` part on the previous\n            # line.\n            return False\n        # Otherwise it may also read better, but we don't do it today and requires\n        # careful considerations for all possible cases. See\n        # https://github.com/psf/black/issues/2156.\n\n    if max_priority == DOT_PRIORITY:\n        # A single stranded method call doesn't require optional parentheses.\n        return True\n\n    assert len(line.leaves) >= 2, \"Stranded delimiter\"\n\n    # With a single delimiter, omit if the expression starts or ends with\n    # a bracket.\n    first = line.leaves[0]\n    second = line.leaves[1]\n    if first.type in OPENING_BRACKETS and second.type not in CLOSING_BRACKETS:\n        if _can_omit_opening_paren(line, first=first, line_length=line_length):\n            return True\n\n        # Note: we are not returning False here because a line might have *both*\n        # a leading opening bracket and a trailing closing bracket.  If the\n        # opening bracket doesn't match our rule, maybe the closing will.\n\n    penultimate = line.leaves[-2]\n    last = line.leaves[-1]\n\n    if (\n        last.type == token.RPAR\n        or last.type == token.RBRACE\n        or (\n            # don't use indexing for omitting optional parentheses;\n            # it looks weird\n            last.type == token.RSQB\n            and last.parent\n            and last.parent.type != syms.trailer\n        )\n    ):\n        if penultimate.type in OPENING_BRACKETS:\n            # Empty brackets don't help.\n            return False\n\n        if is_multiline_string(first):\n            # Additional wrapping of a multiline string in this situation is\n            # unnecessary.\n            return True\n\n        if _can_omit_closing_paren(line, last=last, line_length=line_length):\n            return True\n\n    return False",
      "old_code": "def can_omit_invisible_parens(\n    rhs: RHSResult,\n    line_length: int,\n) -> bool:\n    \"\"\"Does `rhs.body` have a shape safe to reformat without optional parens around it?\n\n    Returns True for only a subset of potentially nice looking formattings but\n    the point is to not return false positives that end up producing lines that\n    are too long.\n    \"\"\"\n    line = rhs.body\n\n    # We need optional parens in order to split standalone comments to their own lines\n    # if there are no nested parens around the standalone comments\n    closing_bracket: Optional[Leaf] = None\n    for leaf in reversed(line.leaves):\n        if closing_bracket and leaf is closing_bracket.opening_bracket:\n            closing_bracket = None\n        if leaf.type == STANDALONE_COMMENT and not closing_bracket:\n            return False\n        if (\n            not closing_bracket\n            and leaf.type in CLOSING_BRACKETS\n            and leaf.opening_bracket in line.leaves\n            and leaf.value\n        ):\n            closing_bracket = leaf\n\n    bt = line.bracket_tracker\n    if not bt.delimiters:\n        # Without delimiters the optional parentheses are useless.\n        return True\n\n    max_priority = bt.max_delimiter_priority()\n    delimiter_count = bt.delimiter_count_with_priority(max_priority)\n    if delimiter_count > 1:\n        # With more than one delimiter of a kind the optional parentheses read better.\n        return False\n\n    if delimiter_count == 1:\n        if max_priority == COMMA_PRIORITY and rhs.head.is_with_or_async_with_stmt:\n            # For two context manager with statements, the optional parentheses read\n            # better. In this case, `rhs.body` is the context managers part of\n            # the with statement. `rhs.head` is the `with (` part on the previous\n            # line.\n            return False\n        # Otherwise it may also read better, but we don't do it today and requires\n        # careful considerations for all possible cases. See\n        # https://github.com/psf/black/issues/2156.\n\n    if max_priority == DOT_PRIORITY:\n        # A single stranded method call doesn't require optional parentheses.\n        return True\n\n    assert len(line.leaves) >= 2, \"Stranded delimiter\"\n\n    # With a single delimiter, omit if the expression starts or ends with\n    # a bracket.\n    first = line.leaves[0]\n    second = line.leaves[1]\n    if first.type in OPENING_BRACKETS and second.type not in CLOSING_BRACKETS:\n        if _can_omit_opening_paren(line, first=first, line_length=line_length):\n            return True\n\n        # Note: we are not returning False here because a line might have *both*\n        # a leading opening bracket and a trailing closing bracket.  If the\n        # opening bracket doesn't match our rule, maybe the closing will.\n\n    penultimate = line.leaves[-2]\n    last = line.leaves[-1]\n\n    if (\n        last.type == token.RPAR\n        or last.type == token.RBRACE\n        or (\n            # don't use indexing for omitting optional parentheses;\n            # it looks weird\n            last.type == token.RSQB\n            and last.parent\n            and last.parent.type != syms.trailer\n        )\n    ):\n        if penultimate.type in OPENING_BRACKETS:\n            # Empty brackets don't help.\n            return False\n\n        if is_multiline_string(first):\n            # Additional wrapping of a multiline string in this situation is\n            # unnecessary.\n            return True\n\n        if _can_omit_closing_paren(line, last=last, line_length=line_length):\n            return True\n\n    return False"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 6,
      "kind": "module",
      "qualname": "src.black.nodes",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 429,
      "kind": "function",
      "qualname": "src.black.nodes.preceding_leaf",
      "span": [
        429,
        444
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def preceding_leaf(node: LN | None) -> Leaf | None:\n    \"\"\"Return the first leaf that precedes `node`, if any.\"\"\"\n    while node:\n        res = node.prev_sibling\n        if res:\n            if isinstance(res, Leaf):\n                return res\n\n            try:\n                return list(res.leaves())[-1]\n\n            except IndexError:\n                return None\n\n        node = node.parent\n    return None",
      "old_code": "def preceding_leaf(node: Optional[LN]) -> Optional[Leaf]:\n    \"\"\"Return the first leaf that precedes `node`, if any.\"\"\"\n    while node:\n        res = node.prev_sibling\n        if res:\n            if isinstance(res, Leaf):\n                return res\n\n            try:\n                return list(res.leaves())[-1]\n\n            except IndexError:\n                return None\n\n        node = node.parent\n    return None"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 447,
      "kind": "function",
      "qualname": "src.black.nodes.prev_siblings_are",
      "span": [
        447,
        460
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def prev_siblings_are(node: LN | None, tokens: list[NodeType | None]) -> bool:\n    \"\"\"Return if the `node` and its previous siblings match types against the provided\n    list of tokens; the provided `node`has its type matched against the last element in\n    the list.  `None` can be used as the first element to declare that the start of the\n    list is anchored at the start of its parent's children.\"\"\"\n    if not tokens:\n        return True\n    if tokens[-1] is None:\n        return node is None\n    if not node:\n        return False\n    if node.type != tokens[-1]:\n        return False\n    return prev_siblings_are(node.prev_sibling, tokens[:-1])",
      "old_code": "def prev_siblings_are(node: Optional[LN], tokens: list[Optional[NodeType]]) -> bool:\n    \"\"\"Return if the `node` and its previous siblings match types against the provided\n    list of tokens; the provided `node`has its type matched against the last element in\n    the list.  `None` can be used as the first element to declare that the start of the\n    list is anchored at the start of its parent's children.\"\"\"\n    if not tokens:\n        return True\n    if tokens[-1] is None:\n        return node is None\n    if not node:\n        return False\n    if node.type != tokens[-1]:\n        return False\n    return prev_siblings_are(node.prev_sibling, tokens[:-1])"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 463,
      "kind": "function",
      "qualname": "src.black.nodes.parent_type",
      "span": [
        463,
        473
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def parent_type(node: LN | None) -> NodeType | None:\n    \"\"\"\n    Returns:\n        @node.parent.type, if @node is not None and has a parent.\n            OR\n        None, otherwise.\n    \"\"\"\n    if node is None or node.parent is None:\n        return None\n\n    return node.parent.type",
      "old_code": "def parent_type(node: Optional[LN]) -> Optional[NodeType]:\n    \"\"\"\n    Returns:\n        @node.parent.type, if @node is not None and has a parent.\n            OR\n        None, otherwise.\n    \"\"\"\n    if node is None or node.parent is None:\n        return None\n\n    return node.parent.type"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 476,
      "kind": "function",
      "qualname": "src.black.nodes.child_towards",
      "span": [
        476,
        481
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def child_towards(ancestor: Node, descendant: LN) -> LN | None:\n    \"\"\"Return the child of `ancestor` that contains `descendant`.\"\"\"\n    node: LN | None = descendant\n    while node and node.parent != ancestor:\n        node = node.parent\n    return node",
      "old_code": "def child_towards(ancestor: Node, descendant: LN) -> Optional[LN]:\n    \"\"\"Return the child of `ancestor` that contains `descendant`.\"\"\"\n    node: Optional[LN] = descendant\n    while node and node.parent != ancestor:\n        node = node.parent\n    return node"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 526,
      "kind": "function",
      "qualname": "src.black.nodes.first_leaf_of",
      "span": [
        526,
        533
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def first_leaf_of(node: LN) -> Leaf | None:\n    \"\"\"Returns the first leaf of the node tree.\"\"\"\n    if isinstance(node, Leaf):\n        return node\n    if node.children:\n        return first_leaf_of(node.children[0])\n    else:\n        return None",
      "old_code": "def first_leaf_of(node: LN) -> Optional[Leaf]:\n    \"\"\"Returns the first leaf of the node tree.\"\"\"\n    if isinstance(node, Leaf):\n        return node\n    if node.children:\n        return first_leaf_of(node.children[0])\n    else:\n        return None"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 990,
      "kind": "function",
      "qualname": "src.black.nodes.unwrap_singleton_parenthesis",
      "span": [
        990,
        1001
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def unwrap_singleton_parenthesis(node: LN) -> LN | None:\n    \"\"\"Returns `wrapped` if `node` is of the shape ( wrapped ).\n\n    Parenthesis can be optional. Returns None otherwise\"\"\"\n    if len(node.children) != 3:\n        return None\n\n    lpar, wrapped, rpar = node.children\n    if not (lpar.type == token.LPAR and rpar.type == token.RPAR):\n        return None\n\n    return wrapped",
      "old_code": "def unwrap_singleton_parenthesis(node: LN) -> Optional[LN]:\n    \"\"\"Returns `wrapped` if `node` is of the shape ( wrapped ).\n\n    Parenthesis can be optional. Returns None otherwise\"\"\"\n    if len(node.children) != 3:\n        return None\n\n    lpar, wrapped, rpar = node.children\n    if not (lpar.type == token.LPAR and rpar.type == token.RPAR):\n        return None\n\n    return wrapped"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 1050,
      "kind": "function",
      "qualname": "src.black.nodes.first_leaf",
      "span": [
        1050,
        1057
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def first_leaf(node: LN) -> Leaf | None:\n    \"\"\"Returns the first leaf of the ancestor node.\"\"\"\n    if isinstance(node, Leaf):\n        return node\n    elif not node.children:\n        return None\n    else:\n        return first_leaf(node.children[0])",
      "old_code": "def first_leaf(node: LN) -> Optional[Leaf]:\n    \"\"\"Returns the first leaf of the ancestor node.\"\"\"\n    if isinstance(node, Leaf):\n        return node\n    elif not node.children:\n        return None\n    else:\n        return first_leaf(node.children[0])"
    },
    {
      "path": "src/black/nodes.py",
      "version": "new",
      "line": 1060,
      "kind": "function",
      "qualname": "src.black.nodes.last_leaf",
      "span": [
        1060,
        1067
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def last_leaf(node: LN) -> Leaf | None:\n    \"\"\"Returns the last leaf of the ancestor node.\"\"\"\n    if isinstance(node, Leaf):\n        return node\n    elif not node.children:\n        return None\n    else:\n        return last_leaf(node.children[-1])",
      "old_code": "def last_leaf(node: LN) -> Optional[Leaf]:\n    \"\"\"Returns the last leaf of the ancestor node.\"\"\"\n    if isinstance(node, Leaf):\n        return node\n    elif not node.children:\n        return None\n    else:\n        return last_leaf(node.children[-1])"
    },
    {
      "path": "src/black/output.py",
      "version": "new",
      "line": 9,
      "kind": "module",
      "qualname": "src.black.output",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/output.py",
      "version": "new",
      "line": 16,
      "kind": "function",
      "qualname": "src.black.output._out",
      "span": [
        16,
        21
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _out(message: str | None = None, nl: bool = True, **styles: Any) -> None:\n    if message is not None:\n        if \"bold\" not in styles:\n            styles[\"bold\"] = True\n        message = style(message, **styles)\n    echo(message, nl=nl, err=True)",
      "old_code": "def _out(message: Optional[str] = None, nl: bool = True, **styles: Any) -> None:\n    if message is not None:\n        if \"bold\" not in styles:\n            styles[\"bold\"] = True\n        message = style(message, **styles)\n    echo(message, nl=nl, err=True)"
    },
    {
      "path": "src/black/output.py",
      "version": "new",
      "line": 25,
      "kind": "function",
      "qualname": "src.black.output._err",
      "span": [
        25,
        30
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _err(message: str | None = None, nl: bool = True, **styles: Any) -> None:\n    if message is not None:\n        if \"fg\" not in styles:\n            styles[\"fg\"] = \"red\"\n        message = style(message, **styles)\n    echo(message, nl=nl, err=True)",
      "old_code": "def _err(message: Optional[str] = None, nl: bool = True, **styles: Any) -> None:\n    if message is not None:\n        if \"fg\" not in styles:\n            styles[\"fg\"] = \"red\"\n        message = style(message, **styles)\n    echo(message, nl=nl, err=True)"
    },
    {
      "path": "src/black/output.py",
      "version": "new",
      "line": 34,
      "kind": "function",
      "qualname": "src.black.output.out",
      "span": [
        34,
        35
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def out(message: str | None = None, nl: bool = True, **styles: Any) -> None:\n    _out(message, nl=nl, **styles)",
      "old_code": "def out(message: Optional[str] = None, nl: bool = True, **styles: Any) -> None:\n    _out(message, nl=nl, **styles)"
    },
    {
      "path": "src/black/output.py",
      "version": "new",
      "line": 38,
      "kind": "function",
      "qualname": "src.black.output.err",
      "span": [
        38,
        39
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def err(message: str | None = None, nl: bool = True, **styles: Any) -> None:\n    _err(message, nl=nl, **styles)",
      "old_code": "def err(message: Optional[str] = None, nl: bool = True, **styles: Any) -> None:\n    _err(message, nl=nl, **styles)"
    },
    {
      "path": "src/black/ranges.py",
      "version": "new",
      "line": 398,
      "kind": "function",
      "qualname": "src.black.ranges._get_line_range",
      "span": [
        398,
        422
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _get_line_range(node_or_nodes: LN | list[LN]) -> set[int]:\n    \"\"\"Returns the line range of this node or list of nodes.\"\"\"\n    if isinstance(node_or_nodes, list):\n        nodes = node_or_nodes\n        if not nodes:\n            return set()\n        first = first_leaf(nodes[0])\n        last = last_leaf(nodes[-1])\n        if first and last:\n            line_start = first.lineno\n            line_end = _leaf_line_end(last)\n            return set(range(line_start, line_end + 1))\n        else:\n            return set()\n    else:\n        node = node_or_nodes\n        if isinstance(node, Leaf):\n            return set(range(node.lineno, _leaf_line_end(node) + 1))\n        else:\n            first = first_leaf(node)\n            last = last_leaf(node)\n            if first and last:\n                return set(range(first.lineno, _leaf_line_end(last) + 1))\n            else:\n                return set()",
      "old_code": "def _get_line_range(node_or_nodes: Union[LN, list[LN]]) -> set[int]:\n    \"\"\"Returns the line range of this node or list of nodes.\"\"\"\n    if isinstance(node_or_nodes, list):\n        nodes = node_or_nodes\n        if not nodes:\n            return set()\n        first = first_leaf(nodes[0])\n        last = last_leaf(nodes[-1])\n        if first and last:\n            line_start = first.lineno\n            line_end = _leaf_line_end(last)\n            return set(range(line_start, line_end + 1))\n        else:\n            return set()\n    else:\n        node = node_or_nodes\n        if isinstance(node, Leaf):\n            return set(range(node.lineno, _leaf_line_end(node) + 1))\n        else:\n            first = first_leaf(node)\n            last = last_leaf(node)\n            if first and last:\n                return set(range(first.lineno, _leaf_line_end(last) + 1))\n            else:\n                return set()"
    },
    {
      "path": "src/black/strings.py",
      "version": "new",
      "line": 291,
      "kind": "function",
      "qualname": "src.black.strings.normalize_fstring_quotes",
      "span": [
        242,
        304
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def normalize_fstring_quotes(\n    quote: str,\n    middles: list[Leaf],\n    is_raw_fstring: bool,\n) -> tuple[list[Leaf], str]:\n    \"\"\"Prefer double quotes but only if it doesn't cause more escaping.\n\n    Adds or removes backslashes as appropriate.\n    \"\"\"\n    if quote == '\"\"\"':\n        return middles, quote\n\n    elif quote == \"'''\":\n        new_quote = '\"\"\"'\n    elif quote == '\"':\n        new_quote = \"'\"\n    else:\n        new_quote = '\"'\n\n    unescaped_new_quote = _cached_compile(rf\"(([^\\\\]|^)(\\\\\\\\)*){new_quote}\")\n    escaped_new_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){new_quote}\")\n    escaped_orig_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){quote}\")\n    if is_raw_fstring:\n        for middle in middles:\n            if unescaped_new_quote.search(middle.value):\n                # There's at least one unescaped new_quote in this raw string\n                # so converting is impossible\n                return middles, quote\n\n        # Do not introduce or remove backslashes in raw strings, just use double quote\n        return middles, '\"'\n\n    new_segments = []\n    for middle in middles:\n        segment = middle.value\n        # remove unnecessary escapes\n        new_segment = sub_twice(escaped_new_quote, rf\"\\1\\2{new_quote}\", segment)\n        if segment != new_segment:\n            # Consider the string without unnecessary escapes as the original\n            middle.value = new_segment\n\n        new_segment = sub_twice(escaped_orig_quote, rf\"\\1\\2{quote}\", new_segment)\n        new_segment = sub_twice(unescaped_new_quote, rf\"\\1\\\\{new_quote}\", new_segment)\n        new_segments.append(new_segment)\n\n    if new_quote == '\"\"\"' and new_segments[-1].endswith('\"'):\n        # edge case:\n        new_segments[-1] = new_segments[-1][:-1] + '\\\\\"'\n\n    for middle, new_segment in zip(middles, new_segments, strict=True):\n        orig_escape_count = middle.value.count(\"\\\\\")\n        new_escape_count = new_segment.count(\"\\\\\")\n\n    if new_escape_count > orig_escape_count:\n        return middles, quote  # Do not introduce more escaping\n\n    if new_escape_count == orig_escape_count and quote == '\"':\n        return middles, quote  # Prefer double quotes\n\n    for middle, new_segment in zip(middles, new_segments, strict=True):\n        middle.value = new_segment\n\n    return middles, new_quote",
      "old_code": "def normalize_fstring_quotes(\n    quote: str,\n    middles: list[Leaf],\n    is_raw_fstring: bool,\n) -> tuple[list[Leaf], str]:\n    \"\"\"Prefer double quotes but only if it doesn't cause more escaping.\n\n    Adds or removes backslashes as appropriate.\n    \"\"\"\n    if quote == '\"\"\"':\n        return middles, quote\n\n    elif quote == \"'''\":\n        new_quote = '\"\"\"'\n    elif quote == '\"':\n        new_quote = \"'\"\n    else:\n        new_quote = '\"'\n\n    unescaped_new_quote = _cached_compile(rf\"(([^\\\\]|^)(\\\\\\\\)*){new_quote}\")\n    escaped_new_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){new_quote}\")\n    escaped_orig_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){quote}\")\n    if is_raw_fstring:\n        for middle in middles:\n            if unescaped_new_quote.search(middle.value):\n                # There's at least one unescaped new_quote in this raw string\n                # so converting is impossible\n                return middles, quote\n\n        # Do not introduce or remove backslashes in raw strings, just use double quote\n        return middles, '\"'\n\n    new_segments = []\n    for middle in middles:\n        segment = middle.value\n        # remove unnecessary escapes\n        new_segment = sub_twice(escaped_new_quote, rf\"\\1\\2{new_quote}\", segment)\n        if segment != new_segment:\n            # Consider the string without unnecessary escapes as the original\n            middle.value = new_segment\n\n        new_segment = sub_twice(escaped_orig_quote, rf\"\\1\\2{quote}\", new_segment)\n        new_segment = sub_twice(unescaped_new_quote, rf\"\\1\\\\{new_quote}\", new_segment)\n        new_segments.append(new_segment)\n\n    if new_quote == '\"\"\"' and new_segments[-1].endswith('\"'):\n        # edge case:\n        new_segments[-1] = new_segments[-1][:-1] + '\\\\\"'\n\n    for middle, new_segment in zip(middles, new_segments):\n        orig_escape_count = middle.value.count(\"\\\\\")\n        new_escape_count = new_segment.count(\"\\\\\")\n\n    if new_escape_count > orig_escape_count:\n        return middles, quote  # Do not introduce more escaping\n\n    if new_escape_count == orig_escape_count and quote == '\"':\n        return middles, quote  # Prefer double quotes\n\n    for middle, new_segment in zip(middles, new_segments):\n        middle.value = new_segment\n\n    return middles, new_quote"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 10,
      "kind": "module",
      "qualname": "src.black.trans",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 1264,
      "kind": "function",
      "qualname": "src.black.trans.BaseStringSplitter._prefer_paren_wrap_match",
      "span": [
        1264,
        1306
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _prefer_paren_wrap_match(LL: list[Leaf]) -> int | None:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the \"prefer paren wrap\" statement\n            requirements listed in the 'Requirements' section of the StringParenWrapper\n            class's docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # The line must start with a string.\n        if LL[0].type != token.STRING:\n            return None\n\n        matching_nodes = [\n            syms.listmaker,\n            syms.dictsetmaker,\n            syms.testlist_gexp,\n        ]\n        # If the string is an immediate child of a list/set/tuple literal...\n        if (\n            parent_type(LL[0]) in matching_nodes\n            or parent_type(LL[0].parent) in matching_nodes\n        ):\n            # And the string is surrounded by commas (or is the first/last child)...\n            prev_sibling = LL[0].prev_sibling\n            next_sibling = LL[0].next_sibling\n            if (\n                not prev_sibling\n                and not next_sibling\n                and parent_type(LL[0]) == syms.atom\n            ):\n                # If it's an atom string, we need to check the parent atom's siblings.\n                parent = LL[0].parent\n                assert parent is not None  # For type checkers.\n                prev_sibling = parent.prev_sibling\n                next_sibling = parent.next_sibling\n            if (not prev_sibling or prev_sibling.type == token.COMMA) and (\n                not next_sibling or next_sibling.type == token.COMMA\n            ):\n                return 0\n\n        return None",
      "old_code": "    def _prefer_paren_wrap_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the \"prefer paren wrap\" statement\n            requirements listed in the 'Requirements' section of the StringParenWrapper\n            class's docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # The line must start with a string.\n        if LL[0].type != token.STRING:\n            return None\n\n        matching_nodes = [\n            syms.listmaker,\n            syms.dictsetmaker,\n            syms.testlist_gexp,\n        ]\n        # If the string is an immediate child of a list/set/tuple literal...\n        if (\n            parent_type(LL[0]) in matching_nodes\n            or parent_type(LL[0].parent) in matching_nodes\n        ):\n            # And the string is surrounded by commas (or is the first/last child)...\n            prev_sibling = LL[0].prev_sibling\n            next_sibling = LL[0].next_sibling\n            if (\n                not prev_sibling\n                and not next_sibling\n                and parent_type(LL[0]) == syms.atom\n            ):\n                # If it's an atom string, we need to check the parent atom's siblings.\n                parent = LL[0].parent\n                assert parent is not None  # For type checkers.\n                prev_sibling = parent.prev_sibling\n                next_sibling = parent.next_sibling\n            if (not prev_sibling or prev_sibling.type == token.COMMA) and (\n                not next_sibling or next_sibling.type == token.COMMA\n            ):\n                return 0\n\n        return None"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 1763,
      "kind": "function",
      "qualname": "src.black.trans.StringSplitter._get_break_idx",
      "span": [
        1763,
        1846
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _get_break_idx(self, string: str, max_break_idx: int) -> int | None:\n        \"\"\"\n        This method contains the algorithm that StringSplitter uses to\n        determine which character to split each string at.\n\n        Args:\n            @string: The substring that we are attempting to split.\n            @max_break_idx: The ideal break index. We will return this value if it\n            meets all the necessary conditions. In the likely event that it\n            doesn't we will try to find the closest index BELOW @max_break_idx\n            that does. If that fails, we will expand our search by also\n            considering all valid indices ABOVE @max_break_idx.\n\n        Pre-Conditions:\n            * assert_is_leaf_string(@string)\n            * 0 <= @max_break_idx < len(@string)\n\n        Returns:\n            break_idx, if an index is able to be found that meets all of the\n            conditions listed in the 'Transformations' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        is_valid_index = is_valid_index_factory(string)\n\n        assert is_valid_index(max_break_idx)\n        assert_is_leaf_string(string)\n\n        _illegal_split_indices = self._get_illegal_split_indices(string)\n\n        def breaks_unsplittable_expression(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff returning @i would result in the splitting of an\n                unsplittable expression (which is NOT allowed).\n            \"\"\"\n            return i in _illegal_split_indices\n\n        def passes_all_checks(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff ALL of the conditions listed in the 'Transformations'\n                section of this classes' docstring would be met by returning @i.\n            \"\"\"\n            is_space = string[i] == \" \"\n            is_split_safe = is_valid_index(i - 1) and string[i - 1] in SPLIT_SAFE_CHARS\n\n            is_not_escaped = True\n            j = i - 1\n            while is_valid_index(j) and string[j] == \"\\\\\":\n                is_not_escaped = not is_not_escaped\n                j -= 1\n\n            is_big_enough = (\n                len(string[i:]) >= self.MIN_SUBSTR_SIZE\n                and len(string[:i]) >= self.MIN_SUBSTR_SIZE\n            )\n            return (\n                (is_space or is_split_safe)\n                and is_not_escaped\n                and is_big_enough\n                and not breaks_unsplittable_expression(i)\n            )\n\n        # First, we check all indices BELOW @max_break_idx.\n        break_idx = max_break_idx\n        while is_valid_index(break_idx - 1) and not passes_all_checks(break_idx):\n            break_idx -= 1\n\n        if not passes_all_checks(break_idx):\n            # If that fails, we check all indices ABOVE @max_break_idx.\n            #\n            # If we are able to find a valid index here, the next line is going\n            # to be longer than the specified line length, but it's probably\n            # better than doing nothing at all.\n            break_idx = max_break_idx + 1\n            while is_valid_index(break_idx + 1) and not passes_all_checks(break_idx):\n                break_idx += 1\n\n            if not is_valid_index(break_idx) or not passes_all_checks(break_idx):\n                return None\n\n        return break_idx",
      "old_code": "    def _get_break_idx(self, string: str, max_break_idx: int) -> Optional[int]:\n        \"\"\"\n        This method contains the algorithm that StringSplitter uses to\n        determine which character to split each string at.\n\n        Args:\n            @string: The substring that we are attempting to split.\n            @max_break_idx: The ideal break index. We will return this value if it\n            meets all the necessary conditions. In the likely event that it\n            doesn't we will try to find the closest index BELOW @max_break_idx\n            that does. If that fails, we will expand our search by also\n            considering all valid indices ABOVE @max_break_idx.\n\n        Pre-Conditions:\n            * assert_is_leaf_string(@string)\n            * 0 <= @max_break_idx < len(@string)\n\n        Returns:\n            break_idx, if an index is able to be found that meets all of the\n            conditions listed in the 'Transformations' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        is_valid_index = is_valid_index_factory(string)\n\n        assert is_valid_index(max_break_idx)\n        assert_is_leaf_string(string)\n\n        _illegal_split_indices = self._get_illegal_split_indices(string)\n\n        def breaks_unsplittable_expression(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff returning @i would result in the splitting of an\n                unsplittable expression (which is NOT allowed).\n            \"\"\"\n            return i in _illegal_split_indices\n\n        def passes_all_checks(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff ALL of the conditions listed in the 'Transformations'\n                section of this classes' docstring would be met by returning @i.\n            \"\"\"\n            is_space = string[i] == \" \"\n            is_split_safe = is_valid_index(i - 1) and string[i - 1] in SPLIT_SAFE_CHARS\n\n            is_not_escaped = True\n            j = i - 1\n            while is_valid_index(j) and string[j] == \"\\\\\":\n                is_not_escaped = not is_not_escaped\n                j -= 1\n\n            is_big_enough = (\n                len(string[i:]) >= self.MIN_SUBSTR_SIZE\n                and len(string[:i]) >= self.MIN_SUBSTR_SIZE\n            )\n            return (\n                (is_space or is_split_safe)\n                and is_not_escaped\n                and is_big_enough\n                and not breaks_unsplittable_expression(i)\n            )\n\n        # First, we check all indices BELOW @max_break_idx.\n        break_idx = max_break_idx\n        while is_valid_index(break_idx - 1) and not passes_all_checks(break_idx):\n            break_idx -= 1\n\n        if not passes_all_checks(break_idx):\n            # If that fails, we check all indices ABOVE @max_break_idx.\n            #\n            # If we are able to find a valid index here, the next line is going\n            # to be longer than the specified line length, but it's probably\n            # better than doing nothing at all.\n            break_idx = max_break_idx + 1\n            while is_valid_index(break_idx + 1) and not passes_all_checks(break_idx):\n                break_idx += 1\n\n            if not is_valid_index(break_idx) or not passes_all_checks(break_idx):\n                return None\n\n        return break_idx"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 1988,
      "kind": "function",
      "qualname": "src.black.trans.StringParenWrapper._return_match",
      "span": [
        1988,
        2010
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _return_match(LL: list[Leaf]) -> int | None:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the return/yield statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a return/yield statement and the first leaf\n        # contains either the \"return\" or \"yield\" keywords...\n        if parent_type(LL[0]) in [syms.return_stmt, syms.yield_expr] and LL[\n            0\n        ].value in [\"return\", \"yield\"]:\n            is_valid_index = is_valid_index_factory(LL)\n\n            idx = 2 if is_valid_index(1) and is_empty_par(LL[1]) else 1\n            # The next visible leaf MUST contain a string...\n            if is_valid_index(idx) and LL[idx].type == token.STRING:\n                return idx\n\n        return None",
      "old_code": "    def _return_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the return/yield statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a return/yield statement and the first leaf\n        # contains either the \"return\" or \"yield\" keywords...\n        if parent_type(LL[0]) in [syms.return_stmt, syms.yield_expr] and LL[\n            0\n        ].value in [\"return\", \"yield\"]:\n            is_valid_index = is_valid_index_factory(LL)\n\n            idx = 2 if is_valid_index(1) and is_empty_par(LL[1]) else 1\n            # The next visible leaf MUST contain a string...\n            if is_valid_index(idx) and LL[idx].type == token.STRING:\n                return idx\n\n        return None"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 2013,
      "kind": "function",
      "qualname": "src.black.trans.StringParenWrapper._else_match",
      "span": [
        2013,
        2037
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _else_match(LL: list[Leaf]) -> int | None:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the ternary expression\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a ternary expression and the first leaf\n        # contains the \"else\" keyword...\n        if (\n            parent_type(LL[0]) == syms.test\n            and LL[0].type == token.NAME\n            and LL[0].value == \"else\"\n        ):\n            is_valid_index = is_valid_index_factory(LL)\n\n            idx = 2 if is_valid_index(1) and is_empty_par(LL[1]) else 1\n            # The next visible leaf MUST contain a string...\n            if is_valid_index(idx) and LL[idx].type == token.STRING:\n                return idx\n\n        return None",
      "old_code": "    def _else_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the ternary expression\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a ternary expression and the first leaf\n        # contains the \"else\" keyword...\n        if (\n            parent_type(LL[0]) == syms.test\n            and LL[0].type == token.NAME\n            and LL[0].value == \"else\"\n        ):\n            is_valid_index = is_valid_index_factory(LL)\n\n            idx = 2 if is_valid_index(1) and is_empty_par(LL[1]) else 1\n            # The next visible leaf MUST contain a string...\n            if is_valid_index(idx) and LL[idx].type == token.STRING:\n                return idx\n\n        return None"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 2040,
      "kind": "function",
      "qualname": "src.black.trans.StringParenWrapper._assert_match",
      "span": [
        2040,
        2072
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _assert_match(LL: list[Leaf]) -> int | None:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the assert statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of an assert statement and the first leaf\n        # contains the \"assert\" keyword...\n        if parent_type(LL[0]) == syms.assert_stmt and LL[0].value == \"assert\":\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find a comma...\n                if leaf.type == token.COMMA:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That comma MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None",
      "old_code": "    def _assert_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the assert statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of an assert statement and the first leaf\n        # contains the \"assert\" keyword...\n        if parent_type(LL[0]) == syms.assert_stmt and LL[0].value == \"assert\":\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find a comma...\n                if leaf.type == token.COMMA:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That comma MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 2075,
      "kind": "function",
      "qualname": "src.black.trans.StringParenWrapper._assign_match",
      "span": [
        2075,
        2119
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _assign_match(LL: list[Leaf]) -> int | None:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the assignment statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of an expression statement or is a function\n        # argument AND the first leaf contains a variable name...\n        if (\n            parent_type(LL[0]) in [syms.expr_stmt, syms.argument, syms.power]\n            and LL[0].type == token.NAME\n        ):\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find either an '=' or '+=' symbol...\n                if leaf.type in [token.EQUAL, token.PLUSEQUAL]:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That symbol MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # The next leaf MAY be a comma iff this line is a part\n                        # of a function argument...\n                        if (\n                            parent_type(LL[0]) == syms.argument\n                            and is_valid_index(idx)\n                            and LL[idx].type == token.COMMA\n                        ):\n                            idx += 1\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None",
      "old_code": "    def _assign_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the assignment statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of an expression statement or is a function\n        # argument AND the first leaf contains a variable name...\n        if (\n            parent_type(LL[0]) in [syms.expr_stmt, syms.argument, syms.power]\n            and LL[0].type == token.NAME\n        ):\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find either an '=' or '+=' symbol...\n                if leaf.type in [token.EQUAL, token.PLUSEQUAL]:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That symbol MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # The next leaf MAY be a comma iff this line is a part\n                        # of a function argument...\n                        if (\n                            parent_type(LL[0]) == syms.argument\n                            and is_valid_index(idx)\n                            and LL[idx].type == token.COMMA\n                        ):\n                            idx += 1\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None"
    },
    {
      "path": "src/black/trans.py",
      "version": "new",
      "line": 2122,
      "kind": "function",
      "qualname": "src.black.trans.StringParenWrapper._dict_or_lambda_match",
      "span": [
        2122,
        2158
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def _dict_or_lambda_match(LL: list[Leaf]) -> int | None:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the dictionary key assignment\n            statement or lambda expression requirements listed in the\n            'Requirements' section of this classes' docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a dictionary key assignment or lambda expression...\n        parent_types = [parent_type(LL[0]), parent_type(LL[0].parent)]\n        if syms.dictsetmaker in parent_types or syms.lambdef in parent_types:\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find a colon, it can either be dict's or lambda's colon...\n                if leaf.type == token.COLON and i < len(LL) - 1:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That colon MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # That string MAY be followed by a comma...\n                        if is_valid_index(idx) and LL[idx].type == token.COMMA:\n                            idx += 1\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None",
      "old_code": "    def _dict_or_lambda_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the dictionary key assignment\n            statement or lambda expression requirements listed in the\n            'Requirements' section of this classes' docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a dictionary key assignment or lambda expression...\n        parent_types = [parent_type(LL[0]), parent_type(LL[0].parent)]\n        if syms.dictsetmaker in parent_types or syms.lambdef in parent_types:\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find a colon, it can either be dict's or lambda's colon...\n                if leaf.type == token.COLON and i < len(LL) - 1:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That colon MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # That string MAY be followed by a comma...\n                        if is_valid_index(idx) and LL[idx].type == token.COMMA:\n                            idx += 1\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None"
    },
    {
      "path": "src/blackd/client.py",
      "version": "new",
      "line": 13,
      "kind": "function",
      "qualname": "src.blackd.client.BlackDClient.__init__",
      "span": [
        10,
        67
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self,\n        url: StrOrURL = \"http://localhost:9090\",\n        line_length: int | None = None,\n        skip_source_first_line: bool = False,\n        skip_string_normalization: bool = False,\n        skip_magic_trailing_comma: bool = False,\n        preview: bool = False,\n        fast: bool = False,\n        python_variant: str | None = None,\n        diff: bool = False,\n        headers: dict[str, str] | None = None,\n    ):\n        \"\"\"\n        Initialize a BlackDClient object.\n        :param url: The URL of the BlackD server.\n        :param line_length: The maximum line length.\n            Corresponds to the ``--line-length`` CLI option.\n        :param skip_source_first_line: True to skip the first line of the source.\n            Corresponds to the ``--skip-source-first-line`` CLI option.\n        :param skip_string_normalization: True to skip string normalization.\n            Corresponds to the ``--skip-string-normalization`` CLI option.\n        :param skip_magic_trailing_comma: True to skip magic trailing comma.\n            Corresponds to the ``--skip-magic-trailing-comma`` CLI option.\n        :param preview: True to enable experimental preview mode.\n            Corresponds to the ``--preview`` CLI option.\n        :param fast: True to enable fast mode.\n            Corresponds to the ``--fast`` CLI option.\n        :param python_variant: The Python variant to use.\n            Corresponds to the ``--pyi`` CLI option if this is \"pyi\".\n            Otherwise, corresponds to the ``--target-version`` CLI option.\n        :param diff: True to enable diff mode.\n            Corresponds to the ``--diff`` CLI option.\n        :param headers: A dictionary of additional custom headers to send with\n            the request.\n        \"\"\"\n        self.url = url\n        self.headers = _DEFAULT_HEADERS.copy()\n\n        if line_length is not None:\n            self.headers[\"X-Line-Length\"] = str(line_length)\n        if skip_source_first_line:\n            self.headers[\"X-Skip-Source-First-Line\"] = \"yes\"\n        if skip_string_normalization:\n            self.headers[\"X-Skip-String-Normalization\"] = \"yes\"\n        if skip_magic_trailing_comma:\n            self.headers[\"X-Skip-Magic-Trailing-Comma\"] = \"yes\"\n        if preview:\n            self.headers[\"X-Preview\"] = \"yes\"\n        if fast:\n            self.headers[\"X-Fast-Or-Safe\"] = \"fast\"\n        if python_variant is not None:\n            self.headers[\"X-Python-Variant\"] = python_variant\n        if diff:\n            self.headers[\"X-Diff\"] = \"yes\"\n\n        if headers is not None:\n            self.headers.update(headers)",
      "old_code": "    def __init__(\n        self,\n        url: StrOrURL = \"http://localhost:9090\",\n        line_length: Optional[int] = None,\n        skip_source_first_line: bool = False,\n        skip_string_normalization: bool = False,\n        skip_magic_trailing_comma: bool = False,\n        preview: bool = False,\n        fast: bool = False,\n        python_variant: Optional[str] = None,\n        diff: bool = False,\n        headers: Optional[dict[str, str]] = None,\n    ):\n        \"\"\"\n        Initialize a BlackDClient object.\n        :param url: The URL of the BlackD server.\n        :param line_length: The maximum line length.\n            Corresponds to the ``--line-length`` CLI option.\n        :param skip_source_first_line: True to skip the first line of the source.\n            Corresponds to the ``--skip-source-first-line`` CLI option.\n        :param skip_string_normalization: True to skip string normalization.\n            Corresponds to the ``--skip-string-normalization`` CLI option.\n        :param skip_magic_trailing_comma: True to skip magic trailing comma.\n            Corresponds to the ``--skip-magic-trailing-comma`` CLI option.\n        :param preview: True to enable experimental preview mode.\n            Corresponds to the ``--preview`` CLI option.\n        :param fast: True to enable fast mode.\n            Corresponds to the ``--fast`` CLI option.\n        :param python_variant: The Python variant to use.\n            Corresponds to the ``--pyi`` CLI option if this is \"pyi\".\n            Otherwise, corresponds to the ``--target-version`` CLI option.\n        :param diff: True to enable diff mode.\n            Corresponds to the ``--diff`` CLI option.\n        :param headers: A dictionary of additional custom headers to send with\n            the request.\n        \"\"\"\n        self.url = url\n        self.headers = _DEFAULT_HEADERS.copy()\n\n        if line_length is not None:\n            self.headers[\"X-Line-Length\"] = str(line_length)\n        if skip_source_first_line:\n            self.headers[\"X-Skip-Source-First-Line\"] = \"yes\"\n        if skip_string_normalization:\n            self.headers[\"X-Skip-String-Normalization\"] = \"yes\"\n        if skip_magic_trailing_comma:\n            self.headers[\"X-Skip-Magic-Trailing-Comma\"] = \"yes\"\n        if preview:\n            self.headers[\"X-Preview\"] = \"yes\"\n        if fast:\n            self.headers[\"X-Fast-Or-Safe\"] = \"fast\"\n        if python_variant is not None:\n            self.headers[\"X-Python-Variant\"] = python_variant\n        if diff:\n            self.headers[\"X-Diff\"] = \"yes\"\n\n        if headers is not None:\n            self.headers.update(headers)"
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 28,
      "kind": "module",
      "qualname": "src.blib2to3.pgen2.driver",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 43,
      "kind": "class",
      "qualname": "src.blib2to3.pgen2.driver.ReleaseRange",
      "span": [
        41,
        48
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class ReleaseRange:\n    start: int\n    end: int | None = None\n    tokens: list[Any] = field(default_factory=list)\n\n    def lock(self) -> None:\n        total_eaten = len(self.tokens)\n        self.end = self.start + total_eaten",
      "old_code": "class ReleaseRange:\n    start: int\n    end: Optional[int] = None\n    tokens: list[Any] = field(default_factory=list)\n\n    def lock(self) -> None:\n        total_eaten = len(self.tokens)\n        self.end = self.start + total_eaten"
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 109,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.driver.Driver.__init__",
      "span": [
        109,
        113
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(self, grammar: Grammar, logger: Logger | None = None) -> None:\n        self.grammar = grammar\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        self.logger = logger",
      "old_code": "    def __init__(self, grammar: Grammar, logger: Optional[Logger] = None) -> None:\n        self.grammar = grammar\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        self.logger = logger"
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 188,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.driver.Driver.parse_file",
      "span": [
        187,
        193
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def parse_file(\n        self, filename: Path, encoding: str | None = None, debug: bool = False\n    ) -> NL:\n        \"\"\"Parse a file and return the syntax tree.\"\"\"\n        with open(filename, encoding=encoding) as stream:\n            text = stream.read()\n        return self.parse_string(text, debug)",
      "old_code": "    def parse_file(\n        self, filename: Path, encoding: Optional[str] = None, debug: bool = False\n    ) -> NL:\n        \"\"\"Parse a file and return the syntax tree.\"\"\"\n        with open(filename, encoding=encoding) as stream:\n            text = stream.read()\n        return self.parse_string(text, debug)"
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 230,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.driver._generate_pickle_name",
      "span": [
        230,
        238
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def _generate_pickle_name(gt: Path, cache_dir: Path | None = None) -> str:\n    head, tail = os.path.splitext(gt)\n    if tail == \".txt\":\n        tail = \"\"\n    name = head + tail + \".\".join(map(str, sys.version_info)) + \".pickle\"\n    if cache_dir:\n        return os.path.join(cache_dir, os.path.basename(name))\n    else:\n        return name",
      "old_code": "def _generate_pickle_name(gt: Path, cache_dir: Optional[Path] = None) -> str:\n    head, tail = os.path.splitext(gt)\n    if tail == \".txt\":\n        tail = \"\"\n    name = head + tail + \".\".join(map(str, sys.version_info)) + \".pickle\"\n    if cache_dir:\n        return os.path.join(cache_dir, os.path.basename(name))\n    else:\n        return name"
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 243,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.driver.load_grammar",
      "span": [
        241,
        263
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def load_grammar(\n    gt: str = \"Grammar.txt\",\n    gp: str | None = None,\n    save: bool = True,\n    force: bool = False,\n    logger: Logger | None = None,\n) -> Grammar:\n    \"\"\"Load the grammar (maybe from a pickle).\"\"\"\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    gp = _generate_pickle_name(gt) if gp is None else gp\n    if force or not _newer(gp, gt):\n        g: grammar.Grammar = pgen.generate_grammar(gt)\n        if save:\n            try:\n                g.dump(gp)\n            except OSError:\n                # Ignore error, caching is not vital.\n                pass\n    else:\n        g = grammar.Grammar()\n        g.load(gp)\n    return g",
      "old_code": "def load_grammar(\n    gt: str = \"Grammar.txt\",\n    gp: Optional[str] = None,\n    save: bool = True,\n    force: bool = False,\n    logger: Optional[Logger] = None,\n) -> Grammar:\n    \"\"\"Load the grammar (maybe from a pickle).\"\"\"\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    gp = _generate_pickle_name(gt) if gp is None else gp\n    if force or not _newer(gp, gt):\n        g: grammar.Grammar = pgen.generate_grammar(gt)\n        if save:\n            try:\n                g.dump(gp)\n            except OSError:\n                # Ignore error, caching is not vital.\n                pass\n    else:\n        g = grammar.Grammar()\n        g.load(gp)\n    return g"
    },
    {
      "path": "src/blib2to3/pgen2/driver.py",
      "version": "new",
      "line": 276,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.driver.load_packaged_grammar",
      "span": [
        275,
        296
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def load_packaged_grammar(\n    package: str, grammar_source: str, cache_dir: Path | None = None\n) -> grammar.Grammar:\n    \"\"\"Normally, loads a pickled grammar by doing\n        pkgutil.get_data(package, pickled_grammar)\n    where *pickled_grammar* is computed from *grammar_source* by adding the\n    Python version and using a ``.pickle`` extension.\n\n    However, if *grammar_source* is an extant file, load_grammar(grammar_source)\n    is called instead. This facilitates using a packaged grammar file when needed\n    but preserves load_grammar's automatic regeneration behavior when possible.\n\n    \"\"\"\n    if os.path.isfile(grammar_source):\n        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None\n        return load_grammar(grammar_source, gp=gp)\n    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)\n    data = pkgutil.get_data(package, pickled_name)\n    assert data is not None\n    g = grammar.Grammar()\n    g.loads(data)\n    return g",
      "old_code": "def load_packaged_grammar(\n    package: str, grammar_source: str, cache_dir: Optional[Path] = None\n) -> grammar.Grammar:\n    \"\"\"Normally, loads a pickled grammar by doing\n        pkgutil.get_data(package, pickled_grammar)\n    where *pickled_grammar* is computed from *grammar_source* by adding the\n    Python version and using a ``.pickle`` extension.\n\n    However, if *grammar_source* is an extant file, load_grammar(grammar_source)\n    is called instead. This facilitates using a packaged grammar file when needed\n    but preserves load_grammar's automatic regeneration behavior when possible.\n\n    \"\"\"\n    if os.path.isfile(grammar_source):\n        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None\n        return load_grammar(grammar_source, gp=gp)\n    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)\n    data = pkgutil.get_data(package, pickled_name)\n    assert data is not None\n    g = grammar.Grammar()\n    g.loads(data)\n    return g"
    },
    {
      "path": "src/blib2to3/pgen2/parse.py",
      "version": "new",
      "line": 15,
      "kind": "module",
      "qualname": "src.blib2to3.pgen2.parse",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/blib2to3/pgen2/parse.py",
      "version": "new",
      "line": 101,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.parse.Recorder.determine_route",
      "span": [
        100,
        112
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def determine_route(\n        self, value: str | None = None, force: bool = False\n    ) -> int | None:\n        alive_ilabels = self.ilabels\n        if len(alive_ilabels) == 0:\n            *_, most_successful_ilabel = self._dead_ilabels\n            raise ParseError(\"bad input\", most_successful_ilabel, value, self.context)\n\n        ilabel, *rest = alive_ilabels\n        if force or not rest:\n            return ilabel\n        else:\n            return None",
      "old_code": "    def determine_route(\n        self, value: Optional[str] = None, force: bool = False\n    ) -> Optional[int]:\n        alive_ilabels = self.ilabels\n        if len(alive_ilabels) == 0:\n            *_, most_successful_ilabel = self._dead_ilabels\n            raise ParseError(\"bad input\", most_successful_ilabel, value, self.context)\n\n        ilabel, *rest = alive_ilabels\n        if force or not rest:\n            return ilabel\n        else:\n            return None"
    },
    {
      "path": "src/blib2to3/pgen2/parse.py",
      "version": "new",
      "line": 119,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.parse.ParseError.__init__",
      "span": [
        118,
        127
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self, msg: str, type: int | None, value: str | None, context: Context\n    ) -> None:\n        Exception.__init__(\n            self, f\"{msg}: type={type!r}, value={value!r}, context={context!r}\"\n        )\n        self.msg = msg\n        self.type = type\n        self.value = value\n        self.context = context",
      "old_code": "    def __init__(\n        self, msg: str, type: Optional[int], value: Optional[str], context: Context\n    ) -> None:\n        Exception.__init__(\n            self, f\"{msg}: type={type!r}, value={value!r}, context={context!r}\"\n        )\n        self.msg = msg\n        self.type = type\n        self.value = value\n        self.context = context"
    },
    {
      "path": "src/blib2to3/pgen2/parse.py",
      "version": "new",
      "line": 160,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.parse.Parser.__init__",
      "span": [
        160,
        198
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(self, grammar: Grammar, convert: Convert | None = None) -> None:\n        \"\"\"Constructor.\n\n        The grammar argument is a grammar.Grammar instance; see the\n        grammar module for more information.\n\n        The parser is not ready yet for parsing; you must call the\n        setup() method to get it started.\n\n        The optional convert argument is a function mapping concrete\n        syntax tree nodes to abstract syntax tree nodes.  If not\n        given, no conversion is done and the syntax tree produced is\n        the concrete syntax tree.  If given, it must be a function of\n        two arguments, the first being the grammar (a grammar.Grammar\n        instance), and the second being the concrete syntax tree node\n        to be converted.  The syntax tree is converted from the bottom\n        up.\n\n        **post-note: the convert argument is ignored since for Black's\n        usage, convert will always be blib2to3.pytree.convert. Allowing\n        this to be dynamic hurts mypyc's ability to use early binding.\n        These docs are left for historical and informational value.\n\n        A concrete syntax tree node is a (type, value, context, nodes)\n        tuple, where type is the node type (a token or symbol number),\n        value is None for symbols and a string for tokens, context is\n        None or an opaque value used for error reporting (typically a\n        (lineno, offset) pair), and nodes is a list of children for\n        symbols, and None for tokens.\n\n        An abstract syntax tree node may be anything; this is entirely\n        up to the converter function.\n\n        \"\"\"\n        self.grammar = grammar\n        # See note in docstring above. TL;DR this is ignored.\n        self.convert = convert or lam_sub\n        self.is_backtracking = False\n        self.last_token: int | None = None",
      "old_code": "    def __init__(self, grammar: Grammar, convert: Optional[Convert] = None) -> None:\n        \"\"\"Constructor.\n\n        The grammar argument is a grammar.Grammar instance; see the\n        grammar module for more information.\n\n        The parser is not ready yet for parsing; you must call the\n        setup() method to get it started.\n\n        The optional convert argument is a function mapping concrete\n        syntax tree nodes to abstract syntax tree nodes.  If not\n        given, no conversion is done and the syntax tree produced is\n        the concrete syntax tree.  If given, it must be a function of\n        two arguments, the first being the grammar (a grammar.Grammar\n        instance), and the second being the concrete syntax tree node\n        to be converted.  The syntax tree is converted from the bottom\n        up.\n\n        **post-note: the convert argument is ignored since for Black's\n        usage, convert will always be blib2to3.pytree.convert. Allowing\n        this to be dynamic hurts mypyc's ability to use early binding.\n        These docs are left for historical and informational value.\n\n        A concrete syntax tree node is a (type, value, context, nodes)\n        tuple, where type is the node type (a token or symbol number),\n        value is None for symbols and a string for tokens, context is\n        None or an opaque value used for error reporting (typically a\n        (lineno, offset) pair), and nodes is a list of children for\n        symbols, and None for tokens.\n\n        An abstract syntax tree node may be anything; this is entirely\n        up to the converter function.\n\n        \"\"\"\n        self.grammar = grammar\n        # See note in docstring above. TL;DR this is ignored.\n        self.convert = convert or lam_sub\n        self.is_backtracking = False\n        self.last_token: Optional[int] = None"
    },
    {
      "path": "src/blib2to3/pgen2/parse.py",
      "version": "new",
      "line": 200,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.parse.Parser.setup",
      "span": [
        200,
        224
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def setup(self, proxy: \"TokenProxy\", start: int | None = None) -> None:\n        \"\"\"Prepare for parsing.\n\n        This *must* be called before starting to parse.\n\n        The optional argument is an alternative start symbol; it\n        defaults to the grammar's start symbol.\n\n        You can use a Parser instance to parse any number of programs;\n        each time you call setup() the parser is reset to an initial\n        state determined by the (implicit or explicit) start symbol.\n\n        \"\"\"\n        if start is None:\n            start = self.grammar.start\n        # Each stack entry is a tuple: (dfa, state, node).\n        # A node is a tuple: (type, value, context, children),\n        # where children is a list of nodes or None, and context may be None.\n        newnode: RawNode = (start, None, None, [])\n        stackentry = (self.grammar.dfas[start], 0, newnode)\n        self.stack: list[tuple[DFAS, int, RawNode]] = [stackentry]\n        self.rootnode: NL | None = None\n        self.used_names: set[str] = set()\n        self.proxy = proxy\n        self.last_token = None",
      "old_code": "    def setup(self, proxy: \"TokenProxy\", start: Optional[int] = None) -> None:\n        \"\"\"Prepare for parsing.\n\n        This *must* be called before starting to parse.\n\n        The optional argument is an alternative start symbol; it\n        defaults to the grammar's start symbol.\n\n        You can use a Parser instance to parse any number of programs;\n        each time you call setup() the parser is reset to an initial\n        state determined by the (implicit or explicit) start symbol.\n\n        \"\"\"\n        if start is None:\n            start = self.grammar.start\n        # Each stack entry is a tuple: (dfa, state, node).\n        # A node is a tuple: (type, value, context, children),\n        # where children is a list of nodes or None, and context may be None.\n        newnode: RawNode = (start, None, None, [])\n        stackentry = (self.grammar.dfas[start], 0, newnode)\n        self.stack: list[tuple[DFAS, int, RawNode]] = [stackentry]\n        self.rootnode: Optional[NL] = None\n        self.used_names: set[str] = set()\n        self.proxy = proxy\n        self.last_token = None"
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 6,
      "kind": "module",
      "qualname": "src.blib2to3.pgen2.pgen",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 22,
      "kind": "class",
      "qualname": "src.blib2to3.pgen2.pgen.ParserGenerator",
      "span": [
        18,
        353
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class ParserGenerator:\n    filename: Path\n    stream: IO[str]\n    generator: Iterator[TokenInfo]\n    first: dict[str, dict[str, int] | None]\n\n    def __init__(self, filename: Path, stream: IO[str] | None = None) -> None:\n        close_stream = None\n        if stream is None:\n            stream = open(filename, encoding=\"utf-8\")\n            close_stream = stream.close\n        self.filename = filename\n        self.generator = tokenize.tokenize(stream.read())\n        self.gettoken()  # Initialize lookahead\n        self.dfas, self.startsymbol = self.parse()\n        if close_stream is not None:\n            close_stream()\n        self.first = {}  # map from symbol name to set of tokens\n        self.addfirstsets()\n\n    def make_grammar(self) -> PgenGrammar:\n        c = PgenGrammar()\n        names = list(self.dfas.keys())\n        names.sort()\n        names.remove(self.startsymbol)\n        names.insert(0, self.startsymbol)\n        for name in names:\n            i = 256 + len(c.symbol2number)\n            c.symbol2number[name] = i\n            c.number2symbol[i] = name\n        for name in names:\n            dfa = self.dfas[name]\n            states = []\n            for state in dfa:\n                arcs = []\n                for label, next in sorted(state.arcs.items()):\n                    arcs.append((self.make_label(c, label), dfa.index(next)))\n                if state.isfinal:\n                    arcs.append((0, dfa.index(state)))\n                states.append(arcs)\n            c.states.append(states)\n            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))\n        c.start = c.symbol2number[self.startsymbol]\n        return c\n\n    def make_first(self, c: PgenGrammar, name: str) -> dict[int, int]:\n        rawfirst = self.first[name]\n        assert rawfirst is not None\n        first = {}\n        for label in sorted(rawfirst):\n            ilabel = self.make_label(c, label)\n            ##assert ilabel not in first # XXX failed on <> ... !=\n            first[ilabel] = 1\n        return first\n\n    def make_label(self, c: PgenGrammar, label: str) -> int:\n        # XXX Maybe this should be a method on a subclass of converter?\n        ilabel = len(c.labels)\n        if label[0].isalpha():\n            # Either a symbol name or a named token\n            if label in c.symbol2number:\n                # A symbol name (a non-terminal)\n                if label in c.symbol2label:\n                    return c.symbol2label[label]\n                else:\n                    c.labels.append((c.symbol2number[label], None))\n                    c.symbol2label[label] = ilabel\n                    return ilabel\n            else:\n                # A named token (NAME, NUMBER, STRING)\n                itoken = getattr(token, label, None)\n                assert isinstance(itoken, int), label\n                assert itoken in token.tok_name, label\n                if itoken in c.tokens:\n                    return c.tokens[itoken]\n                else:\n                    c.labels.append((itoken, None))\n                    c.tokens[itoken] = ilabel\n                    return ilabel\n        else:\n            # Either a keyword or an operator\n            assert label[0] in ('\"', \"'\"), label\n            value = eval(label)\n            if value[0].isalpha():\n                if label[0] == '\"':\n                    keywords = c.soft_keywords\n                else:\n                    keywords = c.keywords\n\n                # A keyword\n                if value in keywords:\n                    return keywords[value]\n                else:\n                    c.labels.append((token.NAME, value))\n                    keywords[value] = ilabel\n                    return ilabel\n            else:\n                # An operator (any non-numeric token)\n                itoken = grammar.opmap[value]  # Fails if unknown token\n                if itoken in c.tokens:\n                    return c.tokens[itoken]\n                else:\n                    c.labels.append((itoken, None))\n                    c.tokens[itoken] = ilabel\n                    return ilabel\n\n    def addfirstsets(self) -> None:\n        names = list(self.dfas.keys())\n        names.sort()\n        for name in names:\n            if name not in self.first:\n                self.calcfirst(name)\n            # print name, self.first[name].keys()\n\n    def calcfirst(self, name: str) -> None:\n        dfa = self.dfas[name]\n        self.first[name] = None  # dummy to detect left recursion\n        state = dfa[0]\n        totalset: dict[str, int] = {}\n        overlapcheck = {}\n        for label in state.arcs:\n            if label in self.dfas:\n                if label in self.first:\n                    fset = self.first[label]\n                    if fset is None:\n                        raise ValueError(f\"recursion for rule {name!r}\")\n                else:\n                    self.calcfirst(label)\n                    fset = self.first[label]\n                    assert fset is not None\n                totalset.update(fset)\n                overlapcheck[label] = fset\n            else:\n                totalset[label] = 1\n                overlapcheck[label] = {label: 1}\n        inverse: dict[str, str] = {}\n        for label, itsfirst in overlapcheck.items():\n            for symbol in itsfirst:\n                if symbol in inverse:\n                    raise ValueError(\n                        f\"rule {name} is ambiguous; {symbol} is in the first sets of\"\n                        f\" {label} as well as {inverse[symbol]}\"\n                    )\n                inverse[symbol] = label\n        self.first[name] = totalset\n\n    def parse(self) -> tuple[dict[str, list[\"DFAState\"]], str]:\n        dfas = {}\n        startsymbol: str | None = None\n        # MSTART: (NEWLINE | RULE)* ENDMARKER\n        while self.type != token.ENDMARKER:\n            while self.type == token.NEWLINE:\n                self.gettoken()\n            # RULE: NAME ':' RHS NEWLINE\n            name = self.expect(token.NAME)\n            self.expect(token.OP, \":\")\n            a, z = self.parse_rhs()\n            self.expect(token.NEWLINE)\n            # self.dump_nfa(name, a, z)\n            dfa = self.make_dfa(a, z)\n            # self.dump_dfa(name, dfa)\n            # oldlen = len(dfa)\n            self.simplify_dfa(dfa)\n            # newlen = len(dfa)\n            dfas[name] = dfa\n            # print name, oldlen, newlen\n            if startsymbol is None:\n                startsymbol = name\n        assert startsymbol is not None\n        return dfas, startsymbol\n\n    def make_dfa(self, start: \"NFAState\", finish: \"NFAState\") -> list[\"DFAState\"]:\n        # To turn an NFA into a DFA, we define the states of the DFA\n        # to correspond to *sets* of states of the NFA.  Then do some\n        # state reduction.  Let's represent sets as dicts with 1 for\n        # values.\n        assert isinstance(start, NFAState)\n        assert isinstance(finish, NFAState)\n\n        def closure(state: NFAState) -> dict[NFAState, int]:\n            base: dict[NFAState, int] = {}\n            addclosure(state, base)\n            return base\n\n        def addclosure(state: NFAState, base: dict[NFAState, int]) -> None:\n            assert isinstance(state, NFAState)\n            if state in base:\n                return\n            base[state] = 1\n            for label, next in state.arcs:\n                if label is None:\n                    addclosure(next, base)\n\n        states = [DFAState(closure(start), finish)]\n        for state in states:  # NB states grows while we're iterating\n            arcs: dict[str, dict[NFAState, int]] = {}\n            for nfastate in state.nfaset:\n                for label, next in nfastate.arcs:\n                    if label is not None:\n                        addclosure(next, arcs.setdefault(label, {}))\n            for label, nfaset in sorted(arcs.items()):\n                for st in states:\n                    if st.nfaset == nfaset:\n                        break\n                else:\n                    st = DFAState(nfaset, finish)\n                    states.append(st)\n                state.addarc(st, label)\n        return states  # List of DFAState instances; first one is start\n\n    def dump_nfa(self, name: str, start: \"NFAState\", finish: \"NFAState\") -> None:\n        print(\"Dump of NFA for\", name)\n        todo = [start]\n        for i, state in enumerate(todo):\n            print(\"  State\", i, state is finish and \"(final)\" or \"\")\n            for label, next in state.arcs:\n                if next in todo:\n                    j = todo.index(next)\n                else:\n                    j = len(todo)\n                    todo.append(next)\n                if label is None:\n                    print(f\"    -> {j}\")\n                else:\n                    print(f\"    {label} -> {j}\")\n\n    def dump_dfa(self, name: str, dfa: Sequence[\"DFAState\"]) -> None:\n        print(\"Dump of DFA for\", name)\n        for i, state in enumerate(dfa):\n            print(\"  State\", i, state.isfinal and \"(final)\" or \"\")\n            for label, next in sorted(state.arcs.items()):\n                print(f\"    {label} -> {dfa.index(next)}\")\n\n    def simplify_dfa(self, dfa: list[\"DFAState\"]) -> None:\n        # This is not theoretically optimal, but works well enough.\n        # Algorithm: repeatedly look for two states that have the same\n        # set of arcs (same labels pointing to the same nodes) and\n        # unify them, until things stop changing.\n\n        # dfa is a list of DFAState instances\n        changes = True\n        while changes:\n            changes = False\n            for i, state_i in enumerate(dfa):\n                for j in range(i + 1, len(dfa)):\n                    state_j = dfa[j]\n                    if state_i == state_j:\n                        # print \"  unify\", i, j\n                        del dfa[j]\n                        for state in dfa:\n                            state.unifystate(state_j, state_i)\n                        changes = True\n                        break\n\n    def parse_rhs(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # RHS: ALT ('|' ALT)*\n        a, z = self.parse_alt()\n        if self.value != \"|\":\n            return a, z\n        else:\n            aa = NFAState()\n            zz = NFAState()\n            aa.addarc(a)\n            z.addarc(zz)\n            while self.value == \"|\":\n                self.gettoken()\n                a, z = self.parse_alt()\n                aa.addarc(a)\n                z.addarc(zz)\n            return aa, zz\n\n    def parse_alt(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ALT: ITEM+\n        a, b = self.parse_item()\n        while self.value in (\"(\", \"[\") or self.type in (token.NAME, token.STRING):\n            c, d = self.parse_item()\n            b.addarc(c)\n            b = d\n        return a, b\n\n    def parse_item(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ITEM: '[' RHS ']' | ATOM ['+' | '*']\n        if self.value == \"[\":\n            self.gettoken()\n            a, z = self.parse_rhs()\n            self.expect(token.OP, \"]\")\n            a.addarc(z)\n            return a, z\n        else:\n            a, z = self.parse_atom()\n            value = self.value\n            if value not in (\"+\", \"*\"):\n                return a, z\n            self.gettoken()\n            z.addarc(a)\n            if value == \"+\":\n                return a, z\n            else:\n                return a, a\n\n    def parse_atom(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ATOM: '(' RHS ')' | NAME | STRING\n        if self.value == \"(\":\n            self.gettoken()\n            a, z = self.parse_rhs()\n            self.expect(token.OP, \")\")\n            return a, z\n        elif self.type in (token.NAME, token.STRING):\n            a = NFAState()\n            z = NFAState()\n            a.addarc(z, self.value)\n            self.gettoken()\n            return a, z\n        else:\n            self.raise_error(\n                f\"expected (...) or NAME or STRING, got {self.type}/{self.value}\"\n            )\n\n    def expect(self, type: int, value: Any | None = None) -> str:\n        if self.type != type or (value is not None and self.value != value):\n            self.raise_error(f\"expected {type}/{value}, got {self.type}/{self.value}\")\n        value = self.value\n        self.gettoken()\n        return value\n\n    def gettoken(self) -> None:\n        tup = next(self.generator)\n        while tup[0] in (tokenize.COMMENT, tokenize.NL):\n            tup = next(self.generator)\n        self.type, self.value, self.begin, self.end, self.line = tup\n        # print token.tok_name[self.type], repr(self.value)\n\n    def raise_error(self, msg: str) -> NoReturn:\n        raise SyntaxError(\n            msg, (str(self.filename), self.end[0], self.end[1], self.line)\n        )",
      "old_code": "class ParserGenerator:\n    filename: Path\n    stream: IO[str]\n    generator: Iterator[TokenInfo]\n    first: dict[str, Optional[dict[str, int]]]\n\n    def __init__(self, filename: Path, stream: Optional[IO[str]] = None) -> None:\n        close_stream = None\n        if stream is None:\n            stream = open(filename, encoding=\"utf-8\")\n            close_stream = stream.close\n        self.filename = filename\n        self.generator = tokenize.tokenize(stream.read())\n        self.gettoken()  # Initialize lookahead\n        self.dfas, self.startsymbol = self.parse()\n        if close_stream is not None:\n            close_stream()\n        self.first = {}  # map from symbol name to set of tokens\n        self.addfirstsets()\n\n    def make_grammar(self) -> PgenGrammar:\n        c = PgenGrammar()\n        names = list(self.dfas.keys())\n        names.sort()\n        names.remove(self.startsymbol)\n        names.insert(0, self.startsymbol)\n        for name in names:\n            i = 256 + len(c.symbol2number)\n            c.symbol2number[name] = i\n            c.number2symbol[i] = name\n        for name in names:\n            dfa = self.dfas[name]\n            states = []\n            for state in dfa:\n                arcs = []\n                for label, next in sorted(state.arcs.items()):\n                    arcs.append((self.make_label(c, label), dfa.index(next)))\n                if state.isfinal:\n                    arcs.append((0, dfa.index(state)))\n                states.append(arcs)\n            c.states.append(states)\n            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))\n        c.start = c.symbol2number[self.startsymbol]\n        return c\n\n    def make_first(self, c: PgenGrammar, name: str) -> dict[int, int]:\n        rawfirst = self.first[name]\n        assert rawfirst is not None\n        first = {}\n        for label in sorted(rawfirst):\n            ilabel = self.make_label(c, label)\n            ##assert ilabel not in first # XXX failed on <> ... !=\n            first[ilabel] = 1\n        return first\n\n    def make_label(self, c: PgenGrammar, label: str) -> int:\n        # XXX Maybe this should be a method on a subclass of converter?\n        ilabel = len(c.labels)\n        if label[0].isalpha():\n            # Either a symbol name or a named token\n            if label in c.symbol2number:\n                # A symbol name (a non-terminal)\n                if label in c.symbol2label:\n                    return c.symbol2label[label]\n                else:\n                    c.labels.append((c.symbol2number[label], None))\n                    c.symbol2label[label] = ilabel\n                    return ilabel\n            else:\n                # A named token (NAME, NUMBER, STRING)\n                itoken = getattr(token, label, None)\n                assert isinstance(itoken, int), label\n                assert itoken in token.tok_name, label\n                if itoken in c.tokens:\n                    return c.tokens[itoken]\n                else:\n                    c.labels.append((itoken, None))\n                    c.tokens[itoken] = ilabel\n                    return ilabel\n        else:\n            # Either a keyword or an operator\n            assert label[0] in ('\"', \"'\"), label\n            value = eval(label)\n            if value[0].isalpha():\n                if label[0] == '\"':\n                    keywords = c.soft_keywords\n                else:\n                    keywords = c.keywords\n\n                # A keyword\n                if value in keywords:\n                    return keywords[value]\n                else:\n                    c.labels.append((token.NAME, value))\n                    keywords[value] = ilabel\n                    return ilabel\n            else:\n                # An operator (any non-numeric token)\n                itoken = grammar.opmap[value]  # Fails if unknown token\n                if itoken in c.tokens:\n                    return c.tokens[itoken]\n                else:\n                    c.labels.append((itoken, None))\n                    c.tokens[itoken] = ilabel\n                    return ilabel\n\n    def addfirstsets(self) -> None:\n        names = list(self.dfas.keys())\n        names.sort()\n        for name in names:\n            if name not in self.first:\n                self.calcfirst(name)\n            # print name, self.first[name].keys()\n\n    def calcfirst(self, name: str) -> None:\n        dfa = self.dfas[name]\n        self.first[name] = None  # dummy to detect left recursion\n        state = dfa[0]\n        totalset: dict[str, int] = {}\n        overlapcheck = {}\n        for label in state.arcs:\n            if label in self.dfas:\n                if label in self.first:\n                    fset = self.first[label]\n                    if fset is None:\n                        raise ValueError(f\"recursion for rule {name!r}\")\n                else:\n                    self.calcfirst(label)\n                    fset = self.first[label]\n                    assert fset is not None\n                totalset.update(fset)\n                overlapcheck[label] = fset\n            else:\n                totalset[label] = 1\n                overlapcheck[label] = {label: 1}\n        inverse: dict[str, str] = {}\n        for label, itsfirst in overlapcheck.items():\n            for symbol in itsfirst:\n                if symbol in inverse:\n                    raise ValueError(\n                        f\"rule {name} is ambiguous; {symbol} is in the first sets of\"\n                        f\" {label} as well as {inverse[symbol]}\"\n                    )\n                inverse[symbol] = label\n        self.first[name] = totalset\n\n    def parse(self) -> tuple[dict[str, list[\"DFAState\"]], str]:\n        dfas = {}\n        startsymbol: Optional[str] = None\n        # MSTART: (NEWLINE | RULE)* ENDMARKER\n        while self.type != token.ENDMARKER:\n            while self.type == token.NEWLINE:\n                self.gettoken()\n            # RULE: NAME ':' RHS NEWLINE\n            name = self.expect(token.NAME)\n            self.expect(token.OP, \":\")\n            a, z = self.parse_rhs()\n            self.expect(token.NEWLINE)\n            # self.dump_nfa(name, a, z)\n            dfa = self.make_dfa(a, z)\n            # self.dump_dfa(name, dfa)\n            # oldlen = len(dfa)\n            self.simplify_dfa(dfa)\n            # newlen = len(dfa)\n            dfas[name] = dfa\n            # print name, oldlen, newlen\n            if startsymbol is None:\n                startsymbol = name\n        assert startsymbol is not None\n        return dfas, startsymbol\n\n    def make_dfa(self, start: \"NFAState\", finish: \"NFAState\") -> list[\"DFAState\"]:\n        # To turn an NFA into a DFA, we define the states of the DFA\n        # to correspond to *sets* of states of the NFA.  Then do some\n        # state reduction.  Let's represent sets as dicts with 1 for\n        # values.\n        assert isinstance(start, NFAState)\n        assert isinstance(finish, NFAState)\n\n        def closure(state: NFAState) -> dict[NFAState, int]:\n            base: dict[NFAState, int] = {}\n            addclosure(state, base)\n            return base\n\n        def addclosure(state: NFAState, base: dict[NFAState, int]) -> None:\n            assert isinstance(state, NFAState)\n            if state in base:\n                return\n            base[state] = 1\n            for label, next in state.arcs:\n                if label is None:\n                    addclosure(next, base)\n\n        states = [DFAState(closure(start), finish)]\n        for state in states:  # NB states grows while we're iterating\n            arcs: dict[str, dict[NFAState, int]] = {}\n            for nfastate in state.nfaset:\n                for label, next in nfastate.arcs:\n                    if label is not None:\n                        addclosure(next, arcs.setdefault(label, {}))\n            for label, nfaset in sorted(arcs.items()):\n                for st in states:\n                    if st.nfaset == nfaset:\n                        break\n                else:\n                    st = DFAState(nfaset, finish)\n                    states.append(st)\n                state.addarc(st, label)\n        return states  # List of DFAState instances; first one is start\n\n    def dump_nfa(self, name: str, start: \"NFAState\", finish: \"NFAState\") -> None:\n        print(\"Dump of NFA for\", name)\n        todo = [start]\n        for i, state in enumerate(todo):\n            print(\"  State\", i, state is finish and \"(final)\" or \"\")\n            for label, next in state.arcs:\n                if next in todo:\n                    j = todo.index(next)\n                else:\n                    j = len(todo)\n                    todo.append(next)\n                if label is None:\n                    print(f\"    -> {j}\")\n                else:\n                    print(f\"    {label} -> {j}\")\n\n    def dump_dfa(self, name: str, dfa: Sequence[\"DFAState\"]) -> None:\n        print(\"Dump of DFA for\", name)\n        for i, state in enumerate(dfa):\n            print(\"  State\", i, state.isfinal and \"(final)\" or \"\")\n            for label, next in sorted(state.arcs.items()):\n                print(f\"    {label} -> {dfa.index(next)}\")\n\n    def simplify_dfa(self, dfa: list[\"DFAState\"]) -> None:\n        # This is not theoretically optimal, but works well enough.\n        # Algorithm: repeatedly look for two states that have the same\n        # set of arcs (same labels pointing to the same nodes) and\n        # unify them, until things stop changing.\n\n        # dfa is a list of DFAState instances\n        changes = True\n        while changes:\n            changes = False\n            for i, state_i in enumerate(dfa):\n                for j in range(i + 1, len(dfa)):\n                    state_j = dfa[j]\n                    if state_i == state_j:\n                        # print \"  unify\", i, j\n                        del dfa[j]\n                        for state in dfa:\n                            state.unifystate(state_j, state_i)\n                        changes = True\n                        break\n\n    def parse_rhs(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # RHS: ALT ('|' ALT)*\n        a, z = self.parse_alt()\n        if self.value != \"|\":\n            return a, z\n        else:\n            aa = NFAState()\n            zz = NFAState()\n            aa.addarc(a)\n            z.addarc(zz)\n            while self.value == \"|\":\n                self.gettoken()\n                a, z = self.parse_alt()\n                aa.addarc(a)\n                z.addarc(zz)\n            return aa, zz\n\n    def parse_alt(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ALT: ITEM+\n        a, b = self.parse_item()\n        while self.value in (\"(\", \"[\") or self.type in (token.NAME, token.STRING):\n            c, d = self.parse_item()\n            b.addarc(c)\n            b = d\n        return a, b\n\n    def parse_item(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ITEM: '[' RHS ']' | ATOM ['+' | '*']\n        if self.value == \"[\":\n            self.gettoken()\n            a, z = self.parse_rhs()\n            self.expect(token.OP, \"]\")\n            a.addarc(z)\n            return a, z\n        else:\n            a, z = self.parse_atom()\n            value = self.value\n            if value not in (\"+\", \"*\"):\n                return a, z\n            self.gettoken()\n            z.addarc(a)\n            if value == \"+\":\n                return a, z\n            else:\n                return a, a\n\n    def parse_atom(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ATOM: '(' RHS ')' | NAME | STRING\n        if self.value == \"(\":\n            self.gettoken()\n            a, z = self.parse_rhs()\n            self.expect(token.OP, \")\")\n            return a, z\n        elif self.type in (token.NAME, token.STRING):\n            a = NFAState()\n            z = NFAState()\n            a.addarc(z, self.value)\n            self.gettoken()\n            return a, z\n        else:\n            self.raise_error(\n                f\"expected (...) or NAME or STRING, got {self.type}/{self.value}\"\n            )\n\n    def expect(self, type: int, value: Optional[Any] = None) -> str:\n        if self.type != type or (value is not None and self.value != value):\n            self.raise_error(f\"expected {type}/{value}, got {self.type}/{self.value}\")\n        value = self.value\n        self.gettoken()\n        return value\n\n    def gettoken(self) -> None:\n        tup = next(self.generator)\n        while tup[0] in (tokenize.COMMENT, tokenize.NL):\n            tup = next(self.generator)\n        self.type, self.value, self.begin, self.end, self.line = tup\n        # print token.tok_name[self.type], repr(self.value)\n\n    def raise_error(self, msg: str) -> NoReturn:\n        raise SyntaxError(\n            msg, (str(self.filename), self.end[0], self.end[1], self.line)\n        )"
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 24,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.pgen.ParserGenerator.__init__",
      "span": [
        24,
        36
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(self, filename: Path, stream: IO[str] | None = None) -> None:\n        close_stream = None\n        if stream is None:\n            stream = open(filename, encoding=\"utf-8\")\n            close_stream = stream.close\n        self.filename = filename\n        self.generator = tokenize.tokenize(stream.read())\n        self.gettoken()  # Initialize lookahead\n        self.dfas, self.startsymbol = self.parse()\n        if close_stream is not None:\n            close_stream()\n        self.first = {}  # map from symbol name to set of tokens\n        self.addfirstsets()",
      "old_code": "    def __init__(self, filename: Path, stream: Optional[IO[str]] = None) -> None:\n        close_stream = None\n        if stream is None:\n            stream = open(filename, encoding=\"utf-8\")\n            close_stream = stream.close\n        self.filename = filename\n        self.generator = tokenize.tokenize(stream.read())\n        self.gettoken()  # Initialize lookahead\n        self.dfas, self.startsymbol = self.parse()\n        if close_stream is not None:\n            close_stream()\n        self.first = {}  # map from symbol name to set of tokens\n        self.addfirstsets()"
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 166,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.pgen.ParserGenerator.parse",
      "span": [
        164,
        187
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def parse(self) -> tuple[dict[str, list[\"DFAState\"]], str]:\n        dfas = {}\n        startsymbol: str | None = None\n        # MSTART: (NEWLINE | RULE)* ENDMARKER\n        while self.type != token.ENDMARKER:\n            while self.type == token.NEWLINE:\n                self.gettoken()\n            # RULE: NAME ':' RHS NEWLINE\n            name = self.expect(token.NAME)\n            self.expect(token.OP, \":\")\n            a, z = self.parse_rhs()\n            self.expect(token.NEWLINE)\n            # self.dump_nfa(name, a, z)\n            dfa = self.make_dfa(a, z)\n            # self.dump_dfa(name, dfa)\n            # oldlen = len(dfa)\n            self.simplify_dfa(dfa)\n            # newlen = len(dfa)\n            dfas[name] = dfa\n            # print name, oldlen, newlen\n            if startsymbol is None:\n                startsymbol = name\n        assert startsymbol is not None\n        return dfas, startsymbol",
      "old_code": "    def parse(self) -> tuple[dict[str, list[\"DFAState\"]], str]:\n        dfas = {}\n        startsymbol: Optional[str] = None\n        # MSTART: (NEWLINE | RULE)* ENDMARKER\n        while self.type != token.ENDMARKER:\n            while self.type == token.NEWLINE:\n                self.gettoken()\n            # RULE: NAME ':' RHS NEWLINE\n            name = self.expect(token.NAME)\n            self.expect(token.OP, \":\")\n            a, z = self.parse_rhs()\n            self.expect(token.NEWLINE)\n            # self.dump_nfa(name, a, z)\n            dfa = self.make_dfa(a, z)\n            # self.dump_dfa(name, dfa)\n            # oldlen = len(dfa)\n            self.simplify_dfa(dfa)\n            # newlen = len(dfa)\n            dfas[name] = dfa\n            # print name, oldlen, newlen\n            if startsymbol is None:\n                startsymbol = name\n        assert startsymbol is not None\n        return dfas, startsymbol"
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 336,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.pgen.ParserGenerator.expect",
      "span": [
        336,
        341
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def expect(self, type: int, value: Any | None = None) -> str:\n        if self.type != type or (value is not None and self.value != value):\n            self.raise_error(f\"expected {type}/{value}, got {self.type}/{self.value}\")\n        value = self.value\n        self.gettoken()\n        return value",
      "old_code": "    def expect(self, type: int, value: Optional[Any] = None) -> str:\n        if self.type != type or (value is not None and self.value != value):\n            self.raise_error(f\"expected {type}/{value}, got {self.type}/{self.value}\")\n        value = self.value\n        self.gettoken()\n        return value"
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 357,
      "kind": "class",
      "qualname": "src.blib2to3.pgen2.pgen.NFAState",
      "span": [
        356,
        365
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class NFAState:\n    arcs: list[tuple[str | None, \"NFAState\"]]\n\n    def __init__(self) -> None:\n        self.arcs = []  # list of (label, NFAState) pairs\n\n    def addarc(self, next: \"NFAState\", label: str | None = None) -> None:\n        assert label is None or isinstance(label, str)\n        assert isinstance(next, NFAState)\n        self.arcs.append((label, next))",
      "old_code": "class NFAState:\n    arcs: list[tuple[Optional[str], \"NFAState\"]]\n\n    def __init__(self) -> None:\n        self.arcs = []  # list of (label, NFAState) pairs\n\n    def addarc(self, next: \"NFAState\", label: Optional[str] = None) -> None:\n        assert label is None or isinstance(label, str)\n        assert isinstance(next, NFAState)\n        self.arcs.append((label, next))"
    },
    {
      "path": "src/blib2to3/pgen2/pgen.py",
      "version": "new",
      "line": 362,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.pgen.NFAState.addarc",
      "span": [
        362,
        365
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def addarc(self, next: \"NFAState\", label: str | None = None) -> None:\n        assert label is None or isinstance(label, str)\n        assert isinstance(next, NFAState)\n        self.arcs.append((label, next))",
      "old_code": "    def addarc(self, next: \"NFAState\", label: Optional[str] = None) -> None:\n        assert label is None or isinstance(label, str)\n        assert isinstance(next, NFAState)\n        self.arcs.append((label, next))"
    },
    {
      "path": "src/blib2to3/pgen2/tokenize.py",
      "version": "new",
      "line": 106,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.tokenize.transform_whitespace",
      "span": [
        105,
        141
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def transform_whitespace(\n    token: pytokens.Token, source: str, prev_token: pytokens.Token | None\n) -> pytokens.Token:\n    r\"\"\"\n    Black treats `\\\\\\n` at the end of a line as a 'NL' token, while it\n    is ignored as whitespace in the regular Python parser.\n    But, only the first one. If there's a `\\\\\\n` following it\n    (as in, a \\ just by itself on a line), that is not made into NL.\n    \"\"\"\n    if (\n        token.type == TokenType.whitespace\n        and prev_token is not None\n        and prev_token.type not in (TokenType.nl, TokenType.newline)\n    ):\n        token_str = source[token.start_index : token.end_index]\n        if token_str.startswith(\"\\\\\\r\\n\"):\n            return pytokens.Token(\n                TokenType.nl,\n                token.start_index,\n                token.start_index + 3,\n                token.start_line,\n                token.start_col,\n                token.start_line,\n                token.start_col + 3,\n            )\n        elif token_str.startswith(\"\\\\\\n\") or token_str.startswith(\"\\\\\\r\"):\n            return pytokens.Token(\n                TokenType.nl,\n                token.start_index,\n                token.start_index + 2,\n                token.start_line,\n                token.start_col,\n                token.start_line,\n                token.start_col + 2,\n            )\n\n    return token",
      "old_code": "def transform_whitespace(\n    token: pytokens.Token, source: str, prev_token: Optional[pytokens.Token]\n) -> pytokens.Token:\n    r\"\"\"\n    Black treats `\\\\\\n` at the end of a line as a 'NL' token, while it\n    is ignored as whitespace in the regular Python parser.\n    But, only the first one. If there's a `\\\\\\n` following it\n    (as in, a \\ just by itself on a line), that is not made into NL.\n    \"\"\"\n    if (\n        token.type == TokenType.whitespace\n        and prev_token is not None\n        and prev_token.type not in (TokenType.nl, TokenType.newline)\n    ):\n        token_str = source[token.start_index : token.end_index]\n        if token_str.startswith(\"\\\\\\r\\n\"):\n            return pytokens.Token(\n                TokenType.nl,\n                token.start_index,\n                token.start_index + 3,\n                token.start_line,\n                token.start_col,\n                token.start_line,\n                token.start_col + 3,\n            )\n        elif token_str.startswith(\"\\\\\\n\") or token_str.startswith(\"\\\\\\r\"):\n            return pytokens.Token(\n                TokenType.nl,\n                token.start_index,\n                token.start_index + 2,\n                token.start_line,\n                token.start_col,\n                token.start_line,\n                token.start_col + 2,\n            )\n\n    return token"
    },
    {
      "path": "src/blib2to3/pgen2/tokenize.py",
      "version": "new",
      "line": 144,
      "kind": "function",
      "qualname": "src.blib2to3.pgen2.tokenize.tokenize",
      "span": [
        144,
        208
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def tokenize(source: str, grammar: Grammar | None = None) -> Iterator[TokenInfo]:\n    lines = source.split(\"\\n\")\n    lines += [\"\"]  # For newline tokens in files that don't end in a newline\n    line, column = 1, 0\n\n    prev_token: pytokens.Token | None = None\n    try:\n        for token in pytokens.tokenize(source):\n            token = transform_whitespace(token, source, prev_token)\n\n            line, column = token.start_line, token.start_col\n            if token.type == TokenType.whitespace:\n                continue\n\n            token_str = source[token.start_index : token.end_index]\n\n            if token.type == TokenType.newline and token_str == \"\":\n                # Black doesn't yield empty newline tokens at the end of a file\n                # if there's no newline at the end of a file.\n                prev_token = token\n                continue\n\n            source_line = lines[token.start_line - 1]\n\n            if token.type == TokenType.identifier and token_str in (\"async\", \"await\"):\n                # Black uses `async` and `await` token types just for those two keywords\n                yield (\n                    ASYNC if token_str == \"async\" else AWAIT,\n                    token_str,\n                    (token.start_line, token.start_col),\n                    (token.end_line, token.end_col),\n                    source_line,\n                )\n            elif token.type == TokenType.op and token_str == \"...\":\n                # Black doesn't have an ellipsis token yet, yield 3 DOTs instead\n                assert token.start_line == token.end_line\n                assert token.end_col == token.start_col + 3\n\n                token_str = \".\"\n                for start_col in range(token.start_col, token.start_col + 3):\n                    end_col = start_col + 1\n                    yield (\n                        TOKEN_TYPE_MAP[token.type],\n                        token_str,\n                        (token.start_line, start_col),\n                        (token.end_line, end_col),\n                        source_line,\n                    )\n            else:\n                token_type = TOKEN_TYPE_MAP.get(token.type)\n                if token_type is None:\n                    raise ValueError(f\"Unknown token type: {token.type!r}\")\n                yield (\n                    TOKEN_TYPE_MAP[token.type],\n                    token_str,\n                    (token.start_line, token.start_col),\n                    (token.end_line, token.end_col),\n                    source_line,\n                )\n            prev_token = token\n\n    except pytokens.UnexpectedEOF:\n        raise TokenError(\"Unexpected EOF in multi-line statement\", (line, column))\n    except pytokens.TokenizeError as exc:\n        raise TokenError(f\"Failed to parse: {type(exc).__name__}\", (line, column))",
      "old_code": "def tokenize(source: str, grammar: Optional[Grammar] = None) -> Iterator[TokenInfo]:\n    lines = source.split(\"\\n\")\n    lines += [\"\"]  # For newline tokens in files that don't end in a newline\n    line, column = 1, 0\n\n    prev_token: Optional[pytokens.Token] = None\n    try:\n        for token in pytokens.tokenize(source):\n            token = transform_whitespace(token, source, prev_token)\n\n            line, column = token.start_line, token.start_col\n            if token.type == TokenType.whitespace:\n                continue\n\n            token_str = source[token.start_index : token.end_index]\n\n            if token.type == TokenType.newline and token_str == \"\":\n                # Black doesn't yield empty newline tokens at the end of a file\n                # if there's no newline at the end of a file.\n                prev_token = token\n                continue\n\n            source_line = lines[token.start_line - 1]\n\n            if token.type == TokenType.identifier and token_str in (\"async\", \"await\"):\n                # Black uses `async` and `await` token types just for those two keywords\n                yield (\n                    ASYNC if token_str == \"async\" else AWAIT,\n                    token_str,\n                    (token.start_line, token.start_col),\n                    (token.end_line, token.end_col),\n                    source_line,\n                )\n            elif token.type == TokenType.op and token_str == \"...\":\n                # Black doesn't have an ellipsis token yet, yield 3 DOTs instead\n                assert token.start_line == token.end_line\n                assert token.end_col == token.start_col + 3\n\n                token_str = \".\"\n                for start_col in range(token.start_col, token.start_col + 3):\n                    end_col = start_col + 1\n                    yield (\n                        TOKEN_TYPE_MAP[token.type],\n                        token_str,\n                        (token.start_line, start_col),\n                        (token.end_line, end_col),\n                        source_line,\n                    )\n            else:\n                token_type = TOKEN_TYPE_MAP.get(token.type)\n                if token_type is None:\n                    raise ValueError(f\"Unknown token type: {token.type!r}\")\n                yield (\n                    TOKEN_TYPE_MAP[token.type],\n                    token_str,\n                    (token.start_line, token.start_col),\n                    (token.end_line, token.end_col),\n                    source_line,\n                )\n            prev_token = token\n\n    except pytokens.UnexpectedEOF:\n        raise TokenError(\"Unexpected EOF in multi-line statement\", (line, column))\n    except pytokens.TokenizeError as exc:\n        raise TokenError(f\"Failed to parse: {type(exc).__name__}\", (line, column))"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 27,
      "kind": "module",
      "qualname": "src.blib2to3.pytree",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 30,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.type_repr",
      "span": [
        30,
        44
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "def type_repr(type_num: int) -> str | int:\n    global _type_reprs\n    if not _type_reprs:\n        from . import pygram\n\n        if not hasattr(pygram, \"python_symbols\"):\n            pygram.initialize(cache_dir=None)\n\n        # printing tokens is possible but not as useful\n        # from .pgen2 import token // token.__dict__.items():\n        for name in dir(pygram.python_symbols):\n            val = getattr(pygram.python_symbols, name)\n            if type(val) == int:\n                _type_reprs[val] = name\n    return _type_reprs.setdefault(type_num, type_num)",
      "old_code": "def type_repr(type_num: int) -> Union[str, int]:\n    global _type_reprs\n    if not _type_reprs:\n        from . import pygram\n\n        if not hasattr(pygram, \"python_symbols\"):\n            pygram.initialize(cache_dir=None)\n\n        # printing tokens is possible but not as useful\n        # from .pgen2 import token // token.__dict__.items():\n        for name in dir(pygram.python_symbols):\n            val = getattr(pygram.python_symbols, name)\n            if type(val) == int:\n                _type_reprs[val] = name\n    return _type_reprs.setdefault(type_num, type_num)"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 128,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Base.replace",
      "span": [
        128,
        150
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def replace(self, new: NL | list[NL]) -> None:\n        \"\"\"Replace this node with a new one in the parent.\"\"\"\n        assert self.parent is not None, str(self)\n        assert new is not None\n        if not isinstance(new, list):\n            new = [new]\n        l_children = []\n        found = False\n        for ch in self.parent.children:\n            if ch is self:\n                assert not found, (self.parent.children, self, new)\n                if new is not None:\n                    l_children.extend(new)\n                found = True\n            else:\n                l_children.append(ch)\n        assert found, (self.children, self, new)\n        self.parent.children = l_children\n        self.parent.changed()\n        self.parent.invalidate_sibling_maps()\n        for x in new:\n            x.parent = self.parent\n        self.parent = None",
      "old_code": "    def replace(self, new: Union[NL, list[NL]]) -> None:\n        \"\"\"Replace this node with a new one in the parent.\"\"\"\n        assert self.parent is not None, str(self)\n        assert new is not None\n        if not isinstance(new, list):\n            new = [new]\n        l_children = []\n        found = False\n        for ch in self.parent.children:\n            if ch is self:\n                assert not found, (self.parent.children, self, new)\n                if new is not None:\n                    l_children.extend(new)\n                found = True\n            else:\n                l_children.append(ch)\n        assert found, (self.children, self, new)\n        self.parent.children = l_children\n        self.parent.changed()\n        self.parent.invalidate_sibling_maps()\n        for x in new:\n            x.parent = self.parent\n        self.parent = None"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 152,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Base.get_lineno",
      "span": [
        152,
        159
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def get_lineno(self) -> int | None:\n        \"\"\"Return the line number which generated the invocant node.\"\"\"\n        node = self\n        while not isinstance(node, Leaf):\n            if not node.children:\n                return None\n            node = node.children[0]\n        return node.lineno",
      "old_code": "    def get_lineno(self) -> Optional[int]:\n        \"\"\"Return the line number which generated the invocant node.\"\"\"\n        node = self\n        while not isinstance(node, Leaf):\n            if not node.children:\n                return None\n            node = node.children[0]\n        return node.lineno"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 168,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Base.remove",
      "span": [
        168,
        181
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def remove(self) -> int | None:\n        \"\"\"\n        Remove the node from the tree. Returns the position of the node in its\n        parent's children before it was removed.\n        \"\"\"\n        if self.parent:\n            for i, node in enumerate(self.parent.children):\n                if node is self:\n                    del self.parent.children[i]\n                    self.parent.changed()\n                    self.parent.invalidate_sibling_maps()\n                    self.parent = None\n                    return i\n        return None",
      "old_code": "    def remove(self) -> Optional[int]:\n        \"\"\"\n        Remove the node from the tree. Returns the position of the node in its\n        parent's children before it was removed.\n        \"\"\"\n        if self.parent:\n            for i, node in enumerate(self.parent.children):\n                if node is self:\n                    del self.parent.children[i]\n                    self.parent.changed()\n                    self.parent.invalidate_sibling_maps()\n                    self.parent = None\n                    return i\n        return None"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 184,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Base.next_sibling",
      "span": [
        184,
        195
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def next_sibling(self) -> NL | None:\n        \"\"\"\n        The node immediately following the invocant in their parent's children\n        list. If the invocant does not have a next sibling, it is None\n        \"\"\"\n        if self.parent is None:\n            return None\n\n        if self.parent.next_sibling_map is None:\n            self.parent.update_sibling_maps()\n        assert self.parent.next_sibling_map is not None\n        return self.parent.next_sibling_map[id(self)]",
      "old_code": "    def next_sibling(self) -> Optional[NL]:\n        \"\"\"\n        The node immediately following the invocant in their parent's children\n        list. If the invocant does not have a next sibling, it is None\n        \"\"\"\n        if self.parent is None:\n            return None\n\n        if self.parent.next_sibling_map is None:\n            self.parent.update_sibling_maps()\n        assert self.parent.next_sibling_map is not None\n        return self.parent.next_sibling_map[id(self)]"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 198,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Base.prev_sibling",
      "span": [
        198,
        209
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def prev_sibling(self) -> NL | None:\n        \"\"\"\n        The node immediately preceding the invocant in their parent's children\n        list. If the invocant does not have a previous sibling, it is None.\n        \"\"\"\n        if self.parent is None:\n            return None\n\n        if self.parent.prev_sibling_map is None:\n            self.parent.update_sibling_maps()\n        assert self.parent.prev_sibling_map is not None\n        return self.parent.prev_sibling_map[id(self)]",
      "old_code": "    def prev_sibling(self) -> Optional[NL]:\n        \"\"\"\n        The node immediately preceding the invocant in their parent's children\n        list. If the invocant does not have a previous sibling, it is None.\n        \"\"\"\n        if self.parent is None:\n            return None\n\n        if self.parent.prev_sibling_map is None:\n            self.parent.update_sibling_maps()\n        assert self.parent.prev_sibling_map is not None\n        return self.parent.prev_sibling_map[id(self)]"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 235,
      "kind": "class",
      "qualname": "src.blib2to3.pytree.Node",
      "span": [
        232,
        365
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class Node(Base):\n    \"\"\"Concrete implementation for interior nodes.\"\"\"\n\n    fixers_applied: list[Any] | None\n    used_names: set[str] | None\n\n    def __init__(\n        self,\n        type: int,\n        children: list[NL],\n        context: Any | None = None,\n        prefix: str | None = None,\n        fixers_applied: list[Any] | None = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a symbol number >= 256), a sequence of\n        child nodes, and an optional context keyword argument.\n\n        As a side effect, the parent pointers of the children are updated.\n        \"\"\"\n        assert type >= 256, type\n        self.type = type\n        self.children = list(children)\n        for ch in self.children:\n            assert ch.parent is None, repr(ch)\n            ch.parent = self\n        self.invalidate_sibling_maps()\n        if prefix is not None:\n            self.prefix = prefix\n        if fixers_applied:\n            self.fixers_applied = fixers_applied[:]\n        else:\n            self.fixers_applied = None\n\n    def __repr__(self) -> str:\n        \"\"\"Return a canonical string representation.\"\"\"\n        assert self.type is not None\n        return f\"{self.__class__.__name__}({type_repr(self.type)}, {self.children!r})\"\n\n    def __str__(self) -> str:\n        \"\"\"\n        Return a pretty string representation.\n\n        This reproduces the input source exactly.\n        \"\"\"\n        return \"\".join(map(str, self.children))\n\n    def _eq(self, other: Base) -> bool:\n        \"\"\"Compare two nodes for equality.\"\"\"\n        return (self.type, self.children) == (other.type, other.children)\n\n    def clone(self) -> \"Node\":\n        assert self.type is not None\n        \"\"\"Return a cloned (deep) copy of self.\"\"\"\n        return Node(\n            self.type,\n            [ch.clone() for ch in self.children],\n            fixers_applied=self.fixers_applied,\n        )\n\n    def post_order(self) -> Iterator[NL]:\n        \"\"\"Return a post-order iterator for the tree.\"\"\"\n        for child in self.children:\n            yield from child.post_order()\n        yield self\n\n    def pre_order(self) -> Iterator[NL]:\n        \"\"\"Return a pre-order iterator for the tree.\"\"\"\n        yield self\n        for child in self.children:\n            yield from child.pre_order()\n\n    @property\n    def prefix(self) -> str:\n        \"\"\"\n        The whitespace and comments preceding this node in the input.\n        \"\"\"\n        if not self.children:\n            return \"\"\n        return self.children[0].prefix\n\n    @prefix.setter\n    def prefix(self, prefix: str) -> None:\n        if self.children:\n            self.children[0].prefix = prefix\n\n    def set_child(self, i: int, child: NL) -> None:\n        \"\"\"\n        Equivalent to 'node.children[i] = child'. This method also sets the\n        child's parent attribute appropriately.\n        \"\"\"\n        child.parent = self\n        self.children[i].parent = None\n        self.children[i] = child\n        self.changed()\n        self.invalidate_sibling_maps()\n\n    def insert_child(self, i: int, child: NL) -> None:\n        \"\"\"\n        Equivalent to 'node.children.insert(i, child)'. This method also sets\n        the child's parent attribute appropriately.\n        \"\"\"\n        child.parent = self\n        self.children.insert(i, child)\n        self.changed()\n        self.invalidate_sibling_maps()\n\n    def append_child(self, child: NL) -> None:\n        \"\"\"\n        Equivalent to 'node.children.append(child)'. This method also sets the\n        child's parent attribute appropriately.\n        \"\"\"\n        child.parent = self\n        self.children.append(child)\n        self.changed()\n        self.invalidate_sibling_maps()\n\n    def invalidate_sibling_maps(self) -> None:\n        self.prev_sibling_map: dict[int, NL | None] | None = None\n        self.next_sibling_map: dict[int, NL | None] | None = None\n\n    def update_sibling_maps(self) -> None:\n        _prev: dict[int, NL | None] = {}\n        _next: dict[int, NL | None] = {}\n        self.prev_sibling_map = _prev\n        self.next_sibling_map = _next\n        previous: NL | None = None\n        for current in self.children:\n            _prev[id(current)] = previous\n            _next[id(previous)] = current\n            previous = current\n        _next[id(current)] = None",
      "old_code": "class Node(Base):\n    \"\"\"Concrete implementation for interior nodes.\"\"\"\n\n    fixers_applied: Optional[list[Any]]\n    used_names: Optional[set[str]]\n\n    def __init__(\n        self,\n        type: int,\n        children: list[NL],\n        context: Optional[Any] = None,\n        prefix: Optional[str] = None,\n        fixers_applied: Optional[list[Any]] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a symbol number >= 256), a sequence of\n        child nodes, and an optional context keyword argument.\n\n        As a side effect, the parent pointers of the children are updated.\n        \"\"\"\n        assert type >= 256, type\n        self.type = type\n        self.children = list(children)\n        for ch in self.children:\n            assert ch.parent is None, repr(ch)\n            ch.parent = self\n        self.invalidate_sibling_maps()\n        if prefix is not None:\n            self.prefix = prefix\n        if fixers_applied:\n            self.fixers_applied = fixers_applied[:]\n        else:\n            self.fixers_applied = None\n\n    def __repr__(self) -> str:\n        \"\"\"Return a canonical string representation.\"\"\"\n        assert self.type is not None\n        return f\"{self.__class__.__name__}({type_repr(self.type)}, {self.children!r})\"\n\n    def __str__(self) -> str:\n        \"\"\"\n        Return a pretty string representation.\n\n        This reproduces the input source exactly.\n        \"\"\"\n        return \"\".join(map(str, self.children))\n\n    def _eq(self, other: Base) -> bool:\n        \"\"\"Compare two nodes for equality.\"\"\"\n        return (self.type, self.children) == (other.type, other.children)\n\n    def clone(self) -> \"Node\":\n        assert self.type is not None\n        \"\"\"Return a cloned (deep) copy of self.\"\"\"\n        return Node(\n            self.type,\n            [ch.clone() for ch in self.children],\n            fixers_applied=self.fixers_applied,\n        )\n\n    def post_order(self) -> Iterator[NL]:\n        \"\"\"Return a post-order iterator for the tree.\"\"\"\n        for child in self.children:\n            yield from child.post_order()\n        yield self\n\n    def pre_order(self) -> Iterator[NL]:\n        \"\"\"Return a pre-order iterator for the tree.\"\"\"\n        yield self\n        for child in self.children:\n            yield from child.pre_order()\n\n    @property\n    def prefix(self) -> str:\n        \"\"\"\n        The whitespace and comments preceding this node in the input.\n        \"\"\"\n        if not self.children:\n            return \"\"\n        return self.children[0].prefix\n\n    @prefix.setter\n    def prefix(self, prefix: str) -> None:\n        if self.children:\n            self.children[0].prefix = prefix\n\n    def set_child(self, i: int, child: NL) -> None:\n        \"\"\"\n        Equivalent to 'node.children[i] = child'. This method also sets the\n        child's parent attribute appropriately.\n        \"\"\"\n        child.parent = self\n        self.children[i].parent = None\n        self.children[i] = child\n        self.changed()\n        self.invalidate_sibling_maps()\n\n    def insert_child(self, i: int, child: NL) -> None:\n        \"\"\"\n        Equivalent to 'node.children.insert(i, child)'. This method also sets\n        the child's parent attribute appropriately.\n        \"\"\"\n        child.parent = self\n        self.children.insert(i, child)\n        self.changed()\n        self.invalidate_sibling_maps()\n\n    def append_child(self, child: NL) -> None:\n        \"\"\"\n        Equivalent to 'node.children.append(child)'. This method also sets the\n        child's parent attribute appropriately.\n        \"\"\"\n        child.parent = self\n        self.children.append(child)\n        self.changed()\n        self.invalidate_sibling_maps()\n\n    def invalidate_sibling_maps(self) -> None:\n        self.prev_sibling_map: Optional[dict[int, Optional[NL]]] = None\n        self.next_sibling_map: Optional[dict[int, Optional[NL]]] = None\n\n    def update_sibling_maps(self) -> None:\n        _prev: dict[int, Optional[NL]] = {}\n        _next: dict[int, Optional[NL]] = {}\n        self.prev_sibling_map = _prev\n        self.next_sibling_map = _next\n        previous: Optional[NL] = None\n        for current in self.children:\n            _prev[id(current)] = previous\n            _next[id(previous)] = current\n            previous = current\n        _next[id(current)] = None"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 242,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Node.__init__",
      "span": [
        238,
        266
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self,\n        type: int,\n        children: list[NL],\n        context: Any | None = None,\n        prefix: str | None = None,\n        fixers_applied: list[Any] | None = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a symbol number >= 256), a sequence of\n        child nodes, and an optional context keyword argument.\n\n        As a side effect, the parent pointers of the children are updated.\n        \"\"\"\n        assert type >= 256, type\n        self.type = type\n        self.children = list(children)\n        for ch in self.children:\n            assert ch.parent is None, repr(ch)\n            ch.parent = self\n        self.invalidate_sibling_maps()\n        if prefix is not None:\n            self.prefix = prefix\n        if fixers_applied:\n            self.fixers_applied = fixers_applied[:]\n        else:\n            self.fixers_applied = None",
      "old_code": "    def __init__(\n        self,\n        type: int,\n        children: list[NL],\n        context: Optional[Any] = None,\n        prefix: Optional[str] = None,\n        fixers_applied: Optional[list[Any]] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a symbol number >= 256), a sequence of\n        child nodes, and an optional context keyword argument.\n\n        As a side effect, the parent pointers of the children are updated.\n        \"\"\"\n        assert type >= 256, type\n        self.type = type\n        self.children = list(children)\n        for ch in self.children:\n            assert ch.parent is None, repr(ch)\n            ch.parent = self\n        self.invalidate_sibling_maps()\n        if prefix is not None:\n            self.prefix = prefix\n        if fixers_applied:\n            self.fixers_applied = fixers_applied[:]\n        else:\n            self.fixers_applied = None"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 352,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Node.invalidate_sibling_maps",
      "span": [
        351,
        353
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def invalidate_sibling_maps(self) -> None:\n        self.prev_sibling_map: dict[int, NL | None] | None = None\n        self.next_sibling_map: dict[int, NL | None] | None = None",
      "old_code": "    def invalidate_sibling_maps(self) -> None:\n        self.prev_sibling_map: Optional[dict[int, Optional[NL]]] = None\n        self.next_sibling_map: Optional[dict[int, Optional[NL]]] = None"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 356,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Node.update_sibling_maps",
      "span": [
        355,
        365
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def update_sibling_maps(self) -> None:\n        _prev: dict[int, NL | None] = {}\n        _next: dict[int, NL | None] = {}\n        self.prev_sibling_map = _prev\n        self.next_sibling_map = _next\n        previous: NL | None = None\n        for current in self.children:\n            _prev[id(current)] = previous\n            _next[id(previous)] = current\n            previous = current\n        _next[id(current)] = None",
      "old_code": "    def update_sibling_maps(self) -> None:\n        _prev: dict[int, Optional[NL]] = {}\n        _next: dict[int, Optional[NL]] = {}\n        self.prev_sibling_map = _prev\n        self.next_sibling_map = _next\n        previous: Optional[NL] = None\n        for current in self.children:\n            _prev[id(current)] = previous\n            _next[id(previous)] = current\n            previous = current\n        _next[id(current)] = None"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 377,
      "kind": "class",
      "qualname": "src.blib2to3.pytree.Leaf",
      "span": [
        368,
        468
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class Leaf(Base):\n    \"\"\"Concrete implementation for leaf nodes.\"\"\"\n\n    # Default values for instance variables\n    value: str\n    fixers_applied: list[Any]\n    bracket_depth: int\n    # Changed later in brackets.py\n    opening_bracket: Optional[\"Leaf\"] = None\n    used_names: set[str] | None\n    _prefix = \"\"  # Whitespace and comments preceding this token in the input\n    lineno: int = 0  # Line where this token starts in the input\n    column: int = 0  # Column where this token starts in the input\n    # If not None, this Leaf is created by converting a block of fmt off/skip\n    # code, and `fmt_pass_converted_first_leaf` points to the first Leaf in the\n    # converted code.\n    fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None\n\n    def __init__(\n        self,\n        type: int,\n        value: str,\n        context: Context | None = None,\n        prefix: str | None = None,\n        fixers_applied: list[Any] = [],\n        opening_bracket: Optional[\"Leaf\"] = None,\n        fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a token number < 256), a string value, and an\n        optional context keyword argument.\n        \"\"\"\n\n        assert 0 <= type < 256, type\n        if context is not None:\n            self._prefix, (self.lineno, self.column) = context\n        self.type = type\n        self.value = value\n        if prefix is not None:\n            self._prefix = prefix\n        self.fixers_applied: list[Any] | None = fixers_applied[:]\n        self.children = []\n        self.opening_bracket = opening_bracket\n        self.fmt_pass_converted_first_leaf = fmt_pass_converted_first_leaf\n\n    def __repr__(self) -> str:\n        \"\"\"Return a canonical string representation.\"\"\"\n        from .pgen2.token import tok_name\n\n        assert self.type is not None\n        return (\n            f\"{self.__class__.__name__}({tok_name.get(self.type, self.type)},\"\n            f\" {self.value!r})\"\n        )\n\n    def __str__(self) -> str:\n        \"\"\"\n        Return a pretty string representation.\n\n        This reproduces the input source exactly.\n        \"\"\"\n        return self._prefix + str(self.value)\n\n    def _eq(self, other: \"Leaf\") -> bool:\n        \"\"\"Compare two nodes for equality.\"\"\"\n        return (self.type, self.value) == (other.type, other.value)\n\n    def clone(self) -> \"Leaf\":\n        assert self.type is not None\n        \"\"\"Return a cloned (deep) copy of self.\"\"\"\n        return Leaf(\n            self.type,\n            self.value,\n            (self.prefix, (self.lineno, self.column)),\n            fixers_applied=self.fixers_applied,\n        )\n\n    def leaves(self) -> Iterator[\"Leaf\"]:\n        yield self\n\n    def post_order(self) -> Iterator[\"Leaf\"]:\n        \"\"\"Return a post-order iterator for the tree.\"\"\"\n        yield self\n\n    def pre_order(self) -> Iterator[\"Leaf\"]:\n        \"\"\"Return a pre-order iterator for the tree.\"\"\"\n        yield self\n\n    @property\n    def prefix(self) -> str:\n        \"\"\"\n        The whitespace and comments preceding this token in the input.\n        \"\"\"\n        return self._prefix\n\n    @prefix.setter\n    def prefix(self, prefix: str) -> None:\n        self.changed()\n        self._prefix = prefix",
      "old_code": "class Leaf(Base):\n    \"\"\"Concrete implementation for leaf nodes.\"\"\"\n\n    # Default values for instance variables\n    value: str\n    fixers_applied: list[Any]\n    bracket_depth: int\n    # Changed later in brackets.py\n    opening_bracket: Optional[\"Leaf\"] = None\n    used_names: Optional[set[str]]\n    _prefix = \"\"  # Whitespace and comments preceding this token in the input\n    lineno: int = 0  # Line where this token starts in the input\n    column: int = 0  # Column where this token starts in the input\n    # If not None, this Leaf is created by converting a block of fmt off/skip\n    # code, and `fmt_pass_converted_first_leaf` points to the first Leaf in the\n    # converted code.\n    fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None\n\n    def __init__(\n        self,\n        type: int,\n        value: str,\n        context: Optional[Context] = None,\n        prefix: Optional[str] = None,\n        fixers_applied: list[Any] = [],\n        opening_bracket: Optional[\"Leaf\"] = None,\n        fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a token number < 256), a string value, and an\n        optional context keyword argument.\n        \"\"\"\n\n        assert 0 <= type < 256, type\n        if context is not None:\n            self._prefix, (self.lineno, self.column) = context\n        self.type = type\n        self.value = value\n        if prefix is not None:\n            self._prefix = prefix\n        self.fixers_applied: Optional[list[Any]] = fixers_applied[:]\n        self.children = []\n        self.opening_bracket = opening_bracket\n        self.fmt_pass_converted_first_leaf = fmt_pass_converted_first_leaf\n\n    def __repr__(self) -> str:\n        \"\"\"Return a canonical string representation.\"\"\"\n        from .pgen2.token import tok_name\n\n        assert self.type is not None\n        return (\n            f\"{self.__class__.__name__}({tok_name.get(self.type, self.type)},\"\n            f\" {self.value!r})\"\n        )\n\n    def __str__(self) -> str:\n        \"\"\"\n        Return a pretty string representation.\n\n        This reproduces the input source exactly.\n        \"\"\"\n        return self._prefix + str(self.value)\n\n    def _eq(self, other: \"Leaf\") -> bool:\n        \"\"\"Compare two nodes for equality.\"\"\"\n        return (self.type, self.value) == (other.type, other.value)\n\n    def clone(self) -> \"Leaf\":\n        assert self.type is not None\n        \"\"\"Return a cloned (deep) copy of self.\"\"\"\n        return Leaf(\n            self.type,\n            self.value,\n            (self.prefix, (self.lineno, self.column)),\n            fixers_applied=self.fixers_applied,\n        )\n\n    def leaves(self) -> Iterator[\"Leaf\"]:\n        yield self\n\n    def post_order(self) -> Iterator[\"Leaf\"]:\n        \"\"\"Return a post-order iterator for the tree.\"\"\"\n        yield self\n\n    def pre_order(self) -> Iterator[\"Leaf\"]:\n        \"\"\"Return a pre-order iterator for the tree.\"\"\"\n        yield self\n\n    @property\n    def prefix(self) -> str:\n        \"\"\"\n        The whitespace and comments preceding this token in the input.\n        \"\"\"\n        return self._prefix\n\n    @prefix.setter\n    def prefix(self, prefix: str) -> None:\n        self.changed()\n        self._prefix = prefix"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 390,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.Leaf.__init__",
      "span": [
        386,
        413
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self,\n        type: int,\n        value: str,\n        context: Context | None = None,\n        prefix: str | None = None,\n        fixers_applied: list[Any] = [],\n        opening_bracket: Optional[\"Leaf\"] = None,\n        fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a token number < 256), a string value, and an\n        optional context keyword argument.\n        \"\"\"\n\n        assert 0 <= type < 256, type\n        if context is not None:\n            self._prefix, (self.lineno, self.column) = context\n        self.type = type\n        self.value = value\n        if prefix is not None:\n            self._prefix = prefix\n        self.fixers_applied: list[Any] | None = fixers_applied[:]\n        self.children = []\n        self.opening_bracket = opening_bracket\n        self.fmt_pass_converted_first_leaf = fmt_pass_converted_first_leaf",
      "old_code": "    def __init__(\n        self,\n        type: int,\n        value: str,\n        context: Optional[Context] = None,\n        prefix: Optional[str] = None,\n        fixers_applied: list[Any] = [],\n        opening_bracket: Optional[\"Leaf\"] = None,\n        fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a token number < 256), a string value, and an\n        optional context keyword argument.\n        \"\"\"\n\n        assert 0 <= type < 256, type\n        if context is not None:\n            self._prefix, (self.lineno, self.column) = context\n        self.type = type\n        self.value = value\n        if prefix is not None:\n            self._prefix = prefix\n        self.fixers_applied: Optional[list[Any]] = fixers_applied[:]\n        self.children = []\n        self.opening_bracket = opening_bracket\n        self.fmt_pass_converted_first_leaf = fmt_pass_converted_first_leaf"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 510,
      "kind": "class",
      "qualname": "src.blib2to3.pytree.BasePattern",
      "span": [
        494,
        582
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "class BasePattern:\n    \"\"\"\n    A pattern is a tree matching pattern.\n\n    It looks for a specific node type (token or symbol), and\n    optionally for a specific content.\n\n    This is an abstract base class.  There are three concrete\n    subclasses:\n\n    - LeafPattern matches a single leaf node;\n    - NodePattern matches a single node (usually non-leaf);\n    - WildcardPattern matches a sequence of nodes of variable length.\n    \"\"\"\n\n    # Defaults for instance variables\n    type: int | None\n    type = None  # Node type (token if < 256, symbol if >= 256)\n    content: Any = None  # Optional content matching pattern\n    name: str | None = None  # Optional name used to store match in results dict\n\n    def __new__(cls, *args, **kwds):\n        \"\"\"Constructor that prevents BasePattern from being instantiated.\"\"\"\n        assert cls is not BasePattern, \"Cannot instantiate BasePattern\"\n        return object.__new__(cls)\n\n    def __repr__(self) -> str:\n        assert self.type is not None\n        args = [type_repr(self.type), self.content, self.name]\n        while args and args[-1] is None:\n            del args[-1]\n        return f\"{self.__class__.__name__}({', '.join(map(repr, args))})\"\n\n    def _submatch(self, node, results=None) -> bool:\n        raise NotImplementedError\n\n    def optimize(self) -> \"BasePattern\":\n        \"\"\"\n        A subclass can define this as a hook for optimizations.\n\n        Returns either self or another node with the same effect.\n        \"\"\"\n        return self\n\n    def match(self, node: NL, results: _Results | None = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a node?\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if self.type is not None and node.type != self.type:\n            return False\n        if self.content is not None:\n            r: _Results | None = None\n            if results is not None:\n                r = {}\n            if not self._submatch(node, r):\n                return False\n            if r:\n                assert results is not None\n                results.update(r)\n        if results is not None and self.name:\n            results[self.name] = node\n        return True\n\n    def match_seq(self, nodes: list[NL], results: _Results | None = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a sequence of nodes?\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if len(nodes) != 1:\n            return False\n        return self.match(nodes[0], results)\n\n    def generate_matches(self, nodes: list[NL]) -> Iterator[tuple[int, _Results]]:\n        \"\"\"\n        Generator yielding all matches for this pattern.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        r: _Results = {}\n        if nodes and self.match(nodes[0], r):\n            yield 1, r",
      "old_code": "class BasePattern:\n    \"\"\"\n    A pattern is a tree matching pattern.\n\n    It looks for a specific node type (token or symbol), and\n    optionally for a specific content.\n\n    This is an abstract base class.  There are three concrete\n    subclasses:\n\n    - LeafPattern matches a single leaf node;\n    - NodePattern matches a single node (usually non-leaf);\n    - WildcardPattern matches a sequence of nodes of variable length.\n    \"\"\"\n\n    # Defaults for instance variables\n    type: Optional[int]\n    type = None  # Node type (token if < 256, symbol if >= 256)\n    content: Any = None  # Optional content matching pattern\n    name: Optional[str] = None  # Optional name used to store match in results dict\n\n    def __new__(cls, *args, **kwds):\n        \"\"\"Constructor that prevents BasePattern from being instantiated.\"\"\"\n        assert cls is not BasePattern, \"Cannot instantiate BasePattern\"\n        return object.__new__(cls)\n\n    def __repr__(self) -> str:\n        assert self.type is not None\n        args = [type_repr(self.type), self.content, self.name]\n        while args and args[-1] is None:\n            del args[-1]\n        return f\"{self.__class__.__name__}({', '.join(map(repr, args))})\"\n\n    def _submatch(self, node, results=None) -> bool:\n        raise NotImplementedError\n\n    def optimize(self) -> \"BasePattern\":\n        \"\"\"\n        A subclass can define this as a hook for optimizations.\n\n        Returns either self or another node with the same effect.\n        \"\"\"\n        return self\n\n    def match(self, node: NL, results: Optional[_Results] = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a node?\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if self.type is not None and node.type != self.type:\n            return False\n        if self.content is not None:\n            r: Optional[_Results] = None\n            if results is not None:\n                r = {}\n            if not self._submatch(node, r):\n                return False\n            if r:\n                assert results is not None\n                results.update(r)\n        if results is not None and self.name:\n            results[self.name] = node\n        return True\n\n    def match_seq(self, nodes: list[NL], results: Optional[_Results] = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a sequence of nodes?\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if len(nodes) != 1:\n            return False\n        return self.match(nodes[0], results)\n\n    def generate_matches(self, nodes: list[NL]) -> Iterator[tuple[int, _Results]]:\n        \"\"\"\n        Generator yielding all matches for this pattern.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        r: _Results = {}\n        if nodes and self.match(nodes[0], r):\n            yield 1, r"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 538,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.BasePattern.match",
      "span": [
        538,
        562
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def match(self, node: NL, results: _Results | None = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a node?\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if self.type is not None and node.type != self.type:\n            return False\n        if self.content is not None:\n            r: _Results | None = None\n            if results is not None:\n                r = {}\n            if not self._submatch(node, r):\n                return False\n            if r:\n                assert results is not None\n                results.update(r)\n        if results is not None and self.name:\n            results[self.name] = node\n        return True",
      "old_code": "    def match(self, node: NL, results: Optional[_Results] = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a node?\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if self.type is not None and node.type != self.type:\n            return False\n        if self.content is not None:\n            r: Optional[_Results] = None\n            if results is not None:\n                r = {}\n            if not self._submatch(node, r):\n                return False\n            if r:\n                assert results is not None\n                results.update(r)\n        if results is not None and self.name:\n            results[self.name] = node\n        return True"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 564,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.BasePattern.match_seq",
      "span": [
        564,
        572
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def match_seq(self, nodes: list[NL], results: _Results | None = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a sequence of nodes?\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if len(nodes) != 1:\n            return False\n        return self.match(nodes[0], results)",
      "old_code": "    def match_seq(self, nodes: list[NL], results: Optional[_Results] = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a sequence of nodes?\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if len(nodes) != 1:\n            return False\n        return self.match(nodes[0], results)"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 588,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.LeafPattern.__init__",
      "span": [
        586,
        609
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self,\n        type: int | None = None,\n        content: str | None = None,\n        name: str | None = None,\n    ) -> None:\n        \"\"\"\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given must be a token type (< 256).  If not given,\n        this matches any *leaf* node; the content may still be required.\n\n        The content, if given, must be a string.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        \"\"\"\n        if type is not None:\n            assert 0 <= type < 256, type\n        if content is not None:\n            assert isinstance(content, str), repr(content)\n        self.type = type\n        self.content = content\n        self.name = name",
      "old_code": "    def __init__(\n        self,\n        type: Optional[int] = None,\n        content: Optional[str] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given must be a token type (< 256).  If not given,\n        this matches any *leaf* node; the content may still be required.\n\n        The content, if given, must be a string.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        \"\"\"\n        if type is not None:\n            assert 0 <= type < 256, type\n        if content is not None:\n            assert isinstance(content, str), repr(content)\n        self.type = type\n        self.content = content\n        self.name = name"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 638,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.NodePattern.__init__",
      "span": [
        636,
        671
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self,\n        type: int | None = None,\n        content: Iterable[str] | None = None,\n        name: str | None = None,\n    ) -> None:\n        \"\"\"\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given, must be a symbol type (>= 256).  If the\n        type is None this matches *any* single node (leaf or not),\n        except if content is not None, in which it only matches\n        non-leaf nodes that also match the content pattern.\n\n        The content, if not None, must be a sequence of Patterns that\n        must match the node's children exactly.  If the content is\n        given, the type must not be None.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        \"\"\"\n        if type is not None:\n            assert type >= 256, type\n        if content is not None:\n            assert not isinstance(content, str), repr(content)\n            newcontent = list(content)\n            for i, item in enumerate(newcontent):\n                assert isinstance(item, BasePattern), (i, item)\n                # I don't even think this code is used anywhere, but it does cause\n                # unreachable errors from mypy. This function's signature does look\n                # odd though *shrug*.\n                if isinstance(item, WildcardPattern):  # type: ignore[unreachable]\n                    self.wildcards = True  # type: ignore[unreachable]\n        self.type = type\n        self.content = newcontent  # TODO: this is unbound when content is None\n        self.name = name",
      "old_code": "    def __init__(\n        self,\n        type: Optional[int] = None,\n        content: Optional[Iterable[str]] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given, must be a symbol type (>= 256).  If the\n        type is None this matches *any* single node (leaf or not),\n        except if content is not None, in which it only matches\n        non-leaf nodes that also match the content pattern.\n\n        The content, if not None, must be a sequence of Patterns that\n        must match the node's children exactly.  If the content is\n        given, the type must not be None.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        \"\"\"\n        if type is not None:\n            assert type >= 256, type\n        if content is not None:\n            assert not isinstance(content, str), repr(content)\n            newcontent = list(content)\n            for i, item in enumerate(newcontent):\n                assert isinstance(item, BasePattern), (i, item)\n                # I don't even think this code is used anywhere, but it does cause\n                # unreachable errors from mypy. This function's signature does look\n                # odd though *shrug*.\n                if isinstance(item, WildcardPattern):  # type: ignore[unreachable]\n                    self.wildcards = True  # type: ignore[unreachable]\n        self.type = type\n        self.content = newcontent  # TODO: this is unbound when content is None\n        self.name = name"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 719,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.WildcardPattern.__init__",
      "span": [
        717,
        759
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(\n        self,\n        content: str | None = None,\n        min: int = 0,\n        max: int = HUGE,\n        name: str | None = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Args:\n            content: optional sequence of subsequences of patterns;\n                     if absent, matches one node;\n                     if present, each subsequence is an alternative [*]\n            min: optional minimum number of times to match, default 0\n            max: optional maximum number of times to match, default HUGE\n            name: optional name assigned to this match\n\n        [*] Thus, if content is [[a, b, c], [d, e], [f, g, h]] this is\n            equivalent to (a b c | d e | f g h); if content is None,\n            this is equivalent to '.' in regular expression terms.\n            The min and max parameters work as follows:\n                min=0, max=maxint: .*\n                min=1, max=maxint: .+\n                min=0, max=1: .?\n                min=1, max=1: .\n            If content is not None, replace the dot with the parenthesized\n            list of alternatives, e.g. (a b c | d e | f g h)*\n        \"\"\"\n        assert 0 <= min <= max <= HUGE, (min, max)\n        if content is not None:\n            f = lambda s: tuple(s)\n            wrapped_content = tuple(map(f, content))  # Protect against alterations\n            # Check sanity of alternatives\n            assert len(wrapped_content), repr(\n                wrapped_content\n            )  # Can't have zero alternatives\n            for alt in wrapped_content:\n                assert len(alt), repr(alt)  # Can have empty alternatives\n        self.content = wrapped_content\n        self.min = min\n        self.max = max\n        self.name = name",
      "old_code": "    def __init__(\n        self,\n        content: Optional[str] = None,\n        min: int = 0,\n        max: int = HUGE,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Args:\n            content: optional sequence of subsequences of patterns;\n                     if absent, matches one node;\n                     if present, each subsequence is an alternative [*]\n            min: optional minimum number of times to match, default 0\n            max: optional maximum number of times to match, default HUGE\n            name: optional name assigned to this match\n\n        [*] Thus, if content is [[a, b, c], [d, e], [f, g, h]] this is\n            equivalent to (a b c | d e | f g h); if content is None,\n            this is equivalent to '.' in regular expression terms.\n            The min and max parameters work as follows:\n                min=0, max=maxint: .*\n                min=1, max=maxint: .+\n                min=0, max=1: .?\n                min=1, max=1: .\n            If content is not None, replace the dot with the parenthesized\n            list of alternatives, e.g. (a b c | d e | f g h)*\n        \"\"\"\n        assert 0 <= min <= max <= HUGE, (min, max)\n        if content is not None:\n            f = lambda s: tuple(s)\n            wrapped_content = tuple(map(f, content))  # Protect against alterations\n            # Check sanity of alternatives\n            assert len(wrapped_content), repr(\n                wrapped_content\n            )  # Can't have zero alternatives\n            for alt in wrapped_content:\n                assert len(alt), repr(alt)  # Can have empty alternatives\n        self.content = wrapped_content\n        self.min = min\n        self.max = max\n        self.name = name"
    },
    {
      "path": "src/blib2to3/pytree.py",
      "version": "new",
      "line": 911,
      "kind": "function",
      "qualname": "src.blib2to3.pytree.NegatedPattern.__init__",
      "span": [
        911,
        922
      ],
      "reason": "diff_new_line_in_def",
      "is_test": false,
      "seed_type": "code_change",
      "new_code": "    def __init__(self, content: BasePattern | None = None) -> None:\n        \"\"\"\n        Initializer.\n\n        The argument is either a pattern or None.  If it is None, this\n        only matches an empty sequence (effectively '$' in regex\n        lingo).  If it is not None, this matches whenever the argument\n        pattern doesn't have any matches.\n        \"\"\"\n        if content is not None:\n            assert isinstance(content, BasePattern), repr(content)\n        self.content = content",
      "old_code": "    def __init__(self, content: Optional[BasePattern] = None) -> None:\n        \"\"\"\n        Initializer.\n\n        The argument is either a pattern or None.  If it is None, this\n        only matches an empty sequence (effectively '$' in regex\n        lingo).  If it is not None, this matches whenever the argument\n        pattern doesn't have any matches.\n        \"\"\"\n        if content is not None:\n            assert isinstance(content, BasePattern), repr(content)\n        self.content = content"
    },
    {
      "path": "tests/optional.py",
      "version": "new",
      "line": 21,
      "kind": "module",
      "qualname": "tests.optional",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "tests/optional.py",
      "version": "new",
      "line": 57,
      "kind": "function",
      "qualname": "tests.optional.pytest_configure",
      "span": [
        52,
        104
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "def pytest_configure(config: \"Config\") -> None:\n    \"\"\"Optional tests are markers.\n\n    Use the syntax in https://docs.pytest.org/en/stable/mark.html#registering-marks.\n    \"\"\"\n    # Extract the configured optional-tests from pytest's ini config in a\n    # version-agnostic way. Depending on pytest version, the value can be a\n    # string, a list of strings, or a ConfigValue wrapper (with a `.value` attr).\n    raw_ot_ini: Any = config.inicfg.get(\"optional-tests\")\n    ot_ini_lines: list[str] = []\n    if raw_ot_ini:\n        value = getattr(raw_ot_ini, \"value\", raw_ot_ini)\n        if isinstance(value, str):\n            ot_ini_lines = value.strip().split(\"\\n\")\n        elif isinstance(value, list):\n            # Best-effort coercion to strings; pytest inis are textual.\n            ot_ini_lines = [str(v) for v in value]\n        else:\n            # Fallback: ignore unexpected shapes (non-iterable, etc.).\n            ot_ini_lines = []\n\n    ot_markers: set[str] = set()\n    ot_run: set[str] = set()\n    marker_re = re.compile(r\"^\\s*(?P<no>no_)?(?P<marker>\\w+)(:\\s*(?P<description>.*))?\")\n    # Iterate over configured markers discovered above.\n    for ot in ot_ini_lines:\n        m = marker_re.match(ot)\n        if not m:\n            raise ValueError(f\"{ot!r} doesn't match pytest marker syntax\")\n\n        marker = (m.group(\"no\") or \"\") + m.group(\"marker\")\n        description = m.group(\"description\")\n        config.addinivalue_line(\"markers\", f\"{marker}: {description}\")\n        config.addinivalue_line(\n            \"markers\", f\"{no(marker)}: run when `{marker}` not passed\"\n        )\n        ot_markers.add(marker)\n\n    # collect requested optional tests\n    passed_args = config.getoption(\"run_optional\")\n    if passed_args:\n        ot_run.update(itertools.chain.from_iterable(a.split(\",\") for a in passed_args))\n    ot_run |= {no(excluded) for excluded in ot_markers - ot_run}\n    ot_markers |= {no(m) for m in ot_markers}\n\n    log.info(\"optional tests to run: %s\", ot_run)\n    unknown_tests = ot_run - ot_markers\n    if unknown_tests:\n        raise ValueError(f\"Unknown optional tests wanted: {unknown_tests!r}\")\n\n    store = config._store\n    store[ALL_POSSIBLE_OPTIONAL_MARKERS] = frozenset(ot_markers)\n    store[ENABLED_OPTIONAL_MARKERS] = frozenset(ot_run)",
      "old_code": "def pytest_configure(config: \"Config\") -> None:\n    \"\"\"Optional tests are markers.\n\n    Use the syntax in https://docs.pytest.org/en/stable/mark.html#registering-marks.\n    \"\"\"\n    ot_ini = config.inicfg.get(\"optional-tests\") or []\n    ot_markers = set()\n    ot_run: set[str] = set()\n    if isinstance(ot_ini, str):\n        ot_ini = ot_ini.strip().split(\"\\n\")\n    marker_re = re.compile(r\"^\\s*(?P<no>no_)?(?P<marker>\\w+)(:\\s*(?P<description>.*))?\")\n    # getattr shim here is so that we support both pytest>=9 and pytest<9\n    for ot in getattr(ot_ini, \"value\", ot_ini):\n        m = marker_re.match(ot)\n        if not m:\n            raise ValueError(f\"{ot!r} doesn't match pytest marker syntax\")\n\n        marker = (m.group(\"no\") or \"\") + m.group(\"marker\")\n        description = m.group(\"description\")\n        config.addinivalue_line(\"markers\", f\"{marker}: {description}\")\n        config.addinivalue_line(\n            \"markers\", f\"{no(marker)}: run when `{marker}` not passed\"\n        )\n        ot_markers.add(marker)\n\n    # collect requested optional tests\n    passed_args = config.getoption(\"run_optional\")\n    if passed_args:\n        ot_run.update(itertools.chain.from_iterable(a.split(\",\") for a in passed_args))\n    ot_run |= {no(excluded) for excluded in ot_markers - ot_run}\n    ot_markers |= {no(m) for m in ot_markers}\n\n    log.info(\"optional tests to run: %s\", ot_run)\n    unknown_tests = ot_run - ot_markers\n    if unknown_tests:\n        raise ValueError(f\"Unknown optional tests wanted: {unknown_tests!r}\")\n\n    store = config._store\n    store[ALL_POSSIBLE_OPTIONAL_MARKERS] = frozenset(ot_markers)\n    store[ENABLED_OPTIONAL_MARKERS] = frozenset(ot_run)"
    },
    {
      "path": "tests/test_black.py",
      "version": "new",
      "line": 23,
      "kind": "module",
      "qualname": "tests.test_black",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "tests/test_black.py",
      "version": "new",
      "line": 1338,
      "kind": "function",
      "qualname": "tests.test_black.BlackTestCase.test_reformat_one_with_stdin_empty._new_wrapper",
      "span": [
        1336,
        1349
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "        def _new_wrapper(\n            output: io.StringIO, io_TextIOWrapper: type[io.TextIOWrapper]\n        ) -> Callable[[Any, Any], io.StringIO | io.TextIOWrapper]:\n            def get_output(*args: Any, **kwargs: Any) -> io.StringIO | io.TextIOWrapper:\n                if args == (sys.stdout.buffer,):\n                    # It's `format_stdin_to_stdout()` calling `io.TextIOWrapper()`,\n                    # return our mock object.\n                    return output\n                # It's something else (i.e. `decode_bytes()`) calling\n                # `io.TextIOWrapper()`, pass through to the original implementation.\n                # See discussion in https://github.com/psf/black/pull/2489\n                return io_TextIOWrapper(*args, **kwargs)\n\n            return get_output",
      "old_code": "        def _new_wrapper(\n            output: io.StringIO, io_TextIOWrapper: type[io.TextIOWrapper]\n        ) -> Callable[[Any, Any], Union[io.StringIO, io.TextIOWrapper]]:\n            def get_output(\n                *args: Any, **kwargs: Any\n            ) -> Union[io.StringIO, io.TextIOWrapper]:\n                if args == (sys.stdout.buffer,):\n                    # It's `format_stdin_to_stdout()` calling `io.TextIOWrapper()`,\n                    # return our mock object.\n                    return output\n                # It's something else (i.e. `decode_bytes()`) calling\n                # `io.TextIOWrapper()`, pass through to the original implementation.\n                # See discussion in https://github.com/psf/black/pull/2489\n                return io_TextIOWrapper(*args, **kwargs)\n\n            return get_output"
    },
    {
      "path": "tests/test_black.py",
      "version": "new",
      "line": 1339,
      "kind": "function",
      "qualname": "tests.test_black.BlackTestCase.test_reformat_one_with_stdin_empty._new_wrapper.get_output",
      "span": [
        1339,
        1347
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "            def get_output(*args: Any, **kwargs: Any) -> io.StringIO | io.TextIOWrapper:\n                if args == (sys.stdout.buffer,):\n                    # It's `format_stdin_to_stdout()` calling `io.TextIOWrapper()`,\n                    # return our mock object.\n                    return output\n                # It's something else (i.e. `decode_bytes()`) calling\n                # `io.TextIOWrapper()`, pass through to the original implementation.\n                # See discussion in https://github.com/psf/black/pull/2489\n                return io_TextIOWrapper(*args, **kwargs)",
      "old_code": "            def get_output(\n                *args: Any, **kwargs: Any\n            ) -> Union[io.StringIO, io.TextIOWrapper]:\n                if args == (sys.stdout.buffer,):\n                    # It's `format_stdin_to_stdout()` calling `io.TextIOWrapper()`,\n                    # return our mock object.\n                    return output\n                # It's something else (i.e. `decode_bytes()`) calling\n                # `io.TextIOWrapper()`, pass through to the original implementation.\n                # See discussion in https://github.com/psf/black/pull/2489\n                return io_TextIOWrapper(*args, **kwargs)"
    },
    {
      "path": "tests/test_black.py",
      "version": "new",
      "line": 2441,
      "kind": "function",
      "qualname": "tests.test_black.assert_collected_sources",
      "span": [
        2440,
        2471
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "def assert_collected_sources(\n    src: Sequence[str | Path],\n    expected: Sequence[str | Path],\n    *,\n    root: Path | None = None,\n    exclude: str | None = None,\n    include: str | None = None,\n    extend_exclude: str | None = None,\n    force_exclude: str | None = None,\n    stdin_filename: str | None = None,\n) -> None:\n    gs_src = tuple(str(Path(s)) for s in src)\n    gs_expected = [Path(s) for s in expected]\n    gs_exclude = None if exclude is None else compile_pattern(exclude)\n    gs_include = DEFAULT_INCLUDE if include is None else compile_pattern(include)\n    gs_extend_exclude = (\n        None if extend_exclude is None else compile_pattern(extend_exclude)\n    )\n    gs_force_exclude = None if force_exclude is None else compile_pattern(force_exclude)\n    collected = black.get_sources(\n        root=root or THIS_DIR,\n        src=gs_src,\n        quiet=False,\n        verbose=False,\n        include=gs_include,\n        exclude=gs_exclude,\n        extend_exclude=gs_extend_exclude,\n        force_exclude=gs_force_exclude,\n        report=black.Report(),\n        stdin_filename=stdin_filename,\n    )\n    assert sorted(collected) == sorted(gs_expected)",
      "old_code": "def assert_collected_sources(\n    src: Sequence[Union[str, Path]],\n    expected: Sequence[Union[str, Path]],\n    *,\n    root: Optional[Path] = None,\n    exclude: Optional[str] = None,\n    include: Optional[str] = None,\n    extend_exclude: Optional[str] = None,\n    force_exclude: Optional[str] = None,\n    stdin_filename: Optional[str] = None,\n) -> None:\n    gs_src = tuple(str(Path(s)) for s in src)\n    gs_expected = [Path(s) for s in expected]\n    gs_exclude = None if exclude is None else compile_pattern(exclude)\n    gs_include = DEFAULT_INCLUDE if include is None else compile_pattern(include)\n    gs_extend_exclude = (\n        None if extend_exclude is None else compile_pattern(extend_exclude)\n    )\n    gs_force_exclude = None if force_exclude is None else compile_pattern(force_exclude)\n    collected = black.get_sources(\n        root=root or THIS_DIR,\n        src=gs_src,\n        quiet=False,\n        verbose=False,\n        include=gs_include,\n        exclude=gs_exclude,\n        extend_exclude=gs_extend_exclude,\n        force_exclude=gs_force_exclude,\n        report=black.Report(),\n        stdin_filename=stdin_filename,\n    )\n    assert sorted(collected) == sorted(gs_expected)"
    },
    {
      "path": "tests/test_docs.py",
      "version": "new",
      "line": 21,
      "kind": "function",
      "qualname": "tests.test_docs.check_feature_list",
      "span": [
        19,
        56
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "def check_feature_list(\n    lines: Sequence[str], expected_feature_names: set[str], label: str\n) -> str | None:\n    start_index = lines.index(f\"(labels/{label}-features)=\\n\")\n    if start_index == -1:\n        return (\n            f\"Could not find the {label} features list in {DOCS_PATH}. Ensure the\"\n            \" preview-features label is present.\"\n        )\n    num_blank_lines_seen = 0\n    seen_preview_feature_names = set()\n    for line in islice(lines, start_index + 1, None):\n        if not line.strip():\n            num_blank_lines_seen += 1\n            if num_blank_lines_seen == 3:\n                break\n            continue\n        if line.startswith(\"- \"):\n            match = re.search(r\"^- `([a-z\\d_]+)`\", line)\n            if match:\n                seen_preview_feature_names.add(match.group(1))\n\n    if seen_preview_feature_names - expected_feature_names:\n        extra = \", \".join(sorted(seen_preview_feature_names - expected_feature_names))\n        return (\n            f\"The following features should not be in the list of {label} features:\"\n            f\" {extra}. Please remove them from the {label}-features label in\"\n            f\" {DOCS_PATH}\"\n        )\n    elif expected_feature_names - seen_preview_feature_names:\n        missing = \", \".join(sorted(expected_feature_names - seen_preview_feature_names))\n        return (\n            f\"The following features are missing from the list of {label} features:\"\n            f\" {missing}. Please document them under the {label}-features label in\"\n            f\" {DOCS_PATH}\"\n        )\n    else:\n        return None",
      "old_code": "def check_feature_list(\n    lines: Sequence[str], expected_feature_names: set[str], label: str\n) -> Optional[str]:\n    start_index = lines.index(f\"(labels/{label}-features)=\\n\")\n    if start_index == -1:\n        return (\n            f\"Could not find the {label} features list in {DOCS_PATH}. Ensure the\"\n            \" preview-features label is present.\"\n        )\n    num_blank_lines_seen = 0\n    seen_preview_feature_names = set()\n    for line in islice(lines, start_index + 1, None):\n        if not line.strip():\n            num_blank_lines_seen += 1\n            if num_blank_lines_seen == 3:\n                break\n            continue\n        if line.startswith(\"- \"):\n            match = re.search(r\"^- `([a-z\\d_]+)`\", line)\n            if match:\n                seen_preview_feature_names.add(match.group(1))\n\n    if seen_preview_feature_names - expected_feature_names:\n        extra = \", \".join(sorted(seen_preview_feature_names - expected_feature_names))\n        return (\n            f\"The following features should not be in the list of {label} features:\"\n            f\" {extra}. Please remove them from the {label}-features label in\"\n            f\" {DOCS_PATH}\"\n        )\n    elif expected_feature_names - seen_preview_feature_names:\n        missing = \", \".join(sorted(expected_feature_names - seen_preview_feature_names))\n        return (\n            f\"The following features are missing from the list of {label} features:\"\n            f\" {missing}. Please document them under the {label}-features label in\"\n            f\" {DOCS_PATH}\"\n        )\n    else:\n        return None"
    },
    {
      "path": "tests/test_schema.py",
      "version": "new",
      "line": 5,
      "kind": "function",
      "qualname": "tests.test_schema.test_schema_entrypoint",
      "span": [
        4,
        12
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "def test_schema_entrypoint() -> None:\n    (black_ep,) = importlib.metadata.entry_points(\n        group=\"validate_pyproject.tool_schema\", name=\"black\"\n    )\n\n    black_fn = black_ep.load()\n    schema = black_fn()\n    assert schema == black_fn(\"black\")\n    assert schema[\"properties\"][\"line-length\"][\"type\"] == \"integer\"",
      "old_code": "def test_schema_entrypoint() -> None:\n    if sys.version_info < (3, 10):\n        eps = importlib.metadata.entry_points()[\"validate_pyproject.tool_schema\"]\n        (black_ep,) = [ep for ep in eps if ep.name == \"black\"]\n    else:\n        (black_ep,) = importlib.metadata.entry_points(\n            group=\"validate_pyproject.tool_schema\", name=\"black\"\n        )\n\n    black_fn = black_ep.load()\n    schema = black_fn()\n    assert schema == black_fn(\"black\")\n    assert schema[\"properties\"][\"line-length\"][\"type\"] == \"integer\""
    },
    {
      "path": "tests/test_trans.py",
      "version": "new",
      "line": 13,
      "kind": "function",
      "qualname": "tests.test_trans.test_fexpr_spans.check",
      "span": [
        5,
        17
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "    def check(\n        string: str, expected_spans: list[tuple[int, int]], expected_slices: list[str]\n    ) -> None:\n        spans = list(iter_fexpr_spans(string))\n\n        # Checking slices isn't strictly necessary, but it's easier to verify at\n        # a glance than only spans\n        assert len(spans) == len(expected_slices)\n        for (i, j), slice in zip(spans, expected_slices, strict=True):\n            assert 0 <= i <= j <= len(string)\n            assert string[i:j] == slice\n\n        assert spans == expected_spans",
      "old_code": "    def check(\n        string: str, expected_spans: list[tuple[int, int]], expected_slices: list[str]\n    ) -> None:\n        spans = list(iter_fexpr_spans(string))\n\n        # Checking slices isn't strictly necessary, but it's easier to verify at\n        # a glance than only spans\n        assert len(spans) == len(expected_slices)\n        for (i, j), slice in zip(spans, expected_slices):\n            assert 0 <= i <= j <= len(string)\n            assert string[i:j] == slice\n\n        assert spans == expected_spans"
    },
    {
      "path": "tests/util.py",
      "version": "new",
      "line": 12,
      "kind": "module",
      "qualname": "tests.util",
      "span": null,
      "reason": "diff_new_line_outside_defs",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": null,
      "old_code": null
    },
    {
      "path": "tests/util.py",
      "version": "new",
      "line": 48,
      "kind": "class",
      "qualname": "tests.util.TestCaseArgs",
      "span": [
        45,
        50
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "class TestCaseArgs:\n    mode: black.Mode = field(default_factory=black.Mode)\n    fast: bool = False\n    minimum_version: tuple[int, int] | None = None\n    lines: Collection[tuple[int, int]] = ()\n    no_preview_line_length_1: bool = False",
      "old_code": "class TestCaseArgs:\n    mode: black.Mode = field(default_factory=black.Mode)\n    fast: bool = False\n    minimum_version: Optional[tuple[int, int]] = None\n    lines: Collection[tuple[int, int]] = ()\n    no_preview_line_length_1: bool = False"
    },
    {
      "path": "tests/util.py",
      "version": "new",
      "line": 99,
      "kind": "function",
      "qualname": "tests.util.assert_format",
      "span": [
        93,
        159
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "def assert_format(\n    source: str,\n    expected: str,\n    mode: black.Mode = DEFAULT_MODE,\n    *,\n    fast: bool = False,\n    minimum_version: tuple[int, int] | None = None,\n    lines: Collection[tuple[int, int]] = (),\n    no_preview_line_length_1: bool = False,\n) -> None:\n    \"\"\"Convenience function to check that Black formats as expected.\n\n    You can pass @minimum_version if you're passing code with newer syntax to guard\n    safety guards so they don't just crash with a SyntaxError. Please note this is\n    separate from TargetVerson Mode configuration.\n    \"\"\"\n    _assert_format_inner(\n        source, expected, mode, fast=fast, minimum_version=minimum_version, lines=lines\n    )\n\n    # For both preview and non-preview tests, ensure that Black doesn't crash on\n    # this code, but don't pass \"expected\" because the precise output may differ.\n    try:\n        if mode.unstable:\n            new_mode = replace(mode, unstable=False, preview=False)\n        else:\n            new_mode = replace(mode, preview=not mode.preview)\n        _assert_format_inner(\n            source,\n            None,\n            new_mode,\n            fast=fast,\n            minimum_version=minimum_version,\n            lines=lines,\n        )\n    except Exception as e:\n        text = (\n            \"unstable\"\n            if mode.unstable\n            else \"non-preview\" if mode.preview else \"preview\"\n        )\n        raise FormatFailure(\n            f\"Black crashed formatting this case in {text} mode.\"\n        ) from e\n    # Similarly, setting line length to 1 is a good way to catch\n    # stability bugs. Some tests are known to be broken in preview mode with line length\n    # of 1 though, and have marked that with a flag --no-preview-line-length-1\n    preview_modes = [False]\n    if not no_preview_line_length_1:\n        preview_modes.append(True)\n\n    for preview_mode in preview_modes:\n\n        try:\n            _assert_format_inner(\n                source,\n                None,\n                replace(mode, preview=preview_mode, line_length=1, unstable=False),\n                fast=fast,\n                minimum_version=minimum_version,\n                lines=lines,\n            )\n        except Exception as e:\n            text = \"preview\" if preview_mode else \"non-preview\"\n            raise FormatFailure(\n                f\"Black crashed formatting this case in {text} mode with line-length=1.\"\n            ) from e",
      "old_code": "def assert_format(\n    source: str,\n    expected: str,\n    mode: black.Mode = DEFAULT_MODE,\n    *,\n    fast: bool = False,\n    minimum_version: Optional[tuple[int, int]] = None,\n    lines: Collection[tuple[int, int]] = (),\n    no_preview_line_length_1: bool = False,\n) -> None:\n    \"\"\"Convenience function to check that Black formats as expected.\n\n    You can pass @minimum_version if you're passing code with newer syntax to guard\n    safety guards so they don't just crash with a SyntaxError. Please note this is\n    separate from TargetVerson Mode configuration.\n    \"\"\"\n    _assert_format_inner(\n        source, expected, mode, fast=fast, minimum_version=minimum_version, lines=lines\n    )\n\n    # For both preview and non-preview tests, ensure that Black doesn't crash on\n    # this code, but don't pass \"expected\" because the precise output may differ.\n    try:\n        if mode.unstable:\n            new_mode = replace(mode, unstable=False, preview=False)\n        else:\n            new_mode = replace(mode, preview=not mode.preview)\n        _assert_format_inner(\n            source,\n            None,\n            new_mode,\n            fast=fast,\n            minimum_version=minimum_version,\n            lines=lines,\n        )\n    except Exception as e:\n        text = (\n            \"unstable\"\n            if mode.unstable\n            else \"non-preview\" if mode.preview else \"preview\"\n        )\n        raise FormatFailure(\n            f\"Black crashed formatting this case in {text} mode.\"\n        ) from e\n    # Similarly, setting line length to 1 is a good way to catch\n    # stability bugs. Some tests are known to be broken in preview mode with line length\n    # of 1 though, and have marked that with a flag --no-preview-line-length-1\n    preview_modes = [False]\n    if not no_preview_line_length_1:\n        preview_modes.append(True)\n\n    for preview_mode in preview_modes:\n\n        try:\n            _assert_format_inner(\n                source,\n                None,\n                replace(mode, preview=preview_mode, line_length=1, unstable=False),\n                fast=fast,\n                minimum_version=minimum_version,\n                lines=lines,\n            )\n        except Exception as e:\n            text = \"preview\" if preview_mode else \"non-preview\"\n            raise FormatFailure(\n                f\"Black crashed formatting this case in {text} mode with line-length=1.\"\n            ) from e"
    },
    {
      "path": "tests/util.py",
      "version": "new",
      "line": 164,
      "kind": "function",
      "qualname": "tests.util._assert_format_inner",
      "span": [
        162,
        183
      ],
      "reason": "diff_new_line_in_def",
      "is_test": true,
      "seed_type": "test_change",
      "new_code": "def _assert_format_inner(\n    source: str,\n    expected: str | None = None,\n    mode: black.Mode = DEFAULT_MODE,\n    *,\n    fast: bool = False,\n    minimum_version: tuple[int, int] | None = None,\n    lines: Collection[tuple[int, int]] = (),\n) -> None:\n    actual = black.format_str(source, mode=mode, lines=lines)\n    if expected is not None:\n        _assert_format_equal(expected, actual)\n    # It's not useful to run safety checks if we're expecting no changes anyway. The\n    # assertion right above will raise if reality does actually make changes. This just\n    # avoids wasted CPU cycles.\n    if not fast and source != actual:\n        # Unfortunately the AST equivalence check relies on the built-in ast module\n        # being able to parse the code being formatted. This doesn't always work out\n        # when checking modern code on older versions.\n        if minimum_version is None or sys.version_info >= minimum_version:\n            black.assert_equivalent(source, actual)\n        black.assert_stable(source, actual, mode=mode, lines=lines)",
      "old_code": "def _assert_format_inner(\n    source: str,\n    expected: Optional[str] = None,\n    mode: black.Mode = DEFAULT_MODE,\n    *,\n    fast: bool = False,\n    minimum_version: Optional[tuple[int, int]] = None,\n    lines: Collection[tuple[int, int]] = (),\n) -> None:\n    actual = black.format_str(source, mode=mode, lines=lines)\n    if expected is not None:\n        _assert_format_equal(expected, actual)\n    # It's not useful to run safety checks if we're expecting no changes anyway. The\n    # assertion right above will raise if reality does actually make changes. This just\n    # avoids wasted CPU cycles.\n    if not fast and source != actual:\n        # Unfortunately the AST equivalence check relies on the built-in ast module\n        # being able to parse the code being formatted. This doesn't always work out\n        # when checking modern code on older versions.\n        if minimum_version is None or sys.version_info >= minimum_version:\n            black.assert_equivalent(source, actual)\n        black.assert_stable(source, actual, mode=mode, lines=lines)"
    }
  ],
  "generated_at": "2026-02-09T16:07:31"
}